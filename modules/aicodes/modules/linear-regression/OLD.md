# Regress√£o Linear & Gradiente Descendente

## Conte√∫do

 - [01 - Introdu√ß√£o √†s Regress√µes Lineares](#01)
 - [02 - M√©todo dos M√≠nimos Quadrados (Sum of Squared Errors: SSE)](#02)
 - [03 - M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)](#03)
 - [04 - M√©todo do Gradiente Descendente](#04)
 - [05 - Tentando minimizar a Fun√ß√£o de Custo](#05)
 - [06 - A Regra da Cadeia](#06)
 - [07 - Aplicando a Regra da Cadeia na Fun√ß√£o de Custo](#07)
 - [08 - Taxa de Aprendizagem (Learning Rate)](#08)
 - [09 - Aplicando o M√©todo do Gradiente Descendente na pr√°tica](#09)
 - [10 - Criando uma Reta de melhor ajuste com com Scikit-Learn](#10)
 - [11 - Dividindo os dados em Treino e Teste](#11)
 - [12 - Pegando o Coeficiente de Determina√ß√£o R<sup>2</sup> com Scikit-Learn](#12)
 - [13 - Usando o conceito de Regress√£o Linear & R<sup>2</sup> em um conjunto de dados reais + random_state](#13)
 - [14 - Fazendo previs√µes](#14)

<div id='01'></div>

## 01 - Introdu√ß√£o √†s Regress√µes Lineares

Antes de cair de cara nos c√≥digos e t√≥picos mais avan√ßados vamos definir algumas coisas bem b√°sicas sobre **Regress√µes Lineares**.

> As **Regress√µes Lineares** mais comuns s√£o a **Simples** e **M√∫ltipla**.

 - No primeiro caso, a **Regress√£o Linear Simples**, tem o objetivo √© investigar a influ√™ncia de uma *Vari√°vel Independente* sobre uma *Vari√°vel Dependente*.
 - No segundo caso, **Regress√£o Linear M√∫ltipla**, analisa a influ√™ncia de v√°rias *Vari√°veis Independentes* em uma *Vari√°vel Dependente*.

Algo parecido com isso:

![image](images/linear-regression-types.jpg)  

### Regress√£o Linear Simples

> O objetivo da regress√£o linear simples √© **prever** o valor de uma *Vari√°vel Dependente* com base em uma *Vari√°vel Independente*.

 - Quanto maior for a rela√ß√£o linear entre a *Vari√°vel Independente* e a *Vari√°vel Dependente*, mais precisa √© a **previs√£o**.
 - Isso tamb√©m significa que quanto maior a propor√ß√£o da vari√¢ncia da *Vari√°vel Dependente* que pode ser explicada pela Vari√°vel Independente, mais precisa √© a previs√£o.

**NOTE:**  
Visualmente, a rela√ß√£o entre as vari√°veis ‚Äã‚Äãpode ser mostrada em um gr√°fico de dispers√£o. Quanto maior a rela√ß√£o linear entre as vari√°veis ‚Äã*‚Äãdependentes* e *independentes*, mais os pontos de dados est√£o em uma linha reta.

Veja o exemplo abaixo:

![image](images/linear-relationship.png)  

### Regress√£o Linear M√∫ltipla

> Ao contr√°rio da *Regress√£o Linear Simples*, a Regress√£o Linear M√∫ltipla permite que mais de duas *Vari√°veis ‚Äã‚ÄãIndependentes* sejam consideradas. O objetivo √© estimar uma vari√°vel com base em v√°rias outras vari√°veis.

**NOTE:**  
A vari√°vel a ser estimada √© chamada de *Vari√°vel Dependente (crit√©rio)*. As vari√°veis ‚Äã‚Äãque s√£o usadas para a previs√£o s√£o chamadas de *Vari√°veis Independentes (preditores)*.

**Regress√£o Multivariada vs Regress√£o M√∫ltipla:**  
A **Regress√£o M√∫ltipla** n√£o deve ser confundida com a **Regress√£o Multivariada**:
 - No primeiro caso, a influ√™ncia de v√°rias vari√°veis ‚Äã‚Äãindependentes em uma vari√°vel dependente √© examinada.
 - No segundo caso, v√°rios modelos de regress√£o s√£o calculados para permitir conclus√µes a serem tiradas sobre v√°rias vari√°veis ‚Äã‚Äãdependentes.

**Conclus√£o:**  
 - Em uma **Regress√£o M√∫ltipla**, uma *Vari√°vel Dependente* √© levada em considera√ß√£o;
 - Enquanto em uma **Regress√£o Multivariada**, v√°rias *Vari√°veis ‚Äã‚ÄãDependentes* s√£o analisadas.

Ok, para come√ßar com nossos exemplos pr√°ticos em **Regress√µes Lineares** vamos imaginar o seguinte... Suponha que n√≥s estamos olhando a rela√ß√£o entre **notas** de alunos e seus **sal√°rios**.

O c√≥digo vai ser o seguinte:

[students.py](src/students.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

plt.figure(figsize=(10, 7))
plt.scatter(df.Grade, df.Salary, color='g')
plt.title('Grades vs Salaries')
plt.xlabel('Grade')
plt.ylabel('Salary')
plt.savefig('../images/plot-01.png', format='png')
plt.show()
```

**OUTPUT:**  

![image](images/plot-01.png)  

Essas vari√°veis s√£o conhecidas, respectivamente como:

 - **Grade (nota):** *Vari√°veis ‚Äã‚Äãindependentes*, *entradas* ou *preditoras*;
 - **Salary (sal√°rio):** *Vari√°veis ‚Äã‚Äãdependentes*, *sa√≠das* ou *respostas*.

Em uma **Regress√£o Linear** √©  comum denotar as sa√≠das com **ùë¶<sub>i</sub>** e as entradas com **ùë•<sub>i</sub>**. Se houver duas ou mais *Vari√°veis ‚ÄãIndependentes*, elas podem ser representadas como um vetor **ùê± = (ùë•‚ÇÅ,‚Ä¶, ùë•·µ£)**, onde **ùëü (ou n)** √© o n√∫mero de entradas.

Mas ent√£o, como eu consigo analisar as **notas** de alunos e seus **sal√°rios**? Bem, existem v√°rias maneiras, por√©m, vamos ver as mais comuns.

---

<div id='02'></div>

## 02 - M√©todo dos M√≠nimos Quadrados (Sum of Squared Errors: SSE)

> O m√©todo dos M√≠nimos Quadrados calcula o erro para cada ponto **(x<sub>i</sub>, y<sub>i</sub>)** em rela√ß√£o a m√©dia de todas as sa√≠das **y<sub>i</sub>**.

N√£o entendeu? Veja a f√≥rmula abaixo:

![images](images/formula-01.png)  

> Resumidamente, n√≥s estamos tirando a `variancia` de cada ponto **(x<sub>i</sub>, y<sub>i</sub>)** em rela√ß√£o a m√©dia de todos os meus **y**.

Agora vamos transformar tudo isso em Python para ficar algo mais automatizado:

[students-SSE.py](src/students-SSE.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['Error'] = df['Salary'] - df['Salary'].mean()
df['Squared Errors'] = df['Error']**2
print(df)
print("Sum of Squared Errors (SSE): ", round(sum(df['Squared Errors'])))
```

**OUTPUT:**  
```python
    Grade  Salary     Error  Squared Errors
0      50   50000  -22400.0    5.017600e+08
1      50   54000  -18400.0    3.385600e+08
2      46   50000  -22400.0    5.017600e+08
3      95  189000  116600.0    1.359556e+10
4      50   55000  -17400.0    3.027600e+08
5       5   40000  -32400.0    1.049760e+09
6      57   59000  -13400.0    1.795600e+08
7      42   42000  -30400.0    9.241600e+08
8      26   47000  -25400.0    6.451600e+08
9      72   78000    5600.0    3.136000e+07
10     78  119000   46600.0    2.171560e+09
11     60   95000   22600.0    5.107600e+08
12     40   49000  -23400.0    5.475600e+08
13     17   29000  -43400.0    1.883560e+09
14     85  130000   57600.0    3.317760e+09
Sum of Squared Errors (SSE):  26501600000
```

**NOTE:**  
Vale ressaltar aqui que n√≥s estamos elevando todos os erros ao quadrado<sup>2</sup> porque alguns deles v√£o ser negativos, e como n√≥s queremos **somar todos os erros** isso acabaria modificando o resultado. Ou seja, estamos deixando todos positivos.

---

<div id='03'></div>

## 03 - M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)

> At√© ent√£o n√≥s estamos utilizando uma abordagem, onde n√≥s tiravamos a **varian√ßa** de cada ponto em rela√ß√£o a m√©dia de todos os resultados **y**.

Agora vamos utilizar uma abordagem que usa uma **reta de melhor ajuste** para ver se conseguimos um efeito melhor. Ou seja, vamos criar uma reta que fique o mais pr√≥ximo poss√≠vel de todos os pontos; Tanto os pontos acima da linha quanto os abaixo.

Para isso, n√≥s podemos utilizar a **Equa√ß√£o da Reta** para criar uma linha que passe o mais pr√≥ximo poss√≠vel de todos os dados:

![image](images/0001.png)  

Tenho certeza, todos n√≥s aprendemos essa f√≥rmula na escola. Para Regress√£o Linear, usamos s√≠mbolos como estes:

**Exemplo 01:**  
![image](images/0003.png)  

**Exemplo 02:**  
![image](images/0005.png)  

**Exemplo 03:**  
![image](images/0002.png)  

**Exemplo com Nota√ß√£o em Matriz:**  
![image](images/0004.png)  

Eu sei que isso pode acabar confundindo um pouco, mas √© s√≥ se lembrar da *Equa√ß√£o da Reta*, **y = mx + b**, onde:

 - O meu **m (ou a)** representa o Coeficiente Angular:
   - Que altera a inclina√ß√£o da reta;
   - E que √© representado por um valor constante.
 - O meu **b** representa o deslocamento da Reta:
   - Intercepta√ß√£o no eixo-y;
   - Que tamb√©m √© representado por um valor constante.

**NOTE:**  
Para criar essa **reta de melhor ajuste** n√≥s precisamos dos melhores valores poss√≠veis para os termos **m** e **b** `para a amostra que estamos trabalhando`. Isso porque os coeficientes **m** e **b** variam de valores de acordo com os dados que n√≥s temos.

> √ìtimo, entendemos a ideia por tr√°s do **Algoritmo de Regress√£o Linear** que usa uma **reta de melhor ajuste** para medir o **tamanho dos nossos erros** e **explicar a rela√ß√£o entre os dados**.

Uma maneira de tentar encontrar os valores para os coeficiente **m** e **b** √© utilizando as seguintes f√≥rmulas:

![image](images/formula-04.png)  
![image](images/formula-05.png)  

Agora vamos testar essa bruxaria em Python para praticar um pouco:

[students-mb-formula.py](src/students-mb-formula.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
b = df['Salary'].mean() - (m * df['Grade'].mean())

print("Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))
```

**OUTPUT:**  
```python
Angular Coefficient (m): 1516
Linear Coefficient (b): -5732.0
```

Ok, agora que n√≥s j√° temos os melhores valores para os coeficientes **m** e **b** `para esse conjunto de dados`  podemos aplicar eles na **Equa√ß√£o da Reta** e criar a **reta de melhor ajuste**:

[students-bestLine-OLS.py](src/students-bestLine-OLS.py)  
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
b = df['Salary'].mean() - (m * df['Grade'].mean())

regression_line = [(m*x) + b for x in df['Grade']]

plt.figure(figsize=(10, 7))
plt.scatter(df.Grade, df.Salary, color='g')
plt.plot(df.Grade, regression_line, color='b')
plt.title('Grades vs Salaries | Ordinary Least Squares: OLS')
plt.xlabel('Grade')
plt.ylabel('Salary')
plt.grid()
plt.savefig('../images/plot-02.png', format='png')
plt.show()
```

**OUTPUT:**  

![image](images/plot-02.png)  

---

![image](images/ohmygod.gif)  

√ìtimo, agora √© s√≥ tirar a `vari√¢ncia` de cada ponto **y<sub>i</sub>** em rela√ß√£o a **reta de melhor ajuste**:

[students-error-OLS.py](src/students-error-OLS.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
b = df['Salary'].mean() - (m * df['Grade'].mean())

df['y = mx + b'] = [(m*x) + b for x in df['Grade']]
df['y_i - y = mx + b'] = df['Salary'] - df['y = mx + b']
df['(y_i - y = mx + b)^2'] = df['y_i - y = mx + b'] ** 2

newDF = df[['Grade', 'Salary', 'y = mx + b', 'y_i - y = mx + b', '(y_i - y = mx + b)^2']]

print(newDF)
print("Sum of Squared Errors (OLS): ", round(sum(newDF['(y_i - y = mx + b)^2'])))
```

**OUTPUT**
```python
    Grade  Salary     y = mx + b  y_i - y = mx + b  (y_i - y = mx + b)^2
0      50   50000   70075.255242     -20075.255242          4.030159e+08
1      50   54000   70075.255242     -16075.255242          2.584138e+08
2      46   50000   64010.703700     -14010.703700          1.962998e+08
3      95  189000  138301.460094      50698.539906          2.570342e+09
4      50   55000   70075.255242     -15075.255242          2.272633e+08
5       5   40000    1849.050390      38150.949610          1.455495e+09
6      57   59000   80688.220441     -21688.220441          4.703789e+08
7      42   42000   57946.152157     -15946.152157          2.542798e+08
8      26   47000   33687.945987      13312.054013          1.772108e+08
9      72   78000  103430.288725     -25430.288725          6.466996e+08
10     78  119000  112527.116039       6472.883961          4.189823e+07
11     60   95000   85236.634098       9763.365902          9.532331e+07
12     40   49000   54913.876386      -5913.876386          3.497393e+07
13     17   29000   20042.705017       8957.294983          8.023313e+07
14     85  130000  123140.081238       6859.918762          4.705849e+07
Sum of Squared Errors (OLS):  6958885882
```

Agora a *Soma dos Erros* foi reduzida significativamente de **26.501.600.000** para **6.958.885.882**. Isso porque n√≥s estamos utilizando uma reta de melhor ajuste que tem os melhores valores de **m** e **b** para esse conjunto de dados, n√£o apenas subtraindo os pontos **(x<sub>i</sub>, y<sub>i</sub>)** pelo a m√©dia de todos os **y**.

**NOTE:**  
Mas essa solu√ß√£o n√£o √© escalon√°vel... Aplicar isso √† Regress√£o Linear foi bastante f√°cil, pois t√≠nhamos bons coeficientes e equa√ß√µes lineares, mas aplicar isso a algoritmos complexos e n√£o lineares como *Support Vector Machine* n√£o seria vi√°vel. Ent√£o vamos encontrar a aproxima√ß√£o num√©rica desta solu√ß√£o por um **m√©todo iterativo**.

---

<div id='04'></div>

## 04 - M√©todo do Gradiente Descendente

![image](images/scared.gif)  

> Calma gente, rss... Eu sei que esse nome **"Gradiente Descendente"** assusta todo mundo, mas voc√™s v√£o ver como √© simples. √â tudo quest√£o de saber quando e como ele √© aplicado.

Ent√£o, antes n√≥s estavamos utilizando o **M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)** que constru√≠a uma **reta de melhor ajuste** a partir de 2 f√≥rmulas... Agora n√≥s vamos ter que criar a mesma reta, por√©m sem aquelas f√≥rmulas. Isso porque n√≥s vamos utilizar uma **abordagem iterativa**.

Para come√ßar, vamos imaginar a seguinte hist√≥ria:

Considere que 20 pessoas (incluindo voc√™) s√£o lan√ßadas aleatoriamente no ar em uma cadeia de montanhas. Sua tarefa √© encontrar **o pico mais alto de todas as montanhas** em 30 dias.

![image](images/gd-01.jpg)  

Cada um de voc√™s tem um *walkie talkie* para se comunicar e um *alt√≠metro* para medir a altitude. Todos os dias voc√™s passam horas localizando o pico mais alto poss√≠vel e relatam sua altitude mais alta do dia a todos os outros.

![image](images/gd-02.png)  

Suponha que no dia 1 voc√™ relate 1000 p√©s; Outra pessoa informa 1230 p√©s e assim por diante. Depois, h√° uma pessoa que reporta 5000 p√©s, que √© o m√°ximo de todos.

Lembre-se de que sua tarefa era atingir coletivamente o pico mais alto de todas as montanhas. O que voc√™ faz a seguir no dia 2? No dia seguinte, todos se reunir√£o em dire√ß√£o √† √°rea onde a altitude m√°xima foi encontrada ontem. Eles pensar√£o que √© prov√°vel que o pico mais alto seja nesta √°rea.

> Por que algu√©m que reportou 500 p√©s ontem procuraria naquela √°rea de novo se h√° outra √°rea que j√° tem 5.000 p√©s?

Portanto, todos os pesquisadores se movem **"rapidamente"** em dire√ß√£o ao ponto mais alto relatado. Agora, essa gan√¢ncia pode levar voc√™ ao pico mais alto de todas as montanhas, mas tamb√©m pode levar a um erro enorme... O cara que estava a 500 p√©s ontem poderia ter ido em dire√ß√£o a um pico que tinha uma altura de 10.000 p√©s! E ele o ignorou e foi em dire√ß√£o aquele de 5.000 p√©. Voc√™ realmente fica preso em um **M√°ximo Local**. E n√£o h√° como saber se voc√™ est√° preso no **M√°ximo Local**.

**Simulated Annealing:**  
Existe um Algoritmo que seria √∫til para n√≥s nesse caso, **Simulated Annealing:**, onde os pesquisadores teriam pesquisado todo o espa√ßo de pesquisa minuciosamente e sem serem tendenciosos para provavelmente encontrar os **m√°ximos globais**.

Vou deixar alguns exemplos visuais de **Simulated Annealing** para deixar mais claro:

![image](images/simulated-annealing-00.gif)  

---

![image](images/simulated-annealing-01.gif)  

---

![image](images/simulated-annealing-03.gif)  

---

![image](images/simulated-annealing-02.gif)  

Voltando para o problema das montanhas... Se tivesse como equacionar esse problema das Montanhas, ou seja, transforma isso em Matem√°tica n√≥s s√≥ precisar√≠amos tentar chegar (ou encontrar) o ponto **M√°ximo Global** da fun√ß√£o, tentando evitar ficar preso em *M√°ximos Locais*.

Agora vamos voltar para o nosso problema de **Regress√£o Linear**, onde, n√≥s quer√≠amos criar uma **reta de melhor ajuste** utilizando a *Equa√ß√£o da Reta*:

![image](images/0001.png)  

**NOTE:**  
Por√©m, vale lembrar que n√≥s n√£o vamos mais utilizar aquelas f√≥rmulas m√°gicas para tentar encontrar bons valores dos coeficientes **m** e **b** para o nosso conjunto de dados. Isso porque n√≥s vamos utilizar uma **abordagem iterativa**.

**Como assim uma Abordagem Iterativa?**  
Resumidamente, n√≥s vamos ficar tentando valores para os coeficientes **m** e **b** at√© achar a **reta de melhor ajuste** para o nosso conjunto de dados.

Um exemplo bem abstrado (e maluco) poderia ser o seguinte:

![image](images/example-01.png)  

Mas se pensarmos bem, n√≥s resolvemos um problema e agora temos outro *(mesmo que mais simples)*, que √© ficar tentando v√°rios valores para os coeficientes **m** e **b** at√© achar a **reta de melhor ajuste** para o nosso conjunto de dados.

![image](images/ELVN.gif)  

Ent√£o, temos um problema hein? J√° pensou ter que criar 1 milh√£o de retas o tamanho do recurso computacional que seria gasto?

Ok temos um problema, como escolher os melhores valores poss√≠veis para os meus **m** e **b**? Bem, pensem comigo nas seguintes abordagens:

 - **Primeira abordagem:** N√≥s podemos ir alterando valores de pouco em pouco, por exemplo, 1 unidade por teste em **m** e **b**:
   - O problema √© que se tivermos muito longe dos melhores valores para **m** e **b** isso vai exigir muito recurso computacional.
 - **Segunda abordagem:** Essa abordagem √© muito simples, vamos aumentar valores muito grandes para testes em **m** e **b**. Por exemplo, 1000, 500, 300, 100...

**Qual a melhor abordagem?  AS DUAS!!!**  
√â isso mesmo... Pense comigo, se eu aumentar grandes unidades (valores) para os meus **m** e **b**; E a medida que eles v√£o se aproximando da melhor reta poss√≠vel eu posso ir diminuindo esses valores at√© chegar na **reta de melhor ajuste** para o nosso conjunto de dados.

Ou seja:

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto.

![image](images/problem.gif)  

Ent√£o, o **Gradiente Descendente** √© o bixo de sete cabe√ßas que consegue aplicar esse conceito que n√≥s aprendemos agora. Mas como?

Primeiro vamos ver aqui a matem√°tica das coisas n√©:

**Equa√ß√£o da Reta - (Regress√£o Linear)**  
![image](images/linear-regression-formule-02.png)  

Agora vamos pegar o nosso gr√°fico de compara√ß√£o entre **notas** de alunos e seus **sal√°rios**:

![image](images/plot-01.png)  

Suponha que n√≥s selecionamos um desses pontos, o ponto **y<sub>i</sub>**, algo parecido com isso:

![image](images/example-02.png)  

Agora suponha que n√≥s pegamos a *equa√ß√£o da reta* e desenhamos uma reta qualquer:

![image](images/example-03.png)  

Ent√£o, agora n√≥s estamos com a seguinte situa√ß√£o:
 - Um ponto **y<sub>i</sub>** selecionado;
 - E criamos uma *equa√ß√£o reta* com valores aleat√≥rios para os coeficientes **m** e **b**.

Agora vem a pergunta chave:

> Como eu sei qu√£o distante essa reta est√° dos meus dados? - Todos eles!

Pode parecer muito dif√≠cil, mas √© simples:

 - **1¬™ -** Eu vou pegar o meu **x<sub>i</sub>**
 - **2¬™ -** Ver qual foi o resultado da equa√ß√£o da reta nesse ponto.
 
N√£o entendeu? Vamos ver isso visualmente:

![image](images/example-04.png)  

Veja que agora n√≥s temos outro ponto de intersec√ß√£o, por√©m esse vai representar minha **(regress√£o<sub>i</sub>)**. Mas o que isso tem a ver com a nossa pergunta chave?

> Como eu sei qu√£o distante essa reta est√° dos meus dados? - Todos eles!

Bem, o ideal seria que essa *equa√ß√£o da reta* passasse bem no nosso ponto **y<sub>i</sub>**,mas como podemos ver tem uma dist√¢ncia entre o meu ponto **y<sub>i</sub>** e a minha **regress√£o<sub>i</sub>**.

U√©, mas se temos uma dist√¢ncia entre esses dois pontos √© s√≥ calcular essa dist√¢ncia n√£o √©?

![image](images/genius.gif)

Ou seja, a diferen√ßa entre a minha **regress√£o<sub>i</sub>** e o meu ponto **y<sub>i</sub>**:

![image](images/example-05.png)  

**Agora vamos ver alguns detalhes aqui:**

 - **1¬™ -** Essa dist√¢ncia entre o meu ponto **y<sub>i</sub>** e a minha **regress√£o<sub>i</sub>** √© o que n√≥s conhecemos como:
   - **Erro para esse ponto y<sub>i</sub>.**
 - **2¬™ -** N√≥s vamos ter que sair medindo esse erro para todos os pontos do nosso gr√°fico:
   - Ou seja, n√≥s vamos tirar o erro para todas os nossos **y<sub>i</sub>** em rela√ß√£o a essa *reta que n√≥s criamos*.
 - **3¬™ -** E por fim, fazer a soma de todos os erros para essa reta:
   - A soma de todos os erros vai n√≥s da o ***tamanho do nosso erro para essa reta***:
     - Quanto *maior* esse valor, *maior vai ser nosso erro*;
     - Quanto *menor* esse valor, *menor vai ser nosso erro*.

**Ok, mas como eu posso equacionar isso?**  
Bem isso √© o que n√≥s conhecemos como **Fun√ß√£o de Custo**, **Loss Function**, **L**, **J**... Existem v√°rias abordagens.

Alguns exemplos s√£o:

**Example-01:**  
![image](images/01.png)  

**Example-02:**  
![image](images/cf-function-01.png)

**Example-03:**  
![image](images/cf-2m.png)

Essas fun√ß√µes v√£o variar de acordo com o seu problema, Algumas utilizam:

 - **RMSE**;
 - **MSE**;
 - **MAE**
 - **ErroM√©dio**... etc.

Como n√≥s estamos focando apenas em somar todos os nossos erros e mais nada, vamos ficar com o primeiro exemplo **(Example-01)** de **Fun√ß√£o de Custo**.

<div id='05'></div>

## 05 - Tentando minimizar a Fun√ß√£o de Custo

Ok, n√≥s j√° temos a nossa **Fun√ß√£o de custo** que vai fazer a soma de todos os nossos erros:

![image](images/01.png)  

Recapitulando:

 - 1¬™ - N√≥s tra√ßamos uma reta;
 - 2¬™ - Calculamos o *erro* para cada ponto na reta;
 - 3¬™ - Por fim, fizemos a *soma de todos os erros* e conseguimos ***tamanho do nosso erro***.

At√© ai tudo bem, calculamos o ***tamanho do nosso erro*** para uma reta, mas lembra que n√≥s tinhamos uma abordagem?

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto - Para encontrar os melhores valores de **m** e **b**.

Ou seja, n√≥s vamos construir outra reta que n√≥s d√™ um valor menor para a nossa **Fun√ß√£o de Custo**.  
Mas como? Simples, n√≥s vamos passar novos valores para os coeficientes **m** e **b**: 

![image](images/02.png)  

Olhando para a abstra√ß√£o acima n√≥s temos que as nossas vari√°veis **m** e **b** na *equa√ß√£o da reta* v√£o receber novos valores; que v√£o ser *subtra√≠dos* de **algo (something)**. Mas que algo √© esse?  

> Ent√£o, √© ai que entra o conceito de **Derivadas**, **Derivadas Parciais** e **M√≠nimos de uma Fun√ß√£o**.

Suponha que a nossa *equa√ß√£o da reta* tem o seguinte gr√°fico (√© s√≥ um exemplo):

![image](images/example-06.png)  

Bem, no exemplo acima n√≥s temos uma par√°bola (√© s√≥ um exemplo) com v√°rias Taxas de Varia√ß√£o e n√≥s estamos interessando nos **pontos m√≠nimos da fun√ß√£o** para as nossas vari√°veis **m** e **b**.

Agora sabendo disso e da nossa abordagem n√≥s vamos:

 - Derivar a nossa **Fun√ß√£o de Custo** para **m** e para **b**:
   - Dando grandes passos quando estivermos muito longe do ponto m√≠nimo;
   - E passos curtos quando estivermos muito pr√≥ximos do ponto m√≠nimo.

Ou seja, como n√≥s temos mais de uma vari√°vel, n√≥s vamos aplicar o conceito de **Derivadas Parciais**, onde, n√≥s Derivamos para uma vari√°vel e deixamos a outra como constante e depois fazemos o mesmo para a outra e vamos diminuindo at√© achar os pontos m√≠nimos poss√≠veis para os coeficientes **m** e **b**:

![image](images/der01.png)  

Ahh, entendido, mas tem 2 observa√ß√µes na abstra√ß√£o acima:

 - 1¬™ - N√≥s temos 2 constantes **Œ± (Alpha)** e **Œ≤ (Beta)** que est√£o multiplicando as nossas Derivadas Parciais;
 - 2¬™ - N√≥s estamos Derivando a nossa **Fun√ß√£o de Custo** para as vari√°veis **m** e **b** da *equa√ß√£o da reta* - **(What?)**.

<div id='06'></div>

## 06 - A Regra da Cadeia

> Por enquanto (mas s√≥ por enquanto) vamos ignorar as constantes **Œ± (Alpha)** e **Œ≤ (Beta)** e vamos relembrar um conceito matem√°tico chamado: **Regra da Cadeia**.

Em c√°lculo, a **Regra da Cadeia** √© uma f√≥rmula para a derivada da fun√ß√£o composta de duas fun√ß√µes - **What?**  
Ok, vamos para os exemplos, suponha que n√≥s temos as seguintes fun√ß√µes:

![image](images/der02.png)  

Veja que agora n√≥s temos 2 fun√ß√µes **y** e **z**, onde:

 - O meu **y** depende de **z**;
 - E o meu **z** depende de **x**.

Agora eu quero Derivar a minha fun√ß√£o **y** para **x**:

![image](images/der03.png)  

U√©, como vai ser? Se o meu **y** depende do **z**; e o meu **z** depende do **x**?

**A REGRA DA CADEIA:**  
√â ai que entra o conceito da **Regra da Cadeia**. Basta seguir a f√≥rmula abaixo:

![image](images/der04.png)  

Agora √© s√≥ tirar a Derivada de **y** para **z**; e de **z** para **x**; e depois multiplicar elas... Vai ficar assim:

![image](images/der05.png)  

**NOTE:**  
√â importante lembrar que algumas dessas vari√°veis tem rela√ß√µes com outra, por exemplo, o meu **z = 3x**, por isso n√≥s mudamos isso na hora de trabalhar a equa√ß√£o.

Mas no fim o que importa √© que a Derivada da fun√ß√£o **y** em rela√ß√£o a **x** √© **18x**.

<div id='07'></div>

## 07 - Aplicando a Regra da Cadeia na Fun√ß√£o de Custo

Ok, agora que n√≥s j√° revisamos o conceito da **Regra da Cadeia**, como eu posso aplicar ela na minha **Fun√ß√£o de Custo** para os coeficientes **m** e **b**? Bem, vamos ver as rela√ß√µes n√©?

Primeiro vamos pegar nossa **Fun√ß√£o de Custo**:

![image](images/01.png)  

Ok, dentro da *Fun√ß√£o de Custo* n√≥s temos a fun√ß√£o **reg<sub>i</sub>** que depende de **m**; E a minha *Fun√ß√£o de Custo* que depende de **reg<sub>i</sub>**. U√©, s√≥ aplicar a Regra da Cadeia ent√£o...

**NOTE:**  
Primeiro vamos apelidar na nossa fun√ß√£o **(reg<sub>i</sub> + y<sub>i</sub>)<sup>2</sup>** de ***error*** para ficar uma abstra√ß√£o mais nominal.

Agora √© s√≥ tirar a Derivada da **Fun√ß√£o de Custo** para **m** seguindo a *Regra da Cadeia*:

![image](images/der06.png)  

 - Veja que a minha **Fun√ß√£o de Custo** depende do **error**; e o meu **error** depende de **m**;
 - Agora √© s√≥ tirar a Derivada da minha **Fun√ß√£o de Custo** para o **erro**; e do meu **erro** para **m** e depois multiplicar:

![image](images/der07.png)  

Simplificando, a Derivada da minha **Fun√ß√£o de Custo** para **m** vai ser :

![image](images/der08.png)  

√ìtimo, agora √© s√≥ tirar a Derivada da **Fun√ß√£o de Custo** em rela√ß√£o a **b**:

![image](images/der09.png)  

Simplificando novamente:

![image](images/der10.png)  

**Pronto, resolvido!**  
Agora n√≥s j√° temos as Derivadas da **Fun√ß√£o de Custo** para **m** e **b**:

![image](images/der11.png)  

Como os nossos **x<sub>i</sub>** e **y<sub>i</sub>** s√£o valores que n√≥s j√° temos no gr√°fico (nossos dados) n√≥s s√≥ vamos Derivando os coeficientes **m** e **b** at√© chegar o mais pr√≥ximo poss√≠vel do m√≠nimo da **Fun√ß√£o de Custo**, seguindo a nossa abordagem:

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto.

![image](images/gd-graph.gif)  

---

<div id='08'></div>

## 08 - Taxa de Aprendizagem (Learning Rate)

Ok pessoas... Voltando alguns passos atr√°s, lembram que n√≥s tinhamos umas constantes que multiplicavam nossas Derivadas da **Fun√ß√£o de Custo** para **m** e **b**? Eram as constantes **Œ± (Alpha)** e **Œ≤ (Beta)**, mas o que elas significavam?

Ent√£o, isso √© o que n√≥s chamamos de **Taxa de Aprendizado (Learning Rate)**, respons√°veis por ajuste e determina o tamanho da etapa em cada itera√ß√£o enquanto se move em dire√ß√£o ao **M√≠nimo de Fun√ß√£o de Custo**.

A **Taxa de Aprendizado (Learning Rate)** √© um hiperpar√¢metro que controla o quanto alterar o modelo em resposta ao erro estimado cada vez que os pesos do modelo s√£o atualizados. Escolher a taxa de aprendizado √© desafiador, pois um valor muito pequeno pode resultar em um longo processo de treinamento que pode travar, enquanto um valor muito grande pode resultar no aprendizado de um conjunto sub-√≥timo de pesos muito r√°pido ou um processo de treinamento inst√°vel.

Seguindo a nossa abordagem de *Gradiente Descendente* a *Taxa de Veria√ß√£o* vai determina o tamanho dos nossos passos em dire√ß√£o ao **M√≠nimo** da **Fun√ß√£o de Custo**.

Veja os exemplos abaixo:

**Exemplo-01:**  
![image](images/learning-rate-01.jpg)  

**Exemplo-02:**  
![image](images/learning-rate-02.jpg)  

---

<div id='09'></div>

## 09 - Aplicando o M√©todo do Gradiente Descendente na pr√°tica

Ok, mas depois de todas essas **bruxarias te√≥ricas**, como colocar tudo isso em pr√°tica? Bem, vamos ver isso agora com Python sem precisar importar nenhuma biblioteca (mas s√≥ por agora):

[students-gd-bestLine.py](src/students-gd-bestLine.py)  
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

m = 7
b = 1
learning_rate = 0.000001

for i in range(1, 1000+1, 1):
  y_pred = m*df['Grade'] + b
  m_derivative = sum(2*df['Grade']*(y_pred - df['Salary']))
  b_derivative = sum(2*(y_pred - df['Salary']))
  m = m - (learning_rate * m_derivative)
  b = b - (learning_rate * b_derivative)
  # print(m, b) # Remove comments to view step-by-step.

print("\nAngular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))

regression_line = [(m*x) + b for x in df['Grade']]

plt.figure(figsize=(10, 7))
plt.scatter(df.Grade, df.Salary, color='g')
plt.plot(df.Grade, regression_line, color='b')
plt.title('Grades vs Salaries | Gradient descent Approach')
plt.xlabel('Grade')
plt.ylabel('Salary')
plt.grid()
plt.savefig('../images/plot-03.png', format='png')
plt.show()
```

**OUTPUT:**  
```python
Angular Coefficient (m): 1425
Linear Coefficient (b): -7
```

![image](images/plot-03.png)

**NOTE:**  
Veja que com o **M√©todo do Gradiente Descendente** n√≥s conseguimos melhores valores para os coeficientes **m** e **b** em rela√ß√£o ao **M√©todo dos M√≠nimos Quadrados Ordin√°rios (OLS)**:

 - **M√©todo dos M√≠nimos Quadrados Ordin√°rios (OLS)**:
   - Angular Coefficient (m): 1516
   - Linear Coefficient (b): -5732.0
 - **M√©todo do Gradiente Descendente**:
   - Angular Coefficient (m): 1425
   - Linear Coefficient (b): -7

---

<div id='10'></div>

## 10 - Criando uma Reta de melhor ajuste com com Scikit-Learn

> Ok, at√© aqui j√° vimos todas essas bruxarias matem√°ticas e te√≥ricas, mas como resolver isso na pr√°tica **(E de maneira simples √© claro!)**?

Gra√ßas ao Python e a maravilhosa comunidade **Open-Source** n√≥s temos a biblioteca [Scikit-Learn](https://scikit-learn.org/stable/index.html) que deixa todo esse trabalho **MUITO F√ÅCIL**! Vamos come√ßar instalando a biblioteca:

```
pip install scikit-learn --upgrade
```

**NOTE:**  
Se voc√™ estiver utilizando um ambiente virtual (assim como eu) √© recomendado salvar essa depend√™ncia:

```
pip freeze > requirements.txt
```

Ok, agora vamos ver a nossa vers√£o atual do Scikit-Learn?

[checkVersion.py](src/checkVersion.py)
```python
def checkVersion():
  import sklearn
  print('Scikit-Learn Version: {0}'.format(sklearn.__version__))

if __name__ =='__main__':
  checkVersion()
```

**OUTPUT:**  
```
Scikit-Learn Version: 0.23.1
```

√ìtimo, tudo lindo e maravilhoso a nossa disposi√ß√£o! Mas como eu crio um exemplo de **Regress√£o Linear**?  
Simples, primeiro vamos criar um conjunto de dados aleat√≥rios (mesmo sem sentido) para representar o nosso conjunto de dados:

[make_sample.py](src/make_sample.py)
```python
def createRegression(samples, variavel_numbers, n_noise):
  from sklearn.datasets import make_regression
  x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
  return x, y

if __name__ =='__main__':
  from matplotlib import pyplot as plt

  reg = createRegression(200, 1, 30)

  plt.figure(figsize=(10, 7))
  plt.scatter(*reg)
  plt.title('Linear Regression Sample')
  plt.savefig('../images/plot-04.png', format='png')
  plt.show()
```

**OUTPUT:**  
![IMAGE](images/plot-04.png)  

A biblioteca **Scikit-Learn** tem um m√©todo chamado [make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) que pode ser usado para criar um conjunto de dados para que possamos trabalhar com eles. Os argumentos mais comuns (que foram o que n√≥s utilizamos) para esse m√©todo s√£o:

 - **n_samples:** O n√∫mero de Amostra de Dados;
 - **n_features:** O n√∫mero de vari√°veis/caracter√≠sticas;
 - **noise:** Quanto ru√≠do ter√° nosso gr√°fico, ou seja, qu√£o bagun√ßado vai est√° os dados.

**NOTE:**  
Toda vez que voc√™ rodar o c√≥digo [make_sample.py](src/make_sample.py) vai ser gerado um conjunto de dados diferentes. Ou seja, vai ser outro gr√°fico/plot.

**NOTE:**  
Outra observa√ß√£o importante que voc√™ deve prestar aten√ß√£o √© que a fun√ß√£o [make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) nos retornou 2 conjuntos de dados - **x** e **y**, ou seja:

```python
x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
```

 - Os valores aleat√≥rios criados para o nosso **eixo-x**;
 - E os seus correspondente no **eixo-y**.

√ìtimo, agora voltando para o nosso problema (Regress√£o Linear):

> Como crio uma **reta de melhor ajuste** com Scikit-Learn?

D√° para fazer isso de forma autom√°tica e simples? Claro, com **Scikit-Learn** e suas bruxarias:

[reg-v1.py](src/reg-v1.py)
```python
def createRegression(samples, variavel_numbers, n_noise):
  from sklearn.datasets import make_regression
  x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
  return x, y

if __name__ =='__main__':

  from sklearn.linear_model import LinearRegression
  from matplotlib import pyplot as plt

  reg = createRegression(200, 1, 30)
  model = LinearRegression() # Linear Regression Instance.

  model.fit(*reg)

  a_coeff = model.coef_ # Angular Coefficient - m
  l_coeff = model.intercept_ # Linear Coefficient - b

  print('Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}'.format(a_coeff, l_coeff))

  plt.figure(figsize=(10, 7))
  plt.scatter(*reg)
  plt.plot(reg[0], a_coeff*reg[0] + l_coeff,color='red')
  plt.savefig('../images/plot-05.png', format='png')
  plt.show()
```

**OUTPUT:**  
```python
Angular Coefficient (m): [34.34928509]
Linear Coefficient (b): -0.23447017542042375
```

![IMAGE](images/plot-05.png)  

**Lindo n√£o?**  
Vou comentar apenas as partes cruciais de agora em diante... Come√ßando por aqui:

```python
model.fit(*reg)
```

√â aqui que entra a m√°gica de achar os melhores valores para **m** e **b**. J√° est√° tudo pronto, n√≥s s√≥ precisamos importar a classe [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) e depois utilizar o m√©todo **fit()** que √© respons√°vel por conseguir os melhores valores para os nossos *Coeficientes* **Angular** e **Linear**.

Pronto, agora que j√° temos os melhores valores para **m** e **b**, vamos pegar esses valores com os atributos **coef_** e **intercept_** e imprimir os mesmos:

```python
a_coeff = model.coef_ # Angular Coefficient - m
l_coeff = model.intercept_ # Linear Coefficient - b
print('Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}'.format(a_coeff, l_coeff))
```

E por fim, vamos gerar a parte visual:

 - Primeiro vamos criar um Scatter Plot para exibir nosso conjunto de dados (os dados de amostra);
 - E depois adiciona uma **reta de melhor ajuste**.

```python
plt.scatter(*reg)
plt.plot(reg[0], a_coeff*reg[0] + l_coeff,color='red')
```

**NOTE:**  
Lembrem que para criar essa **reta de melhor ajuste** n√≥s utilizamos a *Equa√ß√£o da Reta*:

![image](images/linear-regression-formule.png)  

```python
m = a_coeff
x = x[0]
b = l_coeff
```

<div id='11'></div>

## 11 - Dividindo os dados em Treino e Teste

> Uma coisa que voc√™s tem que entender primeiro √© que os modelos de **Machine Learning** aprendem a partir de dados. Sabendo disso √© interessante dividir nosso conjunto de dados em **Dados de Treino** & **Dados de Teste**.

**DADOS DE TREINO:**  
Ok, suponha que n√≥s queremos desenvolver um programa (modelo) que identifique se uma imagem **√© um cachorro** ou **n√£o √© um cachorro**.

De in√≠cio n√≥s vamos receber um conjunto (amostra) com v√°rias imagens de cachorros, depois n√≥s vamos pegar uma parte desse conjunto (normalmente 70%) e dar para o nosso modelo aprender identificando caracter√≠sticas comuns entre cachorros.

**DADOS DE TESTE:**  
Ok, N√≥s reservamos 70% do nosso conjunto de dados (amostra) para o nosso algoritmo aprender e os outros 30%?  
Ent√£o, esses s√£o os **Dados de Testes**. N√≥s vamos passar os dados de testes para o nosso modelo e ver qu√£o bem ele est√° aprendendo. Por exemplo:

> Isso aqui √© um cachorro?

![image](images/dog.gif)  

E o nosso modelo vai ter que dar um retorno dizendo se √© um cachorro ou n√£o.

**NOTE:**  
Viram como √© interessante dividir o conjunto de dados (amostra) em **treino** e **teste**? Outro exemplo, seria identificar uma doen√ßa em pacientes, como n√≥s saber√≠amos se nosso modelo aprendeu (ou est√° aprendendo) bem se deixar ele aprender com todo o conjunto de dados?

> Por isso, ele vai aprender com uma parte (70% no nosso caso) e vamos reserva outra parte (30% no nosso caso) para testar e ver qu√£o bem ele (nosso modelo) est√° aprendendo.

[reg-v2.py](src/reg-v2.py)
```python
def createRegression(samples,variavel_numbers, n_noise):
  from sklearn.datasets import make_regression
  x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
  return x, y

if __name__ =='__main__':

  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import train_test_split
  from matplotlib import pyplot as plt

  reg = createRegression(200, 1, 30)
  model = LinearRegression()

  # Divide the data into Training and Testing - 30% for testing.
  x_train, x_test, y_train, y_test = train_test_split(reg[0], reg[1], test_size=0.30)

  # Just the training data is transferred to the fit() function (which finds the best values ‚Äã‚Äãfor m and b).
  model.fit(x_train, y_train)

  a_coeff = model.coef_ # Angular Coefficient - m
  l_coeff = model.intercept_ # Linear Coefficient - b
  print('Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}'.format(a_coeff, l_coeff))

  # Create plot.
  plt.figure(figsize=(10, 7))
  plt.subplot(211)
  plt.scatter(reg[0], reg[1])
  plt.title('Complete Sample')
  plt.plot(x_train, a_coeff*x_train + l_coeff,color='red')
  plt.subplot(223)
  plt.scatter(x_train, y_train)
  plt.title('Training Set (70%)')
  plt.plot(x_train, a_coeff*x_train + l_coeff,color='blue')
  plt.subplot(224)
  plt.scatter(x_test, y_test)
  plt.title('Testing set (30%)')
  plt.plot(x_train, a_coeff*x_train + l_coeff,color='green')
  plt.savefig('../images/plot-06.png', format='png')
  plt.show()
```

**OUTPUT:**  
```python
Angular Coefficient (m): [4.65851642]
Linear Coefficient (b): 0.760506738734484
```

![image](images/plot-06.png)

Agora vamos comentar s√≥ as partes cruciais que foram utilizadas para dividir os dados em **treino** e **teste**. Primeiro n√≥s importamos o m√©todo **train_test_split()**.

```python
from sklearn.model_selection import train_test_split
```

Depois n√≥s passamos os seguintes argumentos para esse m√©todo:

 - **1¬™ -** Os dados no eixo-x do conjunto de dados;
 - **2¬™ -** Os seus correspondentes no eixo-y;
 - **3¬™ -** Por fim, quanto n√≥s reservamos dos dados para teste: **test_size=0.30 = 30%**.

```python
x_train, x_test, y_train, y_test = train_test_split(reg[0], reg[1], test_size=0.30)
```

**NOTE:**  
Veja que o m√©todo **train_test_split()** retorna os dados j√° separados *(aleatoriamente)* em dados de treino e teste.

Agora por fim, n√≥s vamos treinar o nosso m√≥delo apenas com os dados de treino *(como foi explicado anteriormente)*:

```python
model.fit(x_train, y_train)
```

<div id='12'></div>

## 12 - Pegando o Coeficiente de Determina√ß√£o R<sup>2</sup> com Scikit-Learn

Bem, antes de pegar o **Coeficiente de Determina√ß√£o R<sup>2</sup>** com *Scikit-Learn*, voc√™ sabe que *bixo* √© esse? N√£o? Ok, vamos para uma breve explica√ß√£o...

Suponha que n√≥s criamos um gr√°fico com alguns dados para ver a rela√ß√£o entre pre√ßos de uma casa e seus tamanhos, ficou algo parecido com isso **(e n√£o muito bonito)**:

![image](images/house-01.png)  

Se voc√™ prestar aten√ß√£o vai ver que o nosso gr√°fico tem uma varia√ß√£o crescente, ou seja:

> A medida que aumenta o pre√ßo o tamanho tamb√©m aumenta - vice e versa.

Agora suponha que eu quero criar um modelo que use uma reta para representar esses dados, de maneira que se eu inserir um novo pre√ßo ele tente descobrir (prever) qual o tamanho da casa.

**NOTE:**  
A primeira ideia que n√≥s vamos ter √© calcular a m√©dia dos tamanhos e tra√ßar uma reta. Suponha que a reta ficou assim:

![image](images/house-02.png)  

Bem, essa reta n√£o representa muito bem esse modelo. Se voc√™ prestar aten√ß√£o vai ver que temos bastante erro. Como n√≥s poder√≠amos calcular o erro desse m√≥delo?

 - **1¬∫ -** √â s√≥ pegar cada um dos valores (pontos no gr√°fico);
 - **2¬∫ -** Calcular a dist√¢ncia para a minha reta:
   - Cada ponto voc√™ vai elevar ao quadrado;
   - E depois somar com o pr√≥ximo ponto.

No gr√°fico as dist√¢ncias dos pontos para a reta voc√™ pode ver assim:

![image](images/house-03.png)  

> Isso √© o que n√≥s conhecemos como **Soma dos Quadrados Totais - SQ<sub>t</sub>**

**NOTE:**  
Na verdade o que n√≥s fizemos acima foi tirar a *vari√¢ncia* dos nossos dados.

Continuando... Agora suponha que eu criei um novo modelo, por√©m com uma reta que parece se alinhar melhor com esses dados, veja abaixo:

![image](images/house-04.png)  

Ok, s√≥ de olhar j√° da para ver que essa reta representa bem melhor esses dados do que apenas tirar a m√©dia dos erros de todos os dados.

> A final ela parece est√° crescendo a mesma taxa que esses dados est√£o crescendo.

Mas, como eu posso provar que realmente essa segunda reta est√° melhor do que a outra? U√©, √© s√≥ calcular cada uma dessas dist√¢ncias entre nossos dados e a reta verde (nova reta):

![image](images/house-05.png)  

> Isso √© o que n√≥s conhecemos como **Soma dos Quadrados dos Res√≠duos - SQ<sub>res</sub>**

Ok, se n√≥s calcularmos os dados n√≥s vamos ver que o meu **SQ<sub>res</sub>** √© menor do que o **SQ<sub>t</sub>**. Ou seja, o meu **SQ<sub>res</sub>** est√° melhor ajustado.

**NOTE:**  
Mas como eu sei qu√£o melhor est√° o meu SQ<sub>res</sub> em rela√ß√£o ao SQ<sub>t</sub>? - **Ou seja, qu√£o melhor ele est√° em rela√ß√£o a m√©dia**?

**Coeficiente de determina√ß√£o R<sup>2</sup>:**  
Ent√£o, √© ai que entra o nosso querido **R<sup>2</sup>**... O R<sup>2</sup> nada mais √© do que o meu SQ<sub>t</sub> menos SQ<sub>res</sub> dividido pelo SQ<sub>t</sub>:

![image](images/r2.png)  

Mas o que essa f√≥rmula realmente significa na pr√°tica?

 - **N√∫merador:** Bem, no numerador n√≥s vemos qu√£o bem o meu modelo **SQ<sub>res</sub>** √© em rela√ß√£o ao **SQ<sub>t</sub>**;
- **Denominador:** Quando n√≥s dividimos pelo **SQ<sub>t</sub>** n√≥s estamos `normalizando`, ou seja, n√≥s estamos trazendo esse valor para uma escala que vale entre *zero* e *um*.

**Mas por que normalizar entre *zero* e *um*?**  
Ok, vamos ver... Suponha que n√≥s criamos um modelo de Machine Learning muito ruim que criou uma reta que foi igual a soma dos quadrados totais **SQ<sub>t</sub>**, mais ou menos isso:

![image](images/house-06.png)  

U√©, o meu **SQ<sub>res</sub>** vai ser igual ao meu **SQ<sub>t</sub>**, logo, o meu **R<sup>2</sup>** vai ser **zero (0)**:

![image](images/r2-01.png)  

**NOTE:**  
Agora vamos imaginar outro cen√°rio (√© s√≥ exemplo) onde os meus dados est√£o distribu√≠dos de uma forma onde meu modelo de Machine Learning passe exatamente por todos os dados, ou seja, **n√£o teve nenhum erro**:

![image](images/house-07.png)  

Ent√£o, como o nosso **SQ<sub>res</sub>** n√£o teve nenhum erro, qual vai ser nosso **R<sup>2</sup>** agora?

![image](images/r2-02.png)  

√ìtimo, mas o que o nosso **R<sup>2</sup>** nos diz?

 - Quanto **maior (mais perto de 1) o R<sup>2</sup> melhor** vai ser o meu cen√°rio;
 - Quanto **menor (mais perto de 0) o R<sup>2</sup> pior** vai ser o meu ceun√°rio.

Mas como n√≥s poder√≠amos interpretar o nosso **R<sup>2</sup>**? Por exemplo:

> Eu criei um modelo de Machine Learning que gerou o **R<sup>2</sup>** de 0,87.

O que isso significa?

> Significa que o meu modelo de Machine Learning √© 87% melhor do que simplesmente pegar a m√©dia dos valores.

Outra abordagem de interpreta√ß√£o √© dizer que o meu modelo explica **87% da vari√¢ncia dos dados**. Como assim?

 - Lembre que o c√°lculo do **SQ<sub>t</sub>** √© o c√°lculo da *vari√¢ncia* do nosso conjunto de dados:
   - Ou seja, o resultado total da vari√¢ncia.
 - Enquanto o meu **SQ<sub>res</sub>** mostra quanto desta *vari√¢ncia* foi explicada:
   - Se o meu **R<sup>2</sup>** fosse 1 significaria que 100% da *vari√¢ncia* teria sido explicada - **zero erro na reta**.

Ta, mas como eu programo essa bruxaria toda ai? Simples, veja o c√≥digo abaixo:

[r-squared.py](src/r-squared.py)  
```python
"""
R-Squared or Coefficient of Determination
"""

def createRegression(samples,variavel_numbers, n_noise):
  from sklearn.datasets import make_regression
  x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
  return x, y

if __name__ =='__main__':

  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import train_test_split
  from matplotlib import pyplot as plt

  reg = createRegression(200, 1, 30)
  model = LinearRegression()

  x_train, x_test, y_train, y_test = train_test_split(reg[0], reg[1], test_size=0.30)
  model.fit(x_train, y_train)

  # Coefficient of Determination: R^2 / R-Squared.
  r2 = model.score(x_test, y_test)
  print('Coefficient of Determination: R^2: {0}'.format(r2))
```

**OUTPUT:**  
```python
Coefficient of Determination: R^2: 0.9158177316382643
```

√ìtimo, pegamos o nosso **R<sup>2</sup>** que foi **0.91** ou seja, `n√≥s explicamos 91% do nosso conjunto de dados`. Agora vamos ver qual foi a parte do c√≥digo que fez isso e qual foi a l√≥gica:

Primeiro n√≥s criamos um modelo com os **dados de treino**:

```python
model.fit(x_train, y_train)
```

E depois apenas com os **dados de teste** n√≥s pegamos o **R<sup>2</sup>** com o m√©todo **score()**:

```python
# Coefficient of Determination: R^2 / R-Squared.
r2 = model.score(x_test, y_test)
```

<div id='13'></div>

## 13 - Usando o conceito de Regress√£o Linear & R<sup>2</sup> em um conjunto de dados reais + random_state

> √ìtimo, j√° aprendemos como dividir os dados em *treino* e *teste*, como funciona o *Coeficiente de Determina√ß√£o R<sup>2</sup>*, mas como trabalhar em um conjunto de dados reais tudo isso?

Agora vamos trabalhar com o Dataset do munic√≠pio **King County** localizado no estado de Washington nos Estados Unidos da Am√©rica (USA). Esse Dataset pode ser baixado facilmente dando uma `Googlada` ou voc√™ pode vir aqui no [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction).

Basicamente esse Dataset tem dados das casas vendidas no munic√≠pio de **King County**. Esses dados s√£o formados por 21 colunas, que s√£o:

 - **id -** ID da casa;
 - **date -** Data em que a casa foi vendida;
 - **price -** `Pre√ßo √© meta da previs√£o;`
 - **bedrooms -** N√∫mero de quartos casa;
 - **bathrooms -** N√∫mero de banheiros da casa;
 - **sqft_living -** Metragem quadrada da casa
 - **sqft_lot -** Metragem quadrada do lote;
 - **floors -** Pisos totais (n√≠veis) em casa;
 - **waterfront -** Casa com vista para a √°gua (mar/lagoa);
 - **view -** Foi visualizado;
 - **condition -** Qu√£o boa √© a condi√ß√£o (geral);
 - **grade -** Nota geral dada √† unidade habitacional, com base no sistema de classifica√ß√£o de King County;
 - **sqft_above -** Metragem quadrada de casa al√©m do por√£o;
 - **sqft_basement -** Metragem quadrada do por√£o;
 - **yr_built -** Ano de constru√ß√£o;
 - **yr_renovated -** Ano em que a casa foi reformada;
 - **zipcode -** C√≥digo postal (CEP no Brasil);
 - **lat -** Coordenada de latitude;
 - **long -** Coordenada de longitude;
 - **sqft_living15 -** √Årea da sala de estar em 2015 (implica-- algumas reformas) Isso pode ou n√£o ter afetado a √°rea de tamanho grande;
 - **sqft_lot15 -**  √Årea lotSize em 2015 (implica-- algumas reformas).

**NOTE:**  
At√© ent√£o n√≥s estavamos trabalhando apenas com duas vari√°veis, onde tinhamos o meu ponto **x** e o seu correspondente **y** e ficava muito f√°cil criar uma *Regress√£o Linear* em um plano **bidimensional**.  

S√≥ lembrando a f√≥rmula de **Regress√£o Linear** era essa:

![image](images/linear-regression-formule.png)  

Agora n√≥s temos que aplicar essa f√≥rmula no nosso Dataset **King County**, por√©m, ele tem `v√°rias vari√°veis`. Como isso se aplica na pr√°tica?

**RESPOSTA:**  
Ent√£o, a l√≥gica vai ser a mesma... Por√©m, ao inv√©s de **y = mx + b** n√≥s vamos ter:

![image](images/new-lr.png)  

Onde,

 - Cada vari√°vel vai ser representada por **x<sub>n</sub>** no seu respectivo √≠ndice - **(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ... x<sub>n</sub>)**;
 - Cada vari√°vel **x<sub>n</sub>** vai ter seu pr√≥prio Coeficiente Angular **m<sub>n</sub>**:
   - Esses Coeficientes Angulares **m<sub>n</sub>** que multiplicam as vari√°veis podem ser vistos como pesos *(weight)* de acordo com a vari√°vel.
 - Por fim, n√≥s vamos ter apenas um Coeficiente Linear **(b)**.

√ìtimo, entendido! Mas como aplicar isso em Python e Scikit-Learn e pegar o nosso Coeficiente de Determina√ß√£o **R<sup>2</sup>**?

[houses_predict.py](src/houses_predict.py)  
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt
import pandas as pd

pd.set_option('display.max_columns', 21)
df = pd.read_csv('../datasets/kc_house_data.csv')
df = df.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis=1)

y = df['price']
x = df.drop(['price'], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10)

model = LinearRegression()
model.fit(x_train, y_train)

r2 = model.score(x_test, y_test)
print('Coefficient of Determination: R^2: {0}'.format(r2))
```

**OUTPUT:**  
```python
Coefficient of Determination: R^2: 0.6608668622831475
```

Ou seja,

 - Significa que o meu modelo de Machine Learning √© **66% melhor do que simplesmente pegar a m√©dia dos valores**;
 - Ou, o meu modelo explica **65% da vari√¢ncia dos dados**.

**NOTE:**  
Outra observa√ß√£o √© que talvez o seu resultado seja diferente. Isso porque o m√©todo **train_test_split()** separa os dados de treino e testes aleatoriamente.

Isso pode ser ruim em alguns casos, por exemplo, n√≥s estamos ajustando os pesos (**m<sub>n</sub>**) das vari√°veis para encontrar um **R<sup>2</sup>** melhor para o nosso modelo. Como resolver isso?

**random_state:**  
O m√©todo **train_test_split()** tamb√©m pode receber um argumento chamado **random_state**. Como ele basicamente n√≥s passamos um inteiro e √© feito um embaralhamento dos dados, onde, sempre que voc√™ ou qualquer pessoa executar o m√©todo **train_test_split()** no mesmo conjunto de dados, os dados de treino e testes ser√£o os mesmo se ambos tiverem o mesmo argumento **random_state**.

Veja abaixo como fica:

```python
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10)
```

**OUTPUT:**  
```python
Coeficiente de Determina√ß√£o R^2: 0.6608668622831475
```

**NOTE:**  
Agora se voc√™ ou qualquer outra pessoa executar a mesma amostra de dados com o **random_state=10** os dados de treino e testes v√£o ser sempre o mesmo.

> Ou seja, o Coeficiente de Determina√ß√£o R<sup>2</sup> vai ser sempre o mesmo.

<div id='14'></div>

## 14 - Fazendo previs√µes

√ìtimo, at√© ent√£o n√≥s fizemos v√°rios exemplos de **Regress√£o Linear**; E pegamos o Coeficiente de Determina√ß√£o **R<sup>2</sup>**. Mas como tentar ver previs√µes com os nossos modelos?

Vamos come√ßar com uma nova abordagem... Suponha que n√≥s amamos pizzas e nos √∫ltimos dias n√≥s compramos 15. Sabendo que n√≥s compramos muitas pizzas vamos tentar prever qual o pre√ßo de uma pizza de acordo com o seu di√¢metro.

Ent√£o, os dados das nossas 15 pizzas foram os seguintes:


|Inst√¢ncia | Di√¢metro(cm) | Pre√ßo(R$) |
|----------|--------------|-----------|
|   1      |    	7       |   	8     |
|   2      |      10      |     11    |
|   3	     |      15	    |     16    |
|   4      |      30      |     38.5  |
|   5	     |      45      |     52    |
|   6      |    	13      |   	14    |
|   7      |      60      |     70    |
|   8	     |      100     |     90    |
|   9      |      5       |     6     |
|   10     |      30      |     38.5  |
|   11     |    	90      |   	102   |
|   12     |      18      |     20    |
|   13     |      70	    |     85    |
|   14     |      110     |     100   |
|   15     |      25      |     34    |

√ìtimo, primeiro vamos fazer todo aquele paranau√™:

 - Dividir os dados em treino e teste;
 - Criar uma reta de Regress√£o Linear que melhor explique essa rela√ß√£o;
 - Pegar o Coeficiente de Determina√ß√£o R<sup>2</sup>;
 - E por fim, criar um gr√°fico com tudo isso.

Vai ficar assim:

[pizza_predict.py](src/pizza_predict.py)  
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt


diameter = [[7], [10], [15], [30], [45], [13], [60], [100], [5], [30], [90], [18], [70], [110], [25]]
prices   = [[8], [11], [16], [38.5], [52], [14], [70], [90], [6], [38.5], [102], [20], [85], [100], [34]]

model = LinearRegression()

x_train, x_test, y_train, y_test = train_test_split(diameter, prices, test_size=0.30, random_state=10)

model.fit(x_train, y_train)

a_coeff = model.coef_ # Angular Coefficient - m
l_coeff = model.intercept_ # Linear Coefficient - b

# Coefficient of Determination: R^2 / R-Squared.
r2 = model.score(x_test, y_test)
print('Coefficient of Determination: R^2: {0}'.format(r2))

plt.figure(figsize=(10, 7))
plt.subplot(211)
plt.scatter(diameter, prices)
plt.title('Complete Sample')
plt.plot(x_train, a_coeff*x_train + l_coeff,color='red')
plt.subplot(223)
plt.scatter(x_train, y_train)
plt.title('Training Set (70%)')
plt.plot(x_train, a_coeff*x_train + l_coeff,color='blue')
plt.subplot(224)
plt.scatter(x_test, y_test)
plt.title('Testing set (30%)')
plt.plot(x_train, a_coeff*x_train + l_coeff,color='green')
plt.savefig('../images/plot-07.png', format='png')
plt.show()
```

**OUTPUT:**  
```python
Coeficiente de Determina√ß√£o R^2: 0.9231014405990501
```

![image](images/plot-07.png)  

Ok, isso n√≥s j√° aplicamos v√°rias vezes, mas agora eu quero saber o seguinte:

> Se eu desejar comprar uma pizza de 20cm di√¢metro qual vai ser o pre√ßo?

A classe [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) al√©m do m√©todo **fit()** utilizado para treinar nosso modelo, tamb√©m tem o m√©todo **predict()** que utiliza o nosso modelo linear para tentar fazer previs√µes.

Veja como fica isso com o c√≥digo abaixo:

[pizza_predict-v2.py](src/pizza_predict-v2.py)
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt

diameterPassed = float(input("What's the diameter(cm) of the pizza you want? "))

diameters = [[7], [10], [15], [30], [45], [13], [60], [100], [5], [30], [90], [18], [70], [110], [25]]
prices   = [[8], [11], [16], [38.5], [52], [14], [70], [90], [6], [38.5], [102], [20], [85], [100], [34]]

model = LinearRegression()

x_train, x_test, y_train, y_test = train_test_split(diameters, prices, test_size=0.30, random_state=10)

model.fit(x_train, y_train)
price = model.predict([[diameterPassed]])

print("A {0} cm diameter pizza should cost:: R${1}".format(diameterPassed, round(price[0][0])))
```

**OUTPUT:**  
```
What's the diameter(cm) of the pizza you want? 20
A 20.0 cm diameter pizza should cost:: R$24.0
```

**NOTE:**  
Na verdade n√≥s fizemos ainda melhor do que tentar prever s√≥ para uma pizza de 20cm de di√¢metro. N√≥s criamos um programa de Machine Learning que para **qualquer di√¢metro passado**, ele vai retorna qual ser√° mais ou menos o **pre√ßo da pizza** para esse conjunto (amostra) de dados.

![image](images/genius.gif)  

---

**REFERENCES:**  
[Didatica Tech - M√ìDULO - I](https://didatica.tech/)  
[A matem√°tica do Gradiente Descendente & Regress√£o Linear (machine learning)](https://www.youtube.com/watch?v=htfh2xrnlaE)  
[Metodo da Descida do Gradiente](https://www.youtube.com/watch?v=s0VhfvCB0Vw)  
[AULA 5 - GRADIENTE DESCENDENTE EXPLICADO - CURSO DE INTELIG√äNCIA ARTIFICIAL PARA TODOS](https://www.youtube.com/watch?v=joaYDx1HTcA)  
[Optimization: Ordinary Least Squares Vs. Gradient Descent ‚Äî from scratch](https://towardsdatascience.com/https-medium-com-chayankathuria-optimization-ordinary-least-squares-gradient-descent-from-scratch-8b48151ba756)   

---

**Rodrigo Leite** *- Software Engineer*
