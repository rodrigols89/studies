# Large Language Models

## Conte√∫do

 - **Introdu√ß√£o a LLMs:**
   - [O que s√£o LLMs?](#intro-to-llm)
   - [Como o modelo "entende" linguagem?](#how-understand)
   - [Como LLMs s√£o treinados)](#how-are-trained)
   - [Diferen√ßa entre LLMs e modelos tradicionais de NLP](#llm-vs-nlp)
   - [Por que os Transformers revolucionaram o NLP?](#transformers-inovation)
   - [Como funciona o Mecanismo de Attention (Aten√ß√£o)?](#attention-mechanism)
   - [Exemplos de tarefas resolvidas por LLMs](#llm-examples)
 - **Conceitos utilizados em LLMs:**
   - [Entendendo "Word Embeddings"](#understanding-word-embeddings)
   - [Word2Vec](#word2vec-idea)
 - **Prepara√ß√£o e amostragem de dados (Data preparation & sampling):**
   - **Lendo arquivos (Reading files):**
     - [read_txt()](#read-txt)
   - **Tokenization:**
     - [Tokeniza√ß√£o de texto (Tokenizing text)](#tokenization)
     - [Convertendo tokens em IDs de token (Converting tokens into token IDs)](#token-id)
     - [Convertendo IDs de tokens em tensores de incorpora√ß√£o (Embeddings)](#token-id-to-tensor)
     - [Criando token embeddings](#creating-token-embeddings)
   - **Encode & Decode:**
     - [encode()](#intro-to-encode)
     - [decode()](#intro-to-decode)
   - [**Sliding Window (input-target)**](#sliding-window)
 - **Mecanismo de aten√ß√£o (Attention mechanism):**
 - **Arquiteturas de LLMs (LLMs architecture):**
 - **Pr√©-treinamento (Pretraining):**
 - **Loop de treinamento (Training loop):**
 - **Avalia√ß√£o do modelo (Model evaluation):**
 - **Carregamento pesos pr√©-treinados (Load pretrained weights):**
 - **Afina√ß√£o (Fine-tuning):**
   - **Modelos de classifica√ß√£o (Classification models):**
   - **Assistentes pessoais ou modelos de chat (Personal assistants or chat models):**
 - [**üöÄ Instala√ß√£o / Execu√ß√£o local**](#settings)
 - [**REFER√äNCIAS**](#ref)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->






































































































<!--- ( Introdu√ß√£o a LLMs ) --->

---

<div id="intro-to-llm"></div>

## O que s√£o LLMs?

**üìò Defini√ß√£o:**  
LLMs (Large Language Models) s√£o modelos de aprendizado de m√°quina treinados para **"entender"**, **"gerar"** e **"manipular linguagem natural"**.

> **OBSERVA√á√ÉO:**  
> Eles s√£o chamados de "grandes" por causa da quantidade massiva de par√¢metros (milh√µes ou bilh√µes) e por serem treinados em grandes volumes de texto da internet, livros, artigos, f√≥runs, c√≥digo-fonte etc.




















---

<div id="how-understand"></div>

### Como o modelo "entende" linguagem?

Na verdade, LLMs n√£o entendem no sentido humano. Eles aprendem probabilidades estat√≠sticas:

> Se eu vejo a frase: "O c√©u est√° ___", a palavra mais prov√°vel √© "azul".  
> Esse "palpite" √© feito com base no que ele viu durante o treinamento.




















---

<div id="how-are-trained"></div>

### Como LLMs s√£o treinados

 - **Pr√©-treinamento (Pretraining):**
   - O modelo √© exposto a grandes quantidades de texto e aprende padr√µes de linguagem por meio de tarefas como **"prever a pr√≥xima palavra"** (auto-regressivo) ou **"preencher palavras faltantes"** (m√°scara).
 - **Ajuste fino (Fine-tuning):**
   - O modelo pode ser adaptado para tarefas espec√≠ficas, como:
     - Classifica√ß√£o de texto;
     - Tradu√ß√£o;
     - Gera√ß√£o de c√≥digo;
     - Resumo de documentos.




















---

<div id="llm-vs-nlp"></div>

## Diferen√ßa entre LLMs e modelos tradicionais de NLP

 - Antes dos LLMs, os modelos de **NLP** eram **espec√≠ficos para cada tarefa**, como *an√°lise de sentimentos*, *tradu√ß√£o*, ou *resumo*.
 - Com os **LLMs**, **um √∫nico modelo pode ser usado para v√°rias tarefas** com pouco ou nenhum ajuste.

### Compara√ß√£o Direta

| Caracter√≠stica                  | Modelos Tradicionais de NLP                 | LLMs (Large Language Models)                        |
| ------------------------------- | ------------------------------------------- | --------------------------------------------------- |
| **Arquitetura**                 | Simples (SVM, Regress√£o, Naive Bayes, RNNs) | Transformer (profundo e em larga escala)            |
| **Treinamento**                 | Um modelo por tarefa                        | Um √∫nico modelo para tarefas m√∫ltiplas              |
| **Requer feature engineering?** | Sim! Manual e demorado                      | N√£o. O modelo aprende tudo automaticamente          |
| **Escalabilidade**              | Limitado                                    | Altamente escal√°vel e flex√≠vel                      |
| **Precis√£o/Desempenho**         | OK, mas limitado com grandes volumes        | Alta, especialmente com dados em larga escala       |
| **Entrada/Sa√≠da**               | Tipicamente vetores num√©ricos               | Texto puro (prompts e respostas)                    |
| **Contexto considerado**        | Curto (√†s vezes s√≥ 1 frase)                 | Longo (v√°rios par√°grafos ou at√© milhares de tokens) |
| **Exemplos**                    | TF-IDF + SVM, Word2Vec + LSTM               | GPT, BERT, T5, LLaMA, Claude, Gemini                |

### Explicando com um exemplo

 - **Modelo tradicional (pr√©-LLM):**
   - Transformar o texto em vetores (TF-IDF, Bag of Words).
   - Treinar um SVM ou uma Regress√£o Log√≠stica para prever o sentimento.
   - Modelo s√≥ serve pra essa tarefa.
 - **LLM:**
   - Voc√™ escreve:
     - `Classifique o sentimento desta frase: Estou muito feliz hoje!`
   - O modelo responde:
     - `Sentimento: Positivo`
   - **OBSERVA√á√ÉO:** O mesmo modelo pode tamb√©m traduzir, resumir, gerar c√≥digo...

### Vantagens dos LLMs sobre modelos tradicionais

 - ‚úÖ **Generaliza√ß√£o:** Um modelo para muitas tarefas;
 - ‚úÖ **Zero-shot & few-shot:** Resolve tarefas com poucas instru√ß√µes;
 - ‚úÖ Menos depend√™ncia de dados rotulados;
 - ‚úÖ Contexto mais longo e melhor compreens√£o;
 - ‚úÖ Gera√ß√£o de linguagem natural mais fluida.

### Mas os modelos tradicionais morreram?

 - **‚ùå N√£o! Eles ainda s√£o √∫teis quando:**
   - Voc√™ tem poucos dados e poucos recursos computacionais;
   - A tarefa √© muito espec√≠fica e n√£o exige interpreta√ß√£o profunda;
   - Voc√™ precisa de explicabilidade clara e r√°pida.




















---

<div id="transformers-inovation"></div>

## Por que os Transformers revolucionaram o NLP?

> Transformers s√£o uma arquitetura introduzida no artigo [**"Attention is All You Need" (2017)**](https://arxiv.org/abs/1706.03762).

**OBSERVA√á√ÉO:**  
Eles eliminaram a necessidade de processar palavras em sequ√™ncia como faziam **RNNs** e **LSTMs** ‚Äî e com isso, permitiram muito mais *"paralelismo"*, *"contexto global"* e *"velocidade"*.

### üö´ Problema dos modelos anteriores (RNN, LSTM)

 - Processavam tokens um por um (sequencialmente).
 - Sofriam com longas depend√™ncias ("o que foi dito 30 palavras atr√°s?").
 - Eram lentos para treinar.
 - Tinha dificuldade com frases longas e contexto amplo.

### ‚úÖ Como o Transformer resolveu tudo isso?

**A resposta:** Attention Mechanism.

> O modelo aprende a **"prestar aten√ß√£o"** nas palavras mais importantes do texto ‚Äî independentemente da posi√ß√£o!

### üß© Componentes principais do Transformer

| Componente                    | Fun√ß√£o B√°sica                                                    |
| ----------------------------- | ---------------------------------------------------------------- |
| **Embedding**                 | Converte palavras em vetores num√©ricos.                          |
| **Self-Attention**            | Calcula a import√¢ncia de cada palavra em rela√ß√£o √†s outras.      |
| **Positional Encoding**       | Adiciona informa√ß√£o da posi√ß√£o das palavras (j√° que √© paralelo). |
| **Feedforward Layers**        | Faz transforma√ß√µes profundas nos vetores.                        |
| **Normalization & Residuals** | Ajudam a estabilizar e melhorar o aprendizado.                   |




















---

<div id="attention-mechanism"></div>

## Como funciona o Mecanismo de Attention (Aten√ß√£o)?

> O mecanismo de aten√ß√£o permite que o modelo **"foque" em partes importantes da entrada** ‚Äî como humanos fazem ao ler.

Por exemplo, imagine que n√≥s temos a frase:

Imagine a frase:

> ‚ÄúA ma√ß√£ estava azeda, ent√£o ela foi jogada fora.‚Äù

 - A palavra **"ela"** poderia se referir √† **ma√ß√£** ou √† **azeda**.
 - O modelo precisa **‚Äúprestar aten√ß√£o‚Äù** nas palavras relevantes para entender corretamente.

### üßÆ F√≥rmula matem√°tica (simples)

A f√≥rmula matem√°tica (simples) √© a seguinte:

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$

 - `QK·µÄ`
   - Compara (multiplica) os queries com os keys ‚Üí gera pontua√ß√µes de aten√ß√£o.
 - `/ ‚àöd_k`
   - Normaliza para evitar explos√µes num√©ricas.
 - `softmax`
   - Transforma em probabilidades.
 - `√ó V`
   - Gera a aten√ß√£o ponderada, ou seja, a sa√≠da final.

### üî¢ Exemplo num√©rico ilustrativo

```bash
Frase: ["O", "gato", "correu"]

‚Üí Para "correu", o modelo calcula:

Q(correu) ‚Ä¢ K(O)     ‚Üí  baixa aten√ß√£o
Q(correu) ‚Ä¢ K(gato)  ‚Üí  alta aten√ß√£o
Q(correu) ‚Ä¢ K(correu)‚Üí  m√©dia aten√ß√£o
```

> **Resultado:**  
> O modelo vai ponderar mais o **"gato"**, porque **"gato correu"** tem uma rela√ß√£o forte.




















---

<div id="llm-examples"></div>

### Exemplos de tarefas resolvidas por LLMs

Vamos come√ßar com uma introdu√ß√£o de algumas tarefas que podem ser resolvidas utilizando **"LLMs"**:

| Tarefa                      | Exemplo pr√°tico                             |
| --------------------------- | ------------------------------------------- |
| **Gera√ß√£o de texto**        | Chatbots, reda√ß√£o autom√°tica                |
| **Classifica√ß√£o**           | An√°lise de sentimentos, spam vs. n√£o spam   |
| **Tradu√ß√£o**                | Ingl√™s ‚Üí Portugu√™s, etc.                    |
| **Perguntas e Respostas**   | Assistente de d√∫vidas                       |
| **Resumo autom√°tico**       | Resumir longos artigos                      |
| **Extra√ß√£o de informa√ß√µes** | Pegar nomes, datas, eventos de um texto     |
| **Gera√ß√£o de c√≥digo**       | Auto-complete em IDEs, explica√ß√£o de c√≥digo |

### üöÄ Aplica√ß√µes Reais das LLMs

| **√Årea**                   | **Aplica√ß√£o**                                                                                       |
| -------------------------- | --------------------------------------------------------------------------------------------------- |
| **Assistentes virtuais**   | Chatbots inteligentes (ex: ChatGPT, Google Bard, Alexa)                                             |
| **Educa√ß√£o**               | Tutores personalizados, corre√ß√£o autom√°tica de reda√ß√µes, explica√ß√µes sob demanda                    |
| **Sa√∫de**                  | An√°lise de prontu√°rios, resposta a perguntas m√©dicas, apoio √† decis√£o cl√≠nica                       |
| **Atendimento ao cliente** | Respostas autom√°ticas, suporte 24/7, resumo de intera√ß√µes com usu√°rios                              |
| **Pesquisa e ci√™ncia**     | S√≠ntese de artigos, gera√ß√£o de hip√≥teses, revis√£o autom√°tica de literatura                          |
| **Programa√ß√£o**            | Autocompletar c√≥digo, explicar fun√ß√µes, gerar trechos em diferentes linguagens (ex: GitHub Copilot) |
| **Tradu√ß√£o de idiomas**    | Tradu√ß√µes contextuais e multil√≠ngues, com adapta√ß√£o ao dom√≠nio espec√≠fico                           |
| **Cria√ß√£o de conte√∫do**    | Gera√ß√£o de artigos, roteiros, marketing, posts para redes sociais                                   |
| **Direito**                | An√°lise de contratos, extra√ß√£o de cl√°usulas, sumariza√ß√£o de decis√µes legais                         |
| **An√°lise de sentimentos** | Classifica√ß√£o de avalia√ß√µes e sentimentos em redes sociais e e-commerce                             |
| **Seguran√ßa cibern√©tica**  | Explica√ß√£o de exploits, an√°lise de logs e gera√ß√£o de alertas                                        |
| **Games e NPCs**           | Di√°logos gerados dinamicamente, comportamentos inteligentes, roteiros                               |

### üìà Exemplos Reais de Impacto com LLMs

| **Organiza√ß√£o / Produto** | **Uso de LLMs**                                                                          |
| --------------------------|------------------------------------------------------------------------------------------|
| `Duolingo`                | Feedback em tempo real sobre frases escritas por alunos com explica√ß√µes contextualizadas |
| `Notion AI`               | Gera√ß√£o e reformula√ß√£o de textos, resumos autom√°ticos, cria√ß√£o de tarefas                |
| `Khan Academy (GPT-4)`    | Tutor personalizado que responde d√∫vidas dos alunos com explica√ß√µes passo a passo        |
| `GitHub Copilot`          | Sugest√µes de c√≥digo em tempo real, explica√ß√µes de fun√ß√µes, gera√ß√£o de testes             |
| `GrammarlyGO`             | Reescrita e aprimoramento de textos com base no contexto do usu√°rio                      |
| `Legal Robot`             | An√°lise de contratos com explica√ß√£o em linguagem natural das cl√°usulas jur√≠dicas         |
| `You.com (YouChat)`       | Motor de busca com respostas geradas por LLM, integrando fontes e interatividade         |







































































































<!--- ( Conceitos utilizados em LLMs ) --->

---

<div id="understanding-word-embeddings"></div>

## Entendendo "Word Embeddings"

 - Modelos de Deep Learning, incluindo LLMs, n√£o podem processar texto bruto diretamente;
 - Como o texto √© categ√≥rico, ele n√£o √© compat√≠vel com as opera√ß√µes matem√°ticas utilizadas para implementar e treinar redes neurais;
 - Portanto, precisamos de uma forma de representar palavras como vetores com valores cont√≠nuos.

> **OBSERVA√á√ÉO:**  
> O conceito de converter dados (√°udio, v√≠deo, texto) para o formato de vetor (num√©rico) √© frequentemente chamado de **"embedding"**.

Por exemplo:

![img](images/word-embeddings-01.png)  

> **OBSERVA√á√ÉO:**  
> No entanto, √© importante observar que diferentes formatos de dados exigem *modelos de embbedding* distintos.  
> Por exemplo, um modelo de embedding projetado para texto n√£o seria adequado para embbedding de dados de √°udio ou v√≠deo.




















---

<div id="word2vec-idea"></div>

## Word2Vec

> A principal *ideia* por tr√°s do **Word2Vec** √© que *"palavras que aparecem em contextos semelhantes tendem a ter significados semelhantes"*.

Consequentemente, quando projetadas em embeddings de palavras bidimensionais para fins de visualiza√ß√£o, palavras semelhantes ficam agrupadas.

![img](images/word2vec-idea-01.png)  







































































































<!--- ( Prepara√ß√£o e amostragem de dados (Data preparation & sampling) ) --->

---

<div id="read-txt"></div>

## read_txt()

Aqui n√≥s vamos aprender como ler um arquivo de texto no formato `.txt`:

<!--- ( Python (From Scratch) ) --->
<details>

<summary>Python (From Scratch)</summary>

</br>

[utils.py](src/utils.py)
```python
import re


def read_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
    return text


if __name__ == "__main__":

    file_path = "../datasets/the-verdict.txt"
    text = read_txt(file_path)

    print("Total number of characters:", len(text))
    print("Text type:", type(text), "\n")
```

**OUTPUT:**
```bash
Total number of characters: 20479
Text type: <class 'str'> 
```

> **OBSERVA√á√ÉO:**  
> Podemos usar o conceito **fatiamento (slicing)** para selecionar uma parte espec√≠fica do texto.

```python
print(text[:353])
```

**OUTPUT:**
```bash
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)

"The height of his glory"
```

</details>




















---

<div id="tokenization"></div>

## Tokeniza√ß√£o de texto (Tokenizing text)

> Aqui, vamos discutir como podemos dividir uma entrada texto em *tokens* individuais, uma etapa de pr√©-processamento necess√°ria para criar *embeddings* para um *LLM*.

Esses tokens s√£o palavras individuais ou caracteres especiais, incluindo sinais de pontua√ß√£o, conforme mostrado abaixo:

![img](images/tokenizing-01.png)  

Vamos ver como implementar isso na pr√°tica:

<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

Para criar tokens com a biblioteca biblioteca [ü§ó Transformers (Hugging Face)](https://github.com/huggingface/transformers) vamos utilizar o tokenizador `AutoTokenizer` com o modelo `bert-base-uncased`:

[transformers_tokenizers.py](src/transformers_tokenizers.py)
```python
from transformers import AutoTokenizer

from utils import read_txt


# load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the and read the text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

# Tokenization
tokens = tokenizer.tokenize(text)

print("Total number of tokens (without whitespaces):", len(tokens))
print("Tokens (first 10):", tokens[:10])
```

**OUTPUT:**  
```bash
Total number of tokens (without whitespaces): 5212
Tokens (first 10): ['i', 'had', 'always', 'thought', 'jack', 'gi', '##sb', '##urn', 'rather', 'a']
```

> **Por que n√≥s temos mais tokens utilizando o tokenizador da Hugging Face?**

### üß† Diferen√ßa fundamental: Subword Tokenization

O **BERT** n√£o usa tokeniza√ß√£o por palavras ou pontua√ß√µes simples. Ele usa um m√©todo chamado:

> **üëâ WordPiece Tokenization:**  
> Palavras desconhecidas ou raras s√£o quebradas em subpartes, chamadas *"subwords"*.

Por exemplo:

```bash
'Gisburn' ‚Üí ['gi', '##sb', '##urn']
```

A ideia √© balancear entre:

 - Cobertura de vocabul√°rio (poucos tokens desconhecidos);
 - Tamanho do vocabul√°rio (tornar o modelo mais eficiente).

### `üß† O que s√£o esses ##?`

 - No **BERT**, os tokens que come√ßam com `##` s√£o subpalavras que continuam uma palavra anterior.
 - Por exemplo:
   - `gi` √© o come√ßo da palavra `Gisburn`;
   - ``##sb`` e ``##urn`` s√£o subpalavras que continuam `gi` at√© formar `gi + sb + urn = Gisburn`.

Esse tipo de tokeniza√ß√£o:

 - Reduz o n√∫mero de palavras *OOV (out-of-vocabulary)*;
 - Garante que at√© palavras n√£o vistas no treinamento ainda sejam entendidas em partes.

### üìå Outros detalhes que aumentam a contagem no BERT:

| Fator                                 | Explica√ß√£o                                                                        |
| ------------------------------------- | --------------------------------------------------------------------------------- |
| **Subwords**                          | Palavras como `Gisburn`, `unbelievable` viram v√°rias partes                       |
| **Lowercasing**                       | O `bert-base-uncased` transforma tudo em min√∫sculo antes de tokenizar             |
| **Tokens especiais (em outros usos)** | `[CLS]`, `[SEP]` etc. (no seu caso n√£o est√£o aparecendo porque voc√™ s√≥ tokenizou) |
| **Sem filtragem de pontua√ß√£o**        | O tokenizer BERT inclui pontua√ß√µes como tokens pr√≥prios (`.`, `,`, etc.)          |

### ‚úÖ Conclus√£o

| Tokenizador          | Tipo de tokeniza√ß√£o            | Total de tokens | Exemplo                 |
| -------------------- | ------------------------------ | --------------- | ----------------------- |
| `Seu (com re.split)` | Baseado em pontua√ß√£o e espa√ßos | 4690            | `'Gisburn'`             |
| `BERT ("WordPiece")` | Subword Tokenization           | 5212            | `'gi', '##sb', '##urn'` |

> **OBSERVA√á√ÉO:**  
> O BERT gera mais tokens porque ele quebra palavras em partes menores que est√£o no vocabul√°rio aprendido durante o pr√©-treinamento. Isso permite lidar melhor com palavras raras ou compostas.

</details>




















---

<div id="token-id"></div>

## Convertendo tokens em IDs de token (Converting tokens into token IDs)

Aqui n√≥s vamos ver o processo de atribuir um ID num√©rico √∫nico para cada token (palavra, subpalavra ou s√≠mbolo) com base em um vocabul√°rio fixo do modelo.

### üìò O que √© "Vocabul√°rio" em LLMs?

No contexto de Modelos de Linguagem (LLMs), o vocabul√°rio √© a lista de todos os tokens que o modelo conhece.

**‚úÖ Cada token tem:**

 - Uma forma textual (ex: "hello", "##ing", "!")
 - Um ID √∫nico (ex: 101, 1254, 999)

> **OBSERVA√á√ÉO:**  
> Esse vocabul√°rio √© definido antes do treinamento do modelo, geralmente criado com base em um grande corpus de texto.

### üß† Por que esse vocabul√°rio √© importante?

Porque o modelo s√≥ consegue processar textos usando os *tokens* do seu *vocabul√°rio*. Se uma palavra n√£o estiver nele, ser√° dividida em subpalavras ou marcada como token desconhecido ([UNK]).

Por exemplo, imagine que temos o seguinte texto como entrada:

**Entrada (Input):**
```python
I love learning because I love new things
```

| Token      | Token ID |
| ---------- | -------- |
| `[PAD]`    | 0        |
| `[UNK]`    | 1        |
| `i`        | 2        |
| `love`     | 3        |
| `learning` | 4        |
| `because`  | 5        |
| `new`      | 6        |
| `things`   | 7        |

**üßÆ Tokeniza√ß√£o (simples):**
```python
tokens = ['i', 'love', 'learning', 'because', 'i', 'love', 'new', 'things']
```

> **OBSERVA√á√ÉO:**  
> Mesmo que as palavras **"i"** e **"love"** apare√ßam mais de uma vez, elas ser√£o tokenizadas da mesma forma, pois o vocabul√°rio √© est√°tico.

**üî¢ Convers√£o para Token IDs:**
```python
token_ids = [2, 3, 4, 5, 2, 3, 6, 7]
```

| Palavra      | Token      | Token ID |
| ------------ | ---------- | -------- |
| **I**        | `i`        | 2        |
| **love**     | `love`     | 3        |
| **learning** | `learning` | 4        |
| **because**  | `because`  | 5        |
| **I**        | `i`        | 2        |
| **love**     | `love`     | 3        |
| **new**      | `new`      | 6        |
| **things**   | `things`   | 7        |

**‚úÖ Observa√ß√µes:**

 - Tokens repetidos (como "i" e "love") continuam recebendo o mesmo ID.
 - O modelo trata repeti√ß√µes de **forma contextual**:
   - Ou seja, mesmo com o mesmo token ID, o significado pode *"mudar dependendo do contexto anterior"*.

Agora vamos ver como implementar isso na pr√°tica:

<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

Com a biblioteca **transformers (Hugging Face)** podemos utilizar a fun√ß√£o `tokenizer.get_vocab()` para pegar o vocacul√°rio dos tokens:

[transformers_tokenizers.py](src/transformers_tokenizers.py)
```python
from transformers import AutoTokenizer

from utils import read_txt


# load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the and read the text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

tokens = tokenizer.tokenize(text)   # Tokenize the text
vocabulary = tokenizer.get_vocab()  # Get the vocabulary

print("Vocabulary type:", type(vocabulary))
for voc in list(vocabulary.items())[:10]:
    print(voc)
```

**OUTPUT:**
```bash
Vocabulary type: <class 'dict'>
('women', 2308)
('[unused452]', 457)
('sponsorship', 12026)
('devout', 26092)
('william', 2520)
('glide', 21096)
('referees', 25118)
('handball', 12378)
('1950s', 4856)
('treated', 5845)
```

</details>




















---

<div id="token-id-to-tensor"></div>

## Convertendo IDs de tokens em tensores de incorpora√ß√£o (Embeddings)

Converter os IDs de tokens em *tensores de incorpora√ß√£o ("Embeddings")* √© a √∫ltima etapa da prepara√ß√£o antes do modelo propriamente dito (LLM) processar os dados.

Vamos ver como fazer isso na pr√°tica:

<details>

<summary>PyTorch</summary>

<br/>

Para transformar os IDs de tokens em tensores, voc√™ s√≥ precisa ajustar o par√¢metro `return_tensors="pt" (para utilizar PyTorch)`: 

[embeddings_pytorch.py](src/embeddings_pytorch.py)
```python
# encode process = here we tokenize + convert to IDs
encoding = tokenizer.encode_plus(
    text,
    add_special_tokens=True,
    return_tensors="pt",      # pt = Pytorch
    truncation=True,
    padding=False
)

input_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
attention_mask = encoding["attention_mask"]

print("Tensor input_ids shape:", input_ids.shape)
print("Tensor token_type_ids shape:", token_type_ids.shape)
print("Tensor attention_mask shape:", attention_mask.shape)
```

**OUTPUT:**
```bash
Tensor input_ids shape: torch.Size([1, 512])
Tensor token_type_ids shape: torch.Size([1, 512])
Tensor attention_mask shape: torch.Size([1, 512])
```

> **NOTE:**  
> Vejam que agora n√≥s temos **PyTorch tensores**.

</details>





<details>

<summary>TensorFlow</summary>

<br/>

Para transformar os IDs de tokens em tensores, voc√™ s√≥ precisa ajustar o par√¢metro `return_tensors="tf" (para utilizar TensorFlow)`: 

[embeddings_tensorflow.py](src/embeddings_tensorflow.py)
```python
encoding = tokenizer.encode_plus(
    text,
    add_special_tokens=True,
    return_tensors="tf",      # tf = Tensorflow
    truncation=True,
    padding=False
)

input_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
attention_mask = encoding["attention_mask"]

print("Tensor input_ids shape:", input_ids[0, :10])
print("Tensor token_type_ids shape:", token_type_ids[0, :10])
print("Tensor attention_mask shape:", attention_mask[0, :10])
```

**OUTPUT:**
```bash
Tensor input_ids shape: tf.Tensor([  101  1045  2018  2467  2245  2990 21025 19022 14287  2738], shape=(10,), dtype=int32)
Tensor token_type_ids shape: tf.Tensor([0 0 0 0 0 0 0 0 0 0], shape=(10,), dtype=int32)
Tensor attention_mask shape: tf.Tensor([1 1 1 1 1 1 1 1 1 1], shape=(10,), dtype=int32)
```

> **NOTE:**  
> Vejam que agora n√≥s temos **TensorFlow tensores**.

</details>




















---

<div id="creating-token-embeddings"></div>

## Criando token embeddings

Token Embedding √© o processo de transformar:

 - Tokens *inteiros* (IDs):
   - Que s√£o n√∫meros inteiros representando palavras ou subpalavras.
 - Em vetores densos de *n√∫meros reais*.

Esse processo √© essencial para que os modelos de linguagem (LLMs) possam trabalhar com texto de forma num√©rica.

Por exemplo:

```bash
token_id  = 1037     # "a" no BERT tokenizer.
embedding = [0.1, 0.5, ..., -0.2]  # vetor de 768 dimens√µes.
```

### üß† Por que isso √© importante?

Redes neurais n√£o entendem palavras ou n√∫meros inteiros diretamente ‚Äî elas precisam de *vetores cont√≠nuos* que capturam sem√¢ntica, contexto e rela√ß√£o entre palavras. Embeddings fazem essa ponte.

### üö´ Quando n√£o utilizar?

 - ‚ùå Se estiver usando um modelo pr√©-treinado completo (ex: AutoModel do Hugging Face) ‚Äî os embeddings j√° est√£o l√°.
 - ‚ùå Se estiver apenas tokenizando e n√£o treinando nenhum modelo.









---

<div id="intro-to-encode"></div>

## encode()

> Um **encode()** √© um m√©todo (ou fun√ß√£o) fornecido por tokenizers de modelos pr√©-treinados (como os da biblioteca ü§ó transformers) para converter uma string de texto em IDs num√©ricos de tokens.

O m√©todo (ou fun√ß√£o) `.encode()` realiza duas etapas principais:

 - **Tokeniza√ß√£o:**
   - Divide o texto em unidades menores chamadas tokens (subpalavras, palavras ou peda√ßos).
 - **Mapeamento para IDs:**
   - Converte cada token em um n√∫mero inteiro com base no vocabul√°rio do modelo.

![img](images/encode-01.png)  

Vamos ver como implementar isso na pr√°tica:

<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

A biblioteca **ü§ó Transformers** j√° possui `.encode()` pronto. Ela fornece tokenizers otimizados, que:

 - Tokenizam o texto (tokenize);
 - Criam vocabul√°rio (vocab);
 - Fazem mapeamento de tokens para IDs (convert_tokens_to_ids);
 - E at√© fazem o encode autom√°tico com padding, truncamento e m√°scara de aten√ß√£o, se necess√°rio.

[transformers_encode_decode.py](src/transformers_encode_decode.py)
```python
from transformers import AutoTokenizer

from utils import read_txt


# load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the and read the text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

# encode = Here we tokenize + convert to IDs
encoding = tokenizer.encode_plus(
    text,
    add_special_tokens=True,  # Add [CLS], [SEP]
    return_tensors=None,      # "pt" for Pytorch, "tf" for TensorFlow or None
    truncation=True,          # Truncates if too long
    padding=False             # Do not add padding
)
```

No c√≥digo acima n√≥s j√° tokenizamos o texto e convertemos para IDs.

> **Mas qual o tipo eo que tem dentro de "encoding"?**

```python
print("Return type:", type(encoding))
print("\nencoding:\n", encoding)
```

**OUTPUT:**
```bash
Return type: <class 'transformers.tokenization_utils_base.BatchEncoding'>

encoding:
 {'input_ids': [101, 1045, 2018, 2467, 2245, 2990, 21025, 19022, 14287, 2738, 1037, 10036, 11067, 1011, 1011, 2295, 1037, 2204, 3507, 2438, 1011, 1011, 2061, 2009, 2001, 2053, 2307, 4474, 2000, 2033, 2000, 2963
, 2008, 1010, 1999, 1996, 4578, 1997, 2010, 8294, 1010, 2002, 2018, 3333, 2010, 4169, 1010, 2496, 1037, 4138, 7794, 1010, 1998, 2511, 2370, 1999, 1037, 6992, 2006, 1996, 15544, 14356, 2050, 1012, 1006, 2295, 10
45, 2738, 2245, 2009, 2052, 2031, 2042, 4199, 2030, 7701, 1012, 1007, 1000, 1996, 4578, 1997, 2010, 8294, 1000, 1011, 1011, 2008, 2001, 2054, 1996, 2308, 2170, 2009, 1012, 1045, 2064, 2963, 3680, 1012, 12137, 1
6215, 9328, 1011, 1011, 2010, 2197, 3190, 4133, 3334, 1011, 1011, 2139, 24759, 28741, 2290, 2010, 14477, 21408, 16671, 3085, 19935, 21261, 1012, 1000, 1997, 2607, 2009, 1005, 1055, 2183, 2000, 4604, 1996, 3643,
 1997, 2026, 3861, 1005, 2126, 2039, 1025, 2021, 1045, 2123, 1005, 1056, 2228, 1997, 2008, 1010, 2720, 1012, 6174, 3511, 1011, 1011, 1996, 3279, 2000, 12098, 5339, 2003, 2035, 1045, 2228, 1997, 1012, 1000, 1996
, 2773, 1010, 2006, 3680, 1012, 16215, 9328, 1005, 1055, 2970, 1010, 28608, 2049, 1035, 12667, 1035, 2004, 2295, 2027, 2020, 7686, 1999, 2019, 10866, 13005, 1997, 13536, 1012, 1998, 2009, 2001, 2025, 2069, 1996
, 3680, 1012, 16215, 9328, 2015, 2040, 9587, 21737, 2094, 1012, 2018, 2025, 1996, 19401, 2014, 10092, 28983, 1010, 2012, 1996, 2197, 28680, 3916, 2265, 1010, 3030, 2033, 2077, 21025, 19022, 14287, 1005, 1055, 1
000, 4231, 1011, 10487, 1000, 2000, 2360, 1010, 2007, 4000, 1999, 2014, 2159, 1024, 1000, 2057, 4618, 2025, 2298, 2588, 2049, 2066, 2153, 1000, 1029, 2092, 999, 1011, 1011, 2130, 2083, 1996, 26113, 1997, 2014,
10092, 1005, 1055, 4000, 1045, 2371, 2583, 2000, 2227, 1996, 2755, 2007, 1041, 16211, 3490, 16383, 1012, 3532, 2990, 21025, 19022, 14287, 999, 1996, 2308, 2018, 2081, 2032, 1011, 1011, 2009, 2001, 11414, 2008,
2027, 2323, 9587, 14287, 2032, 1012, 2426, 2010, 2219, 3348, 8491, 23161, 2020, 2657, 1010, 1998, 1999, 2010, 2219, 3119, 6684, 1037, 20227, 1012, 2658, 14225, 1029, 3383, 1012, 2065, 2009, 2020, 1010, 1996, 62
25, 1997, 1996, 7477, 2001, 19354, 26022, 2011, 2210, 8149, 17490, 3051, 1010, 2040, 1010, 1999, 2035, 2204, 4752, 1010, 2716, 2041, 1999, 1996, 15552, 1037, 2200, 8502, 1000, 20815, 1000, 2006, 2990, 1011, 101
1, 2028, 1997, 2216, 2265, 2100, 4790, 24802, 2007, 6721, 4087, 6447, 2008, 1045, 2031, 2657, 1006, 1045, 2180, 1005, 1056, 2360, 2011, 3183, 1007, 4102, 2000, 21025, 19022, 14287, 1005, 1055, 4169, 1012, 1998,
 2061, 1011, 1011, 2010, 10663, 2108, 4593, 20868, 2890, 6767, 21170, 1011, 1011, 1996, 6594, 6360, 2351, 2041, 1010, 1998, 1010, 2004, 3680, 1012, 16215, 9328, 2018, 10173, 1010, 1996, 3976, 1997, 1000, 21025,
 19022, 14287, 2015, 1000, 2253, 2039, 1012, 2009, 2001, 2025, 6229, 2093, 2086, 2101, 2008, 1010, 1999, 1996, 2607, 1997, 1037, 2261, 3134, 1005, 8909, 2989, 2006, 1996, 15544, 14356, 2050, 1010, 2009, 3402, 4
158, 2000, 2033, 2000, 4687, 2339, 21025, 19022, 14287, 2018, 2445, 2039, 2010, 4169, 1012, 2006, 9185, 1010, 2009, 2428, 2001, 1037, 23421, 3291, 1012, 2000, 26960, 2010, 2564, 2052, 2031, 2042, 2205, 3733, 10
2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Vejam que n√≥s temos:

 - Um objeto `transformers.tokenization_utils_base.BatchEncoding`;
 - Em `encoding` temos um dicion√°rio:
   - Com as chaves: `input_ids`, `token_type_ids` e `attention_mask`.
   - E os valores temos *listas* para cada chave.

> **Mas o que s√£o `input_ids`, `token_type_ids` e `attention_mask`?**

 - `input_ids`
   - IDs dos tokens, incluindo tokens especiais ([CLS], [SEP], [PAD]).
 - `token_type_ids`
   - Informa a qual senten√ßa cada token pertence.
 - `attention_mask`
   - 	Informa quais tokens devem ser processados (1) ou ignorados (0).

**OBSERVA√á√ÉO:**  
Sabendo que n√≥s temos um dicion√°rio com essas chaves (`input_ids`, `token_type_ids` e `attention_mask`), podemos acessar seus valores usando a l√≥gica de dicion√°rios em Python:

```python
token_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
attention_mask = encoding["attention_mask"]
```

> **E os vocabul√°rios?**  
> Eles ainda est√£o dispos√≠veis utilizando `tokenizer.get_vocab()`, lembre-se que **"tokenizer"** √© um objeto transformers.

```python
vocabulary = tokenizer.get_vocab()
for vocab in list(vocabulary.items())[:10]:
    print(vocab)
```

**OUTPUT:**
```bash
('titan', 16537)
('##vu', 19722)
('ascended', 19644)
('coats', 15695)
('persist', 29486)
('squeak', 29552)
('‚Çç', 1558)
('##erine', 24226)
('##irus', 26013)
('##valent', 24879)
```

> **OBSERVA√á√ÉO:**  
> Com isso n√≥s temos o b√°sico de um `encode()`.

</details>




















---



<div id="intro-to-decode"></div>

## decode()

> Um **decode()** √© um m√©todo (ou fun√ß√£o) que transforma uma sequ√™ncia de IDs de tokens de volta em texto leg√≠vel (strings).

Por exemplo:

![img](images/decode-01.png)  

Vamos ver como implementar isso na pr√°tica:

<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

Da mesma maneira que a biblioteca tem um mecanismo de `encode()`, ela tem um mecanismo de `decode()`:

[transformers_encode_decode.py](src/transformers_encode_decode.py)
```python
# decode process = here we convert IDs back to text
token_ids = encoding["input_ids"]
decoded_text = tokenizer.decode(token_ids)

print("\nDecoded text (first 353 chars):\n", decoded_text[:353])
```

**OUTPUT:**
```bash
Decoded text (first 353 chars):
 [CLS] i had always thought jack gisburn rather a cheap genius - - though a good fellow enough - - so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, marrie
d a rich widow, and established himself in a villa on the riviera. ( though i rather thought it would have been rome or florence. ) " the height
```

</details>




















---

<div id="sliding-window"></div>

## Sliding Window (input-target)

Como j√° aprendemos, alguns LLMs s√£o pr√©-treinados para **prever da pr√≥xima palavra em um texto**, por exemplo:

![img](images/sliding-window-01.png)  

Vamos ver um exemplo mais f√°cil:

```python
"O rato roeu a roupa do rei de Roma"
```

**Tokenizado:**
```python
tokens = ["O", "rato", "roeu", "a", "roupa", "do", "rei", "de", "Roma"]
```

Agora, criamos pares `input ‚Üí target` com *Sliding Window*:

| Input                                                    | Target    |
| -------------------------------------------------------- | --------- |
| `["O"]`                                                  | `"rato"`  |
| `["O", "rato"]`                                          | `"roeu"`  |
| `["O", "rato", "roeu"]`                                  | `"a"`     |
| `["O", "rato", "roeu", "a"]`                             | `"roupa"` |
| `["O", "rato", "roeu", "a", "roupa"]`                    | `"do"`    |
| `["O", "rato", "roeu", "a", "roupa", "do"]`              | `"rei"`   |
| `["O", "rato", "roeu", "a", "roupa", "do", "rei"]`       | `"de"`    |
| `["O", "rato", "roeu", "a", "roupa", "do", "rei", "de"]` | `"Roma"`  |

### üß† Como o modelo aprende?

O modelo recebe o **input (lista de tokens)** e √© treinado para prever o **target (pr√≥ximo token)**. Isso √© a base do aprendizado de linguagem.

### ‚úÖ Resumo

| Conceito            | Explica√ß√£o                                                   |
| ------------------- | ------------------------------------------------------------ |
| **O que √©?**        | T√©cnica que divide texto longo em blocos sobrepostos         |
| **Para que serve?** | Permitir input cont√≠nuo para LLMs com limita√ß√£o de tokens    |
| **Quando usar?**    | Em textos longos, gera√ß√£o de texto, fine-tuning              |
| **Quando evitar?**  | Quando o modelo aceita entradas longas ou a tarefa √© pequena |

Agora vamos ver como implementar isso na pr√°tica:






























































































<!--- ( üöÄ Instala√ß√£o / Execu√ß√£o local ) --->

---

<div id="settings"></div>

## üöÄ Instala√ß√£o / Execu√ß√£o local

*Crie e ative o ambiente virtual (recomendado):**  

```bash
python -m venv environment
```

**LINUX:**  
```bash
source environment/bin/activate
```

**WINDOWS:**  
```bash
source environment/Scripts/activate
```

**ATUALIZE O PIP:**
```bash
python -m pip install --upgrade pip
```

**Instale as depend√™ncias:**  

```bash
pip install -U -v --require-virtualenv -r requirements.txt
```










<!--- ( REFER√äNCIAS ) --->

---

<div id="ref"></div>

## REFER√äNCIAS

 - **AI Agents:**
   - [ChatGPT](https://chat.openai.com/)
 - **Books:**
   - [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**
<!--->

<details>

<summary>Python (From Scratch)</summary>

<br/>

[](src/)
```python

```

**OUTPUT:**
```bash

```

</details>
