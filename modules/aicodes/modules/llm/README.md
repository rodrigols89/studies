# Large Language Models

## Conte√∫do

 - **Introdu√ß√£o a LLMs:**
   - [O que s√£o LLMs?](#intro-to-llm)
   - [Como o modelo "entende" linguagem?](#how-understand)
   - [Como LLMs s√£o treinados)](#how-are-trained)
   - [Diferen√ßa entre LLMs e modelos tradicionais de NLP](#llm-vs-nlp)
   - [Por que os Transformers revolucionaram o NLP?](#transformers-inovation)
   - [Como funciona o Mecanismo de Attention (Aten√ß√£o)?](#attention-mechanism)
   - [Exemplos de tarefas resolvidas por LLMs](#llm-examples)
 - **Conceitos utilizados em LLMs:**
   - [Entendendo "Word Embeddings"](#understanding-word-embeddings)
   - [Word2Vec](#word2vec-idea)
 - **Prepara√ß√£o e amostragem de dados (Data preparation & sampling):**
   - **Lendo arquivos (Reading files):**
     - [read_txt()](#read-txt)
   - **Tokenization:**
     - [Tokeniza√ß√£o de texto (Tokenizing text)](#tokenization)
     - [Convertendo tokens em IDs de token (Converting tokens into token IDs)](#token-id)
 - **Mecanismo de aten√ß√£o (Attention mechanism):**
 - **Arquiteturas de LLMs (LLMs architecture):**
 - **Pr√©-treinamento (Pretraining):**
 - **Loop de treinamento (Training loop):**
 - **Avalia√ß√£o do modelo (Model evaluation):**
 - **Carregamento pesos pr√©-treinados (Load pretrained weights):**
 - **Afina√ß√£o (Fine-tuning):**
   - **Modelos de classifica√ß√£o (Classification models):**
   - **Assistentes pessoais ou modelos de chat (Personal assistants or chat models):**
 - [**üöÄ Instala√ß√£o / Execu√ß√£o local**](#settings)
 - [**REFER√äNCIAS**](#ref)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->






































































































<!--- ( Introdu√ß√£o a LLMs ) --->

---

<div id="intro-to-llm"></div>

## O que s√£o LLMs?

**üìò Defini√ß√£o:**  
LLMs (Large Language Models) s√£o modelos de aprendizado de m√°quina treinados para **"entender"**, **"gerar"** e **"manipular linguagem natural"**.

> **OBSERVA√á√ÉO:**  
> Eles s√£o chamados de "grandes" por causa da quantidade massiva de par√¢metros (milh√µes ou bilh√µes) e por serem treinados em grandes volumes de texto da internet, livros, artigos, f√≥runs, c√≥digo-fonte etc.




















---

<div id="how-understand"></div>

### Como o modelo "entende" linguagem?

Na verdade, LLMs n√£o entendem no sentido humano. Eles aprendem probabilidades estat√≠sticas:

> Se eu vejo a frase: "O c√©u est√° ___", a palavra mais prov√°vel √© "azul".  
> Esse "palpite" √© feito com base no que ele viu durante o treinamento.




















---

<div id="how-are-trained"></div>

### Como LLMs s√£o treinados

 - **Pr√©-treinamento (Pretraining):**
   - O modelo √© exposto a grandes quantidades de texto e aprende padr√µes de linguagem por meio de tarefas como **"prever a pr√≥xima palavra"** (auto-regressivo) ou **"preencher palavras faltantes"** (m√°scara).
 - **Ajuste fino (Fine-tuning):**
   - O modelo pode ser adaptado para tarefas espec√≠ficas, como:
     - Classifica√ß√£o de texto;
     - Tradu√ß√£o;
     - Gera√ß√£o de c√≥digo;
     - Resumo de documentos.




















---

<div id="llm-vs-nlp"></div>

## Diferen√ßa entre LLMs e modelos tradicionais de NLP

 - Antes dos LLMs, os modelos de **NLP** eram **espec√≠ficos para cada tarefa**, como *an√°lise de sentimentos*, *tradu√ß√£o*, ou *resumo*.
 - Com os **LLMs**, **um √∫nico modelo pode ser usado para v√°rias tarefas** com pouco ou nenhum ajuste.

### Compara√ß√£o Direta

| Caracter√≠stica                  | Modelos Tradicionais de NLP                 | LLMs (Large Language Models)                        |
| ------------------------------- | ------------------------------------------- | --------------------------------------------------- |
| **Arquitetura**                 | Simples (SVM, Regress√£o, Naive Bayes, RNNs) | Transformer (profundo e em larga escala)            |
| **Treinamento**                 | Um modelo por tarefa                        | Um √∫nico modelo para tarefas m√∫ltiplas              |
| **Requer feature engineering?** | Sim! Manual e demorado                      | N√£o. O modelo aprende tudo automaticamente          |
| **Escalabilidade**              | Limitado                                    | Altamente escal√°vel e flex√≠vel                      |
| **Precis√£o/Desempenho**         | OK, mas limitado com grandes volumes        | Alta, especialmente com dados em larga escala       |
| **Entrada/Sa√≠da**               | Tipicamente vetores num√©ricos               | Texto puro (prompts e respostas)                    |
| **Contexto considerado**        | Curto (√†s vezes s√≥ 1 frase)                 | Longo (v√°rios par√°grafos ou at√© milhares de tokens) |
| **Exemplos**                    | TF-IDF + SVM, Word2Vec + LSTM               | GPT, BERT, T5, LLaMA, Claude, Gemini                |

### Explicando com um exemplo

 - **Modelo tradicional (pr√©-LLM):**
   - Transformar o texto em vetores (TF-IDF, Bag of Words).
   - Treinar um SVM ou uma Regress√£o Log√≠stica para prever o sentimento.
   - Modelo s√≥ serve pra essa tarefa.
 - **LLM:**
   - Voc√™ escreve:
     - `Classifique o sentimento desta frase: Estou muito feliz hoje!`
   - O modelo responde:
     - `Sentimento: Positivo`
   - **OBSERVA√á√ÉO:** O mesmo modelo pode tamb√©m traduzir, resumir, gerar c√≥digo...

### Vantagens dos LLMs sobre modelos tradicionais

 - ‚úÖ **Generaliza√ß√£o:** Um modelo para muitas tarefas;
 - ‚úÖ **Zero-shot & few-shot:** Resolve tarefas com poucas instru√ß√µes;
 - ‚úÖ Menos depend√™ncia de dados rotulados;
 - ‚úÖ Contexto mais longo e melhor compreens√£o;
 - ‚úÖ Gera√ß√£o de linguagem natural mais fluida.

### Mas os modelos tradicionais morreram?

 - **‚ùå N√£o! Eles ainda s√£o √∫teis quando:**
   - Voc√™ tem poucos dados e poucos recursos computacionais;
   - A tarefa √© muito espec√≠fica e n√£o exige interpreta√ß√£o profunda;
   - Voc√™ precisa de explicabilidade clara e r√°pida.




















---

<div id="transformers-inovation"></div>

## Por que os Transformers revolucionaram o NLP?

> Transformers s√£o uma arquitetura introduzida no artigo [**"Attention is All You Need" (2017)**](https://arxiv.org/abs/1706.03762).

**OBSERVA√á√ÉO:**  
Eles eliminaram a necessidade de processar palavras em sequ√™ncia como faziam **RNNs** e **LSTMs** ‚Äî e com isso, permitiram muito mais *"paralelismo"*, *"contexto global"* e *"velocidade"*.

### üö´ Problema dos modelos anteriores (RNN, LSTM)

 - Processavam tokens um por um (sequencialmente).
 - Sofriam com longas depend√™ncias ("o que foi dito 30 palavras atr√°s?").
 - Eram lentos para treinar.
 - Tinha dificuldade com frases longas e contexto amplo.

### ‚úÖ Como o Transformer resolveu tudo isso?

**A resposta:** Attention Mechanism.

> O modelo aprende a **"prestar aten√ß√£o"** nas palavras mais importantes do texto ‚Äî independentemente da posi√ß√£o!

### üß© Componentes principais do Transformer

| Componente                    | Fun√ß√£o B√°sica                                                    |
| ----------------------------- | ---------------------------------------------------------------- |
| **Embedding**                 | Converte palavras em vetores num√©ricos.                          |
| **Self-Attention**            | Calcula a import√¢ncia de cada palavra em rela√ß√£o √†s outras.      |
| **Positional Encoding**       | Adiciona informa√ß√£o da posi√ß√£o das palavras (j√° que √© paralelo). |
| **Feedforward Layers**        | Faz transforma√ß√µes profundas nos vetores.                        |
| **Normalization & Residuals** | Ajudam a estabilizar e melhorar o aprendizado.                   |




















---

<div id="attention-mechanism"></div>

## Como funciona o Mecanismo de Attention (Aten√ß√£o)?

> O mecanismo de aten√ß√£o permite que o modelo **"foque" em partes importantes da entrada** ‚Äî como humanos fazem ao ler.

Por exemplo, imagine que n√≥s temos a frase:

Imagine a frase:

> ‚ÄúA ma√ß√£ estava azeda, ent√£o ela foi jogada fora.‚Äù

 - A palavra **"ela"** poderia se referir √† **ma√ß√£** ou √† **azeda**.
 - O modelo precisa **‚Äúprestar aten√ß√£o‚Äù** nas palavras relevantes para entender corretamente.

### üßÆ F√≥rmula matem√°tica (simples)

A f√≥rmula matem√°tica (simples) √© a seguinte:

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$

 - `QK·µÄ`
   - Compara (multiplica) os queries com os keys ‚Üí gera pontua√ß√µes de aten√ß√£o.
 - `/ ‚àöd_k`
   - Normaliza para evitar explos√µes num√©ricas.
 - `softmax`
   - Transforma em probabilidades.
 - `√ó V`
   - Gera a aten√ß√£o ponderada, ou seja, a sa√≠da final.

### üî¢ Exemplo num√©rico ilustrativo

```bash
Frase: ["O", "gato", "correu"]

‚Üí Para "correu", o modelo calcula:

Q(correu) ‚Ä¢ K(O)     ‚Üí  baixa aten√ß√£o
Q(correu) ‚Ä¢ K(gato)  ‚Üí  alta aten√ß√£o
Q(correu) ‚Ä¢ K(correu)‚Üí  m√©dia aten√ß√£o
```

> **Resultado:**  
> O modelo vai ponderar mais o **"gato"**, porque **"gato correu"** tem uma rela√ß√£o forte.




















---

<div id="llm-examples"></div>

### Exemplos de tarefas resolvidas por LLMs

Vamos come√ßar com uma introdu√ß√£o de algumas tarefas que podem ser resolvidas utilizando **"LLMs"**:

| Tarefa                      | Exemplo pr√°tico                             |
| --------------------------- | ------------------------------------------- |
| **Gera√ß√£o de texto**        | Chatbots, reda√ß√£o autom√°tica                |
| **Classifica√ß√£o**           | An√°lise de sentimentos, spam vs. n√£o spam   |
| **Tradu√ß√£o**                | Ingl√™s ‚Üí Portugu√™s, etc.                    |
| **Perguntas e Respostas**   | Assistente de d√∫vidas                       |
| **Resumo autom√°tico**       | Resumir longos artigos                      |
| **Extra√ß√£o de informa√ß√µes** | Pegar nomes, datas, eventos de um texto     |
| **Gera√ß√£o de c√≥digo**       | Auto-complete em IDEs, explica√ß√£o de c√≥digo |

### üöÄ Aplica√ß√µes Reais das LLMs

| **√Årea**                   | **Aplica√ß√£o**                                                                                       |
| -------------------------- | --------------------------------------------------------------------------------------------------- |
| **Assistentes virtuais**   | Chatbots inteligentes (ex: ChatGPT, Google Bard, Alexa)                                             |
| **Educa√ß√£o**               | Tutores personalizados, corre√ß√£o autom√°tica de reda√ß√µes, explica√ß√µes sob demanda                    |
| **Sa√∫de**                  | An√°lise de prontu√°rios, resposta a perguntas m√©dicas, apoio √† decis√£o cl√≠nica                       |
| **Atendimento ao cliente** | Respostas autom√°ticas, suporte 24/7, resumo de intera√ß√µes com usu√°rios                              |
| **Pesquisa e ci√™ncia**     | S√≠ntese de artigos, gera√ß√£o de hip√≥teses, revis√£o autom√°tica de literatura                          |
| **Programa√ß√£o**            | Autocompletar c√≥digo, explicar fun√ß√µes, gerar trechos em diferentes linguagens (ex: GitHub Copilot) |
| **Tradu√ß√£o de idiomas**    | Tradu√ß√µes contextuais e multil√≠ngues, com adapta√ß√£o ao dom√≠nio espec√≠fico                           |
| **Cria√ß√£o de conte√∫do**    | Gera√ß√£o de artigos, roteiros, marketing, posts para redes sociais                                   |
| **Direito**                | An√°lise de contratos, extra√ß√£o de cl√°usulas, sumariza√ß√£o de decis√µes legais                         |
| **An√°lise de sentimentos** | Classifica√ß√£o de avalia√ß√µes e sentimentos em redes sociais e e-commerce                             |
| **Seguran√ßa cibern√©tica**  | Explica√ß√£o de exploits, an√°lise de logs e gera√ß√£o de alertas                                        |
| **Games e NPCs**           | Di√°logos gerados dinamicamente, comportamentos inteligentes, roteiros                               |

### üìà Exemplos Reais de Impacto com LLMs

| **Organiza√ß√£o / Produto** | **Uso de LLMs**                                                                          |
| --------------------------|------------------------------------------------------------------------------------------|
| `Duolingo`                | Feedback em tempo real sobre frases escritas por alunos com explica√ß√µes contextualizadas |
| `Notion AI`               | Gera√ß√£o e reformula√ß√£o de textos, resumos autom√°ticos, cria√ß√£o de tarefas                |
| `Khan Academy (GPT-4)`    | Tutor personalizado que responde d√∫vidas dos alunos com explica√ß√µes passo a passo        |
| `GitHub Copilot`          | Sugest√µes de c√≥digo em tempo real, explica√ß√µes de fun√ß√µes, gera√ß√£o de testes             |
| `GrammarlyGO`             | Reescrita e aprimoramento de textos com base no contexto do usu√°rio                      |
| `Legal Robot`             | An√°lise de contratos com explica√ß√£o em linguagem natural das cl√°usulas jur√≠dicas         |
| `You.com (YouChat)`       | Motor de busca com respostas geradas por LLM, integrando fontes e interatividade         |







































































































<!--- ( Conceitos utilizados em LLMs ) --->

---

<div id="understanding-word-embeddings"></div>

## Entendendo "Word Embeddings"

 - Modelos de Deep Learning, incluindo LLMs, n√£o podem processar texto bruto diretamente;
 - Como o texto √© categ√≥rico, ele n√£o √© compat√≠vel com as opera√ß√µes matem√°ticas utilizadas para implementar e treinar redes neurais;
 - Portanto, precisamos de uma forma de representar palavras como vetores com valores cont√≠nuos.

> **OBSERVA√á√ÉO:**  
> O conceito de converter dados (√°udio, v√≠deo, texto) para o formato de vetor (num√©rico) √© frequentemente chamado de **"embedding"**.

Por exemplo:

![img](images/word-embeddings-01.png)  

> **OBSERVA√á√ÉO:**  
> No entanto, √© importante observar que diferentes formatos de dados exigem *modelos de embbedding* distintos.  
> Por exemplo, um modelo de embedding projetado para texto n√£o seria adequado para embbedding de dados de √°udio ou v√≠deo.




















---

<div id="word2vec-idea"></div>

## Word2Vec

> A principal *ideia* por tr√°s do **Word2Vec** √© que *"palavras que aparecem em contextos semelhantes tendem a ter significados semelhantes"*.

Consequentemente, quando projetadas em embeddings de palavras bidimensionais para fins de visualiza√ß√£o, palavras semelhantes ficam agrupadas.

![img](images/word2vec-idea-01.png)  







































































































<!--- ( Prepara√ß√£o e amostragem de dados (Data preparation & sampling) ) --->

---

<div id="read-txt"></div>

## read_txt()

Aqui n√≥s vamos aprender como ler um arquivo de texto no formato `.txt`:

<!--- ( Python (From Scratch) ) --->
<details>

<summary>Python (From Scratch)</summary>

</br>

[utils.py](src/utils.py)
```python
import re


def read_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
    return text


if __name__ == "__main__":

    file_path = "../datasets/the-verdict.txt"
    text = read_txt(file_path)

    print("Total number of characters:", len(text))
    print("Text type:", type(text), "\n")

    print(text[:353])
```

**OUTPUT:**
```bash
Total number of characters: 20479
Text type: <class 'str'> 
```

> **OBSERVA√á√ÉO:**  
> Podemos usar o conceito **fatiamento (slicing)** para selecionar uma parte espec√≠fica do texto.

```python
print(text[:353])
```

**OUTPUT:**
```bash
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)

"The height of his glory"
```

</details>




















---

<div id="tokenization"></div>

## Tokeniza√ß√£o de texto (Tokenizing text)

> Aqui, vamos discutir como podemos dividir umma entrada texto *tokens* individuais, uma etapa de pr√©-processamento necess√°ria para criar *embeddings* para um *LLM*.

Esses tokens s√£o palavras individuais ou caracteres especiais, incluindo sinais de pontua√ß√£o, conforme mostrado abaixo:

![img](images/tokenizing-01.png)  

Vamos implementar uma fun√ß√£o `tokenizer_txt()` para ver como isso funciona:

<!--- ( Python (From Scratch) ) --->
<details>

<summary>Python (From Scratch)</summary>

</br>

[tokenizer.py](src/tokenizer.py)
```python
import re

from utils import read_txt


def tokenizer_txt(text):
    text_tokenized = re.split(r'([,.:;?_!"()\']|--|\s)', text)
    text_tokenized = [item.strip() for item in text_tokenized if item.strip()]
    return text_tokenized


if __name__ == "__main__":

    file_path = "../datasets/the-verdict.txt"
    text = read_txt(file_path)

    tokens = tokenizer_txt(text)
    print("Total number of tokens (without whitespaces):", len(tokens))
```

**OUTPUT:**  
```bash
Total number of tokens (without whitespaces): 4690
```

> **OBSERVA√á√ÉO:**  
> Vejam que n√≥s temos 4690 tokens (sem espa√ßos em branco).

Por exemplo, vamos imprimir os primeiros 30 tokens para uma verifica√ß√£o visual r√°pida:

```python
print(tokens[:30])
```

**OUTPUT:**  
```bash
['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']
```

> **OBSERVA√á√ÉO:**  
> Vejam que n√≥s temos uma lista (sim, uma lista normal Python) com todos os tokens, sem espa√ßos em branco do nosso texto.

</details>

<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

Aqui vamos utilizar o tokenizador `AutoTokenizer` da biblioteca [ü§ó Transformers (Hugging Face)](https://github.com/huggingface/transformers) com o modelo `bert-base-uncased`:

[ready_tokenizers.py](src/ready_tokenizers.py)
```python
from transformers import AutoTokenizer

from utils import read_txt


# load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the and read the text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

# Tokenization
tokens = tokenizer.tokenize(text)
print("Total number of tokens (without whitespaces):", len(tokens))
print("Tokens:", tokens[:30])
```

**OUTPUT:**  
```bash
Total number of tokens (without whitespaces): 5212
Tokens: ['i', 'had', 'always', 'thought', 'jack', 'gi', '##sb', '##urn', 'rather', 'a', 'cheap', 'genius', '-', '-', 'though', 'a', 'good', 'fellow', 'enough', '-', '-', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to']
```

> **Por que n√≥s temos mais tokens utilizando o tokenizador da Hugging Face?**

### üß† Diferen√ßa fundamental: Subword Tokenization

O **BERT** n√£o usa tokeniza√ß√£o por palavras ou pontua√ß√µes simples. Ele usa um m√©todo chamado:

> **üëâ WordPiece Tokenization:**  
> Palavras desconhecidas ou raras s√£o quebradas em subpartes, chamadas *"subwords"*.

Por exemplo:

```bash
'Gisburn' ‚Üí ['gi', '##sb', '##urn']
```

A ideia √© balancear entre:

 - Cobertura de vocabul√°rio (poucos tokens desconhecidos);
 - Tamanho do vocabul√°rio (tornar o modelo mais eficiente).

### `üß† O que s√£o esses ##?`

 - No **BERT**, os tokens que come√ßam com `##` s√£o subpalavras que continuam uma palavra anterior.
 - Por exemplo:
   - `gi` √© o come√ßo da palavra `Gisburn`;
   - ``##sb`` e ``##urn`` s√£o subpalavras que continuam `gi` at√© formar `gi + sb + urn = Gisburn`.

Esse tipo de tokeniza√ß√£o:

 - Reduz o n√∫mero de palavras *OOV (out-of-vocabulary)*;
 - Garante que at√© palavras n√£o vistas no treinamento ainda sejam entendidas em partes.

### üìå Outros detalhes que aumentam a contagem no BERT:

| Fator                                 | Explica√ß√£o                                                                        |
| ------------------------------------- | --------------------------------------------------------------------------------- |
| **Subwords**                          | Palavras como `Gisburn`, `unbelievable` viram v√°rias partes                       |
| **Lowercasing**                       | O `bert-base-uncased` transforma tudo em min√∫sculo antes de tokenizar             |
| **Tokens especiais (em outros usos)** | `[CLS]`, `[SEP]` etc. (no seu caso n√£o est√£o aparecendo porque voc√™ s√≥ tokenizou) |
| **Sem filtragem de pontua√ß√£o**        | O tokenizer BERT inclui pontua√ß√µes como tokens pr√≥prios (`.`, `,`, etc.)          |

### ‚úÖ Conclus√£o

| Tokenizador          | Tipo de tokeniza√ß√£o            | Total de tokens | Exemplo                 |
| -------------------- | ------------------------------ | --------------- | ----------------------- |
| `Seu (com re.split)` | Baseado em pontua√ß√£o e espa√ßos | 4690            | `'Gisburn'`             |
| `BERT ("WordPiece")` | Subword Tokenization           | 5212            | `'gi', '##sb', '##urn'` |

> **OBSERVA√á√ÉO:**  
> O BERT gera mais tokens porque ele quebra palavras em partes menores que est√£o no vocabul√°rio aprendido durante o pr√©-treinamento. Isso permite lidar melhor com palavras raras ou compostas.

</details>




















---

<div id="token-id"></div>

## Convertendo tokens em IDs de token (Converting tokens into token IDs)

Aqui n√≥s vamos ver o processo de atribuir um ID num√©rico √∫nico para cada token (palavra, subpalavra ou s√≠mbolo) com base em um vocabul√°rio fixo do modelo.

### üìò O que √© "Vocabul√°rio" em LLMs?

No contexto de Modelos de Linguagem (LLMs), o vocabul√°rio √© a lista de todos os tokens que o modelo conhece.

**‚úÖ Cada token tem:**

 - Uma forma textual (ex: "hello", "##ing", "!")
 - Um ID √∫nico (ex: 101, 1254, 999)

> **OBSERVA√á√ÉO:**  
> Esse vocabul√°rio √© definido antes do treinamento do modelo, geralmente criado com base em um grande corpus de texto.

### üß† Por que esse vocabul√°rio √© importante?

Porque o modelo s√≥ consegue processar textos usando os *tokens* do seu *vocabul√°rio*. Se uma palavra n√£o estiver nele, ser√° dividida em subpalavras ou marcada como token desconhecido ([UNK]).

Por exemplo, imagine que temos o seguinte texto como entrada:

**Entrada (Input):**
```python
I love learning because I love new things
```

| Token      | Token ID |
| ---------- | -------- |
| `[PAD]`    | 0        |
| `[UNK]`    | 1        |
| `i`        | 2        |
| `love`     | 3        |
| `learning` | 4        |
| `because`  | 5        |
| `new`      | 6        |
| `things`   | 7        |

**üßÆ Tokeniza√ß√£o (simples):**
```python
tokens = ['i', 'love', 'learning', 'because', 'i', 'love', 'new', 'things']
```

> **OBSERVA√á√ÉO:**  
> Mesmo que as palavras **"i"** e **"love"** apare√ßam mais de uma vez, elas ser√£o tokenizadas da mesma forma, pois o vocabul√°rio √© est√°tico.

**üî¢ Convers√£o para Token IDs:**
```python
token_ids = [2, 3, 4, 5, 2, 3, 6, 7]
```

| Palavra      | Token      | Token ID |
| ------------ | ---------- | -------- |
| **I**        | `i`        | 2        |
| **love**     | `love`     | 3        |
| **learning** | `learning` | 4        |
| **because**  | `because`  | 5        |
| **I**        | `i`        | 2        |
| **love**     | `love`     | 3        |
| **new**      | `new`      | 6        |
| **things**   | `things`   | 7        |

**‚úÖ Observa√ß√µes:**

 - Tokens repetidos (como "i" e "love") continuam recebendo o mesmo ID.
 - O modelo trata repeti√ß√µes de **forma contextual**:
   - Ou seja, mesmo com o mesmo token ID, o significado pode *"mudar dependendo do contexto anterior"*.

Agora vamos ver como implementar isso na pr√°tica:


<!--- ( Python (From Scratch) ) --->
<details>

<summary>Python (From Scratch)</summary>

<br/>

Para criar esse mecanismo de adicionar um `id` para cada token vamos precisar utilizar nossa l√≥gica de programa√ß√£o. Por exemplo, n√≥s vamos ter que pegar todas as **"palavras √∫nicas"** do texto:

```python
tokens = tokenizer_txt(text)
unique_tokens = set(tokens)

print("type:", type(unique_tokens))
print("Number of unique tokens:", len(unique_tokens))
```

**OUTPUT:**
```bash
type: <class 'set'>
Number of unique tokens: 1130
```

Veja que:

 - Nos temos um objeto do tipo `set`:
   - Ou seja, n√£o temos objetos repetidos.
 - Com o total de **1130** palavras √∫nicas.
 - **OBSERVA√á√ÉO:** Como `set()` n√£o permite *slicing* n√£o vou mostrar todas as palavras (tokens).

Continuando, tamb√©m seria interessante **ordenar** esse **conjunto (set)**:

```python
tokens = tokenizer_txt(text)
unique_tokens = sorted(set(tokens))
```

> **√ìtimo, n√≥s j√° temos todas as palavras (tokens) √∫nicas do texto e agora?**

Agora √© s√≥ mapear (criando um dicion√°rio) para cada palavra (token) um √≠ndice (id) do vocabul√°rio:

```python
vocabulary = {}
for id, token in enumerate(unique_tokens):
    vocabulary[token] = id
```

> **OBSERVA√á√ÉO:**  
> Vejam que n√≥s fizemos uma troca, *onde era a key ficou o token (valor)*, e onde era o valor ficou o id (key).

Vamos mostrar alguns exemplos:

```python
tokens = tokenizer_txt(text)

unique_tokens = sorted(set(tokens))

vocabulary = {}
for id, token in enumerate(unique_tokens):
    vocabulary[token] = id

for i, item in enumerate(vocabulary.items()):
    print(item)
    if i >= 20:
        break
```

**OUTPUT:**
```bash
('!', 0)
('"', 1)
("'", 2)
('(', 3)
(')', 4)
(',', 5)
('--', 6)
('.', 7)
(':', 8)
(';', 9)
('?', 10)
('A', 11)
('Ah', 12)
('Among', 13)
('And', 14)
('Are', 15)
('Arrt', 16)
('As', 17)
('At', 18)
('Be', 19)
('Begin', 20)
```

√ìtimo, n√≥s j√° temos toda a l√≥gica necess√°ria para criar nossa fun√ß√£o `create_vocabulary()`:

[tokenizer.py](src/tokenizer.py)
```python
def create_vocabulary(tokens):
    unique_tokens = sorted(set(tokens))
    vocabulary = {}
    for id, token in enumerate(unique_tokens):
        vocabulary[token] = id
    return vocabulary


if __name__ == "__main__":

    file_path = "../datasets/the-verdict.txt"
    text = read_txt(file_path)

    tokens = tokenizer_txt(text)  # Tokenize the text
    vocabulary = create_vocabulary(tokens)  # Create the vocabulary

    print("Vocabulary size:", len(vocabulary))

    # Print some vocabulary examples
    for i, item in enumerate(vocabulary.items()):
        print(item)
        if i >= 20:
            break
```

**OUTPUT:**
```bash
Vocabulary size: 1130
('!', 0)
('"', 1)
("'", 2)
('(', 3)
(')', 4)
(',', 5)
('--', 6)
('.', 7)
(':', 8)
(';', 9)
('?', 10)
('A', 11)
('Ah', 12)
('Among', 13)
('And', 14)
('Are', 15)
('Arrt', 16)
('As', 17)
('At', 18)
('Be', 19)
('Begin', 20)
```

</details>


<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

Com a biblioteca **transformers (Hugging Face)** podemos utilizar a fun√ß√£o `tokenizer.get_vocab()` para pegar o vocacul√°rio dos tokens:

[ready_tokenizers.py](src/ready_tokenizers.py)
```python
tokens = tokenizer.tokenize(text)  # Tokenize the text
vocabulary = tokenizer.get_vocab()  # Get the vocabulary

print(list(vocabulary.items())[:10])  # Print the first 10 vocabulary items
```

**OUTPUT:**
```bash
Vocabulary size: 30522
('##ibility', 13464)
('palazzo', 18482)
('1735', 26063)
('carter', 5708)
('transformations', 21865)
('[unused926]', 931)
('nervously', 12531)
('1753', 23810)
('libre', 21091)
('ramon', 12716)
('grange', 18203)
('##ryl', 23320)
('encore', 19493)
('additions', 13134)
('## í', 29704)
('amenities', 19870)
('##gus', 12349)
('##cite', 17847)
('tears', 4000)
('##bber', 29325)
('aggressive', 9376)
```































































































































































<!--- ( üöÄ Instala√ß√£o / Execu√ß√£o local ) --->

---

<div id="settings"></div>

## üöÄ Instala√ß√£o / Execu√ß√£o local

*Crie e ative o ambiente virtual (recomendado):**  

```bash
python -m venv environment
```

**LINUX:**  
```bash
source environment/bin/activate
```

**WINDOWS:**  
```bash
source environment/Scripts/activate
```

**ATUALIZE O PIP:**
```bash
python -m pip install --upgrade pip
```

**Instale as depend√™ncias:**  

```bash
pip install -U -v --require-virtualenv -r requirements.txt
```










<!--- ( REFER√äNCIAS ) --->

---

<div id="ref"></div>

## REFER√äNCIAS

 - [ChatGPT](https://chat.openai.com/)

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**

<!--->

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[](src/)
```python

```

</details>
