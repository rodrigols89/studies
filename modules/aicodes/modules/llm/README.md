# Large Language Models

## Conte√∫do

 - **Fundamentos Te√≥ricos:**
   - [O que s√£o LLMs?](#intro-to-llm)
   - [Como o modelo "entende" linguagem?](#how-understand)
   - [Como LLMs s√£o treinados)](#how-are-trained)
   - [Diferen√ßa entre LLMs e modelos tradicionais de NLP](#llm-vs-nlp)
   - [Por que os Transformers revolucionaram o NLP?](#transformers-inovation)
   - [Como funciona o Mecanismo de Attention (Aten√ß√£o)?](#attention-mechanism)
   - [Exemplos de tarefas resolvidas por LLMs](#llm-examples)
   - [Entendendo "Word Embeddings"](#understanding-word-embeddings)
   - [Word2Vec](#word2vec-idea)
   - [Sliding Window (input-target)](#sliding-window)
 - **Prepara√ß√£o e amostragem de dados (Data preparation & sampling):**
     - [Tokeniza√ß√£o de texto (Tokenizing text)](#tokenization)
     - [Convertendo tokens em IDs de token (Converting tokens into token IDs)](#token-id)
     - [Convertendo IDs de tokens em tensores de incorpora√ß√£o (Embeddings)](#token-id-to-tensor)
     - [Criando token embeddings](#creating-token-embeddings)
 - **Mecanismo de aten√ß√£o (Attention mechanism):**
 - **Arquiteturas de LLMs (LLMs architecture):**
 - **Pr√©-treinamento (Pretraining):**
 - **Loop de treinamento (Training loop):**
 - **Avalia√ß√£o do modelo (Model evaluation):**
 - **Carregamento pesos pr√©-treinados (Load pretrained weights):**
 - **Afina√ß√£o (Fine-tuning):**
   - **Modelos de classifica√ß√£o (Classification models):**
   - **Assistentes pessoais ou modelos de chat (Personal assistants or chat models):**
 - **Utils:**
   - [read_txt()](#read_txt)
 - [**üöÄ Instala√ß√£o / Execu√ß√£o local**](#settings)
 - [**REFER√äNCIAS**](#ref)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->






































































































<!--- ( Fundamentos Te√≥ricos ) --->

---

<div id="intro-to-llm"></div>

## O que s√£o LLMs?

**üìò Defini√ß√£o:**  
LLMs (Large Language Models) s√£o modelos de aprendizado de m√°quina treinados para **"entender"**, **"gerar"** e **"manipular linguagem natural"**.

> **OBSERVA√á√ÉO:**  
> Eles s√£o chamados de "grandes" por causa da quantidade massiva de par√¢metros (milh√µes ou bilh√µes) e por serem treinados em grandes volumes de texto da internet, livros, artigos, f√≥runs, c√≥digo-fonte etc.




















---

<div id="how-understand"></div>

### Como o modelo "entende" linguagem?

Na verdade, LLMs n√£o entendem no sentido humano. Eles aprendem probabilidades estat√≠sticas:

> Se eu vejo a frase: "O c√©u est√° ___", a palavra mais prov√°vel √© "azul".  
> Esse "palpite" √© feito com base no que ele viu durante o treinamento.





















---

<div id="how-are-trained"></div>

### Como LLMs s√£o treinados

 - **Pr√©-treinamento (Pretraining):**
   - O modelo √© exposto a grandes quantidades de texto e aprende padr√µes de linguagem por meio de tarefas como **"prever a pr√≥xima palavra"** (auto-regressivo) ou **"preencher palavras faltantes"** (m√°scara).
 - **Ajuste fino (Fine-tuning):**
   - O modelo pode ser adaptado para tarefas espec√≠ficas, como:
     - Classifica√ß√£o de texto;
     - Tradu√ß√£o;
     - Gera√ß√£o de c√≥digo;
     - Resumo de documentos.





















---

<div id="llm-vs-nlp"></div>

## Diferen√ßa entre LLMs e modelos tradicionais de NLP

 - Antes dos LLMs, os modelos de **NLP** eram **espec√≠ficos para cada tarefa**, como *an√°lise de sentimentos*, *tradu√ß√£o*, ou *resumo*.
 - Com os **LLMs**, **um √∫nico modelo pode ser usado para v√°rias tarefas** com pouco ou nenhum ajuste.

### Compara√ß√£o Direta

| Caracter√≠stica                  | Modelos Tradicionais de NLP                 | LLMs (Large Language Models)                        |
| ------------------------------- | ------------------------------------------- | --------------------------------------------------- |
| **Arquitetura**                 | Simples (SVM, Regress√£o, Naive Bayes, RNNs) | Transformer (profundo e em larga escala)            |
| **Treinamento**                 | Um modelo por tarefa                        | Um √∫nico modelo para tarefas m√∫ltiplas              |
| **Requer feature engineering?** | Sim! Manual e demorado                      | N√£o. O modelo aprende tudo automaticamente          |
| **Escalabilidade**              | Limitado                                    | Altamente escal√°vel e flex√≠vel                      |
| **Precis√£o/Desempenho**         | OK, mas limitado com grandes volumes        | Alta, especialmente com dados em larga escala       |
| **Entrada/Sa√≠da**               | Tipicamente vetores num√©ricos               | Texto puro (prompts e respostas)                    |
| **Contexto considerado**        | Curto (√†s vezes s√≥ 1 frase)                 | Longo (v√°rios par√°grafos ou at√© milhares de tokens) |
| **Exemplos**                    | TF-IDF + SVM, Word2Vec + LSTM               | GPT, BERT, T5, LLaMA, Claude, Gemini                |

### Explicando com um exemplo

 - **Modelo tradicional (pr√©-LLM):**
   - Transformar o texto em vetores (TF-IDF, Bag of Words).
   - Treinar um SVM ou uma Regress√£o Log√≠stica para prever o sentimento.
   - Modelo s√≥ serve pra essa tarefa.
 - **LLM:**
   - Voc√™ escreve:
     - `Classifique o sentimento desta frase: Estou muito feliz hoje!`
   - O modelo responde:
     - `Sentimento: Positivo`
   - **OBSERVA√á√ÉO:** O mesmo modelo pode tamb√©m traduzir, resumir, gerar c√≥digo...

### Vantagens dos LLMs sobre modelos tradicionais

 - ‚úÖ **Generaliza√ß√£o:** Um modelo para muitas tarefas;
 - ‚úÖ **Zero-shot & few-shot:** Resolve tarefas com poucas instru√ß√µes;
 - ‚úÖ Menos depend√™ncia de dados rotulados;
 - ‚úÖ Contexto mais longo e melhor compreens√£o;
 - ‚úÖ Gera√ß√£o de linguagem natural mais fluida.

### Mas os modelos tradicionais morreram?

 - **‚ùå N√£o! Eles ainda s√£o √∫teis quando:**
   - Voc√™ tem poucos dados e poucos recursos computacionais;
   - A tarefa √© muito espec√≠fica e n√£o exige interpreta√ß√£o profunda;
   - Voc√™ precisa de explicabilidade clara e r√°pida.





















---

<div id="transformers-inovation"></div>

## Por que os Transformers revolucionaram o NLP?

> Transformers s√£o uma arquitetura introduzida no artigo [**"Attention is All You Need" (2017)**](https://arxiv.org/abs/1706.03762).

**OBSERVA√á√ÉO:**  
Eles eliminaram a necessidade de processar palavras em sequ√™ncia como faziam **RNNs** e **LSTMs** ‚Äî e com isso, permitiram muito mais *"paralelismo"*, *"contexto global"* e *"velocidade"*.

### üö´ Problema dos modelos anteriores (RNN, LSTM)

 - Processavam tokens um por um (sequencialmente).
 - Sofriam com longas depend√™ncias ("o que foi dito 30 palavras atr√°s?").
 - Eram lentos para treinar.
 - Tinha dificuldade com frases longas e contexto amplo.

### ‚úÖ Como o Transformer resolveu tudo isso?

**A resposta:** Attention Mechanism.

> O modelo aprende a **"prestar aten√ß√£o"** nas palavras mais importantes do texto ‚Äî independentemente da posi√ß√£o!

### üß© Componentes principais do Transformer

| Componente                    | Fun√ß√£o B√°sica                                                    |
| ----------------------------- | ---------------------------------------------------------------- |
| **Embedding**                 | Converte palavras em vetores num√©ricos.                          |
| **Self-Attention**            | Calcula a import√¢ncia de cada palavra em rela√ß√£o √†s outras.      |
| **Positional Encoding**       | Adiciona informa√ß√£o da posi√ß√£o das palavras (j√° que √© paralelo). |
| **Feedforward Layers**        | Faz transforma√ß√µes profundas nos vetores.                        |
| **Normalization & Residuals** | Ajudam a estabilizar e melhorar o aprendizado.                   |





















---

<div id="attention-mechanism"></div>

## Como funciona o Mecanismo de Attention (Aten√ß√£o)?

> O mecanismo de aten√ß√£o permite que o modelo **"foque" em partes importantes da entrada** ‚Äî como humanos fazem ao ler.

Por exemplo, imagine que n√≥s temos a frase:

Imagine a frase:

> ‚ÄúA ma√ß√£ estava azeda, ent√£o ela foi jogada fora.‚Äù

 - A palavra **"ela"** poderia se referir √† **ma√ß√£** ou √† **azeda**.
 - O modelo precisa **‚Äúprestar aten√ß√£o‚Äù** nas palavras relevantes para entender corretamente.

### üßÆ F√≥rmula matem√°tica (simples)

A f√≥rmula matem√°tica (simples) √© a seguinte:

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$

 - `QK·µÄ`
   - Compara (multiplica) os queries com os keys ‚Üí gera pontua√ß√µes de aten√ß√£o.
 - `/ ‚àöd_k`
   - Normaliza para evitar explos√µes num√©ricas.
 - `softmax`
   - Transforma em probabilidades.
 - `√ó V`
   - Gera a aten√ß√£o ponderada, ou seja, a sa√≠da final.

### üî¢ Exemplo num√©rico ilustrativo

```bash
Frase: ["O", "gato", "correu"]

‚Üí Para "correu", o modelo calcula:

Q(correu) ‚Ä¢ K(O)     ‚Üí  baixa aten√ß√£o
Q(correu) ‚Ä¢ K(gato)  ‚Üí  alta aten√ß√£o
Q(correu) ‚Ä¢ K(correu)‚Üí  m√©dia aten√ß√£o
```

> **Resultado:**  
> O modelo vai ponderar mais o **"gato"**, porque **"gato correu"** tem uma rela√ß√£o forte.





















---

<div id="llm-examples"></div>

### Exemplos de tarefas resolvidas por LLMs

Vamos come√ßar com uma introdu√ß√£o de algumas tarefas que podem ser resolvidas utilizando **"LLMs"**:

| Tarefa                      | Exemplo pr√°tico                             |
| --------------------------- | ------------------------------------------- |
| **Gera√ß√£o de texto**        | Chatbots, reda√ß√£o autom√°tica                |
| **Classifica√ß√£o**           | An√°lise de sentimentos, spam vs. n√£o spam   |
| **Tradu√ß√£o**                | Ingl√™s ‚Üí Portugu√™s, etc.                    |
| **Perguntas e Respostas**   | Assistente de d√∫vidas                       |
| **Resumo autom√°tico**       | Resumir longos artigos                      |
| **Extra√ß√£o de informa√ß√µes** | Pegar nomes, datas, eventos de um texto     |
| **Gera√ß√£o de c√≥digo**       | Auto-complete em IDEs, explica√ß√£o de c√≥digo |

### üöÄ Aplica√ß√µes Reais das LLMs

| **√Årea**                   | **Aplica√ß√£o**                                                                                       |
| -------------------------- | --------------------------------------------------------------------------------------------------- |
| **Assistentes virtuais**   | Chatbots inteligentes (ex: ChatGPT, Google Bard, Alexa)                                             |
| **Educa√ß√£o**               | Tutores personalizados, corre√ß√£o autom√°tica de reda√ß√µes, explica√ß√µes sob demanda                    |
| **Sa√∫de**                  | An√°lise de prontu√°rios, resposta a perguntas m√©dicas, apoio √† decis√£o cl√≠nica                       |
| **Atendimento ao cliente** | Respostas autom√°ticas, suporte 24/7, resumo de intera√ß√µes com usu√°rios                              |
| **Pesquisa e ci√™ncia**     | S√≠ntese de artigos, gera√ß√£o de hip√≥teses, revis√£o autom√°tica de literatura                          |
| **Programa√ß√£o**            | Autocompletar c√≥digo, explicar fun√ß√µes, gerar trechos em diferentes linguagens (ex: GitHub Copilot) |
| **Tradu√ß√£o de idiomas**    | Tradu√ß√µes contextuais e multil√≠ngues, com adapta√ß√£o ao dom√≠nio espec√≠fico                           |
| **Cria√ß√£o de conte√∫do**    | Gera√ß√£o de artigos, roteiros, marketing, posts para redes sociais                                   |
| **Direito**                | An√°lise de contratos, extra√ß√£o de cl√°usulas, sumariza√ß√£o de decis√µes legais                         |
| **An√°lise de sentimentos** | Classifica√ß√£o de avalia√ß√µes e sentimentos em redes sociais e e-commerce                             |
| **Seguran√ßa cibern√©tica**  | Explica√ß√£o de exploits, an√°lise de logs e gera√ß√£o de alertas                                        |
| **Games e NPCs**           | Di√°logos gerados dinamicamente, comportamentos inteligentes, roteiros                               |

### üìà Exemplos Reais de Impacto com LLMs

| **Organiza√ß√£o / Produto** | **Uso de LLMs**                                                                          |
| --------------------------|------------------------------------------------------------------------------------------|
| `Duolingo`                | Feedback em tempo real sobre frases escritas por alunos com explica√ß√µes contextualizadas |
| `Notion AI`               | Gera√ß√£o e reformula√ß√£o de textos, resumos autom√°ticos, cria√ß√£o de tarefas                |
| `Khan Academy (GPT-4)`    | Tutor personalizado que responde d√∫vidas dos alunos com explica√ß√µes passo a passo        |
| `GitHub Copilot`          | Sugest√µes de c√≥digo em tempo real, explica√ß√µes de fun√ß√µes, gera√ß√£o de testes             |
| `GrammarlyGO`             | Reescrita e aprimoramento de textos com base no contexto do usu√°rio                      |
| `Legal Robot`             | An√°lise de contratos com explica√ß√£o em linguagem natural das cl√°usulas jur√≠dicas         |
| `You.com (YouChat)`       | Motor de busca com respostas geradas por LLM, integrando fontes e interatividade         |





















---

<div id="understanding-word-embeddings"></div>

## Entendendo "Word Embeddings"

 - Modelos de Deep Learning, incluindo LLMs, n√£o podem processar texto bruto diretamente;
 - Como o texto √© categ√≥rico, ele n√£o √© compat√≠vel com as opera√ß√µes matem√°ticas utilizadas para implementar e treinar redes neurais;
 - Portanto, precisamos de uma forma de representar palavras como vetores com valores cont√≠nuos.

> **OBSERVA√á√ÉO:**  
> O conceito de converter dados (√°udio, v√≠deo, texto) para o formato de vetor (num√©rico) √© frequentemente chamado de **"embedding"**.

Por exemplo:

![img](images/word-embeddings-01.png)  

> **OBSERVA√á√ÉO:**  
> No entanto, √© importante observar que diferentes formatos de dados exigem *modelos de embbedding* distintos.  
> Por exemplo, um modelo de embedding projetado para texto n√£o seria adequado para embbedding de dados de √°udio ou v√≠deo.





















---

<div id="word2vec-idea"></div>

## Word2Vec

> A principal *ideia* por tr√°s do **Word2Vec** √© que *"palavras que aparecem em contextos semelhantes tendem a ter significados semelhantes"*.

Consequentemente, quando projetadas em embeddings de palavras bidimensionais para fins de visualiza√ß√£o, palavras semelhantes ficam agrupadas.

![img](images/word2vec-idea-01.png)  





















---

<div id="sliding-window"></div>

## Sliding Window (input-target)

Como j√° aprendemos, alguns LLMs s√£o pr√©-treinados para **prever da pr√≥xima palavra em um texto**, por exemplo:

![img](images/sliding-window-01.png)  

Vamos ver um exemplo mais f√°cil:

```python
"O rato roeu a roupa do rei de Roma"
```

**Tokenizado:**
```python
tokens = ["O", "rato", "roeu", "a", "roupa", "do", "rei", "de", "Roma"]
```

Agora, criamos pares `input ‚Üí target` com *Sliding Window*:

| Input                                                    | Target    |
| -------------------------------------------------------- | --------- |
| `["O"]`                                                  | `"rato"`  |
| `["O", "rato"]`                                          | `"roeu"`  |
| `["O", "rato", "roeu"]`                                  | `"a"`     |
| `["O", "rato", "roeu", "a"]`                             | `"roupa"` |
| `["O", "rato", "roeu", "a", "roupa"]`                    | `"do"`    |
| `["O", "rato", "roeu", "a", "roupa", "do"]`              | `"rei"`   |
| `["O", "rato", "roeu", "a", "roupa", "do", "rei"]`       | `"de"`    |
| `["O", "rato", "roeu", "a", "roupa", "do", "rei", "de"]` | `"Roma"`  |

### üß† Como o modelo aprende?

O modelo recebe o **input (lista de tokens)** e √© treinado para prever o **target (pr√≥ximo token)**. Isso √© a base do aprendizado de linguagem.

### ‚úÖ Resumo

| Conceito            | Explica√ß√£o                                                   |
| ------------------- | ------------------------------------------------------------ |
| **O que √©?**        | T√©cnica que divide texto longo em blocos sobrepostos         |
| **Para que serve?** | Permitir input cont√≠nuo para LLMs com limita√ß√£o de tokens    |
| **Quando usar?**    | Em textos longos, gera√ß√£o de texto, fine-tuning              |
| **Quando evitar?**  | Quando o modelo aceita entradas longas ou a tarefa √© pequena |







































































































<!--- ( Prepara√ß√£o e amostragem de dados (Data preparation & sampling) ) --->

---

<div id="tokenization"></div>

## Tokeniza√ß√£o de texto (Tokenizing text)

> Aqui, vamos discutir como podemos dividir uma entrada texto em *tokens* individuais, uma etapa de pr√©-processamento necess√°ria para criar *embeddings* para um *LLM*.

Esses tokens s√£o palavras individuais ou caracteres especiais, incluindo sinais de pontua√ß√£o, conforme mostrado abaixo:

![img](images/tokenizing-01.png)  

Vamos ver como implementar isso na pr√°tica:

<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

Para criar tokens com a biblioteca biblioteca [ü§ó Transformers (Hugging Face)](https://github.com/huggingface/transformers) vamos utilizar o tokenizador `AutoTokenizer` com o modelo `bert-base-uncased`:

[transformers_tokenizers.py](src/transformers_tokenizers.py)
```python
from transformers import AutoTokenizer

from utils import read_txt


# load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the and read the text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

# Tokenization
tokens = tokenizer.tokenize(text)

print("Total number of tokens (without whitespaces):", len(tokens))
print("Tokens (first 10):", tokens[:10])
```

**OUTPUT:**  
```bash
Total number of tokens (without whitespaces): 5212
Tokens (first 10): ['i', 'had', 'always', 'thought', 'jack', 'gi', '##sb', '##urn', 'rather', 'a']
```

> **Por que n√≥s temos mais tokens utilizando o tokenizador da Hugging Face?**

### üß† Diferen√ßa fundamental: Subword Tokenization

O **BERT** n√£o usa tokeniza√ß√£o por palavras ou pontua√ß√µes simples. Ele usa um m√©todo chamado:

> **üëâ WordPiece Tokenization:**  
> Palavras desconhecidas ou raras s√£o quebradas em subpartes, chamadas *"subwords"*.

Por exemplo:

```bash
'Gisburn' ‚Üí ['gi', '##sb', '##urn']
```

A ideia √© balancear entre:

 - Cobertura de vocabul√°rio (poucos tokens desconhecidos);
 - Tamanho do vocabul√°rio (tornar o modelo mais eficiente).

### `üß† O que s√£o esses ##?`

 - No **BERT**, os tokens que come√ßam com `##` s√£o subpalavras que continuam uma palavra anterior.
 - Por exemplo:
   - `gi` √© o come√ßo da palavra `Gisburn`;
   - ``##sb`` e ``##urn`` s√£o subpalavras que continuam `gi` at√© formar `gi + sb + urn = Gisburn`.

Esse tipo de tokeniza√ß√£o:

 - Reduz o n√∫mero de palavras *OOV (out-of-vocabulary)*;
 - Garante que at√© palavras n√£o vistas no treinamento ainda sejam entendidas em partes.

### üìå Outros detalhes que aumentam a contagem no BERT:

| Fator                                 | Explica√ß√£o                                                                        |
| ------------------------------------- | --------------------------------------------------------------------------------- |
| **Subwords**                          | Palavras como `Gisburn`, `unbelievable` viram v√°rias partes                       |
| **Lowercasing**                       | O `bert-base-uncased` transforma tudo em min√∫sculo antes de tokenizar             |
| **Tokens especiais (em outros usos)** | `[CLS]`, `[SEP]` etc. (no seu caso n√£o est√£o aparecendo porque voc√™ s√≥ tokenizou) |
| **Sem filtragem de pontua√ß√£o**        | O tokenizer BERT inclui pontua√ß√µes como tokens pr√≥prios (`.`, `,`, etc.)          |

### ‚úÖ Conclus√£o

| Tokenizador          | Tipo de tokeniza√ß√£o            | Total de tokens | Exemplo                 |
| -------------------- | ------------------------------ | --------------- | ----------------------- |
| `Seu (com re.split)` | Baseado em pontua√ß√£o e espa√ßos | 4690            | `'Gisburn'`             |
| `BERT ("WordPiece")` | Subword Tokenization           | 5212            | `'gi', '##sb', '##urn'` |

> **OBSERVA√á√ÉO:**  
> O BERT gera mais tokens porque ele quebra palavras em partes menores que est√£o no vocabul√°rio aprendido durante o pr√©-treinamento. Isso permite lidar melhor com palavras raras ou compostas.

</details>





















---

<div id="token-id"></div>

## Convertendo tokens em IDs de token (Converting tokens into token IDs)

Aqui n√≥s vamos ver o processo de atribuir um ID num√©rico √∫nico para cada token (palavra, subpalavra ou s√≠mbolo) com base em um vocabul√°rio fixo do modelo.

### üìò O que √© "Vocabul√°rio" em LLMs?

No contexto de Modelos de Linguagem (LLMs), o vocabul√°rio √© a lista de todos os tokens que o modelo conhece.

**‚úÖ Cada token tem:**

 - Uma forma textual (ex: "hello", "##ing", "!")
 - Um ID √∫nico (ex: 101, 1254, 999)

> **OBSERVA√á√ÉO:**  
> Esse vocabul√°rio √© definido antes do treinamento do modelo, geralmente criado com base em um grande corpus de texto.

### üß† Por que esse vocabul√°rio √© importante?

Porque o modelo s√≥ consegue processar textos usando os *tokens* do seu *vocabul√°rio*. Se uma palavra n√£o estiver nele, ser√° dividida em subpalavras ou marcada como token desconhecido ([UNK]).

Por exemplo, imagine que temos o seguinte texto como entrada:

**Entrada (Input):**
```python
I love learning because I love new things
```

| Token      | Token ID |
| ---------- | -------- |
| `[PAD]`    | 0        |
| `[UNK]`    | 1        |
| `i`        | 2        |
| `love`     | 3        |
| `learning` | 4        |
| `because`  | 5        |
| `new`      | 6        |
| `things`   | 7        |

**üßÆ Tokeniza√ß√£o (simples):**
```python
tokens = ['i', 'love', 'learning', 'because', 'i', 'love', 'new', 'things']
```

> **OBSERVA√á√ÉO:**  
> Mesmo que as palavras **"i"** e **"love"** apare√ßam mais de uma vez, elas ser√£o tokenizadas da mesma forma, pois o vocabul√°rio √© est√°tico.

**üî¢ Convers√£o para Token IDs:**
```python
token_ids = [2, 3, 4, 5, 2, 3, 6, 7]
```

| Palavra      | Token      | Token ID |
| ------------ | ---------- | -------- |
| **I**        | `i`        | 2        |
| **love**     | `love`     | 3        |
| **learning** | `learning` | 4        |
| **because**  | `because`  | 5        |
| **I**        | `i`        | 2        |
| **love**     | `love`     | 3        |
| **new**      | `new`      | 6        |
| **things**   | `things`   | 7        |

**‚úÖ Observa√ß√µes:**

 - Tokens repetidos (como "i" e "love") continuam recebendo o mesmo ID.
 - O modelo trata repeti√ß√µes de **forma contextual**:
   - Ou seja, mesmo com o mesmo token ID, o significado pode *"mudar dependendo do contexto anterior"*.

Agora vamos ver como implementar isso na pr√°tica:

<!--- ( Transformers (Hugging Face) ) --->
<details>

<summary>Transformers (Hugging Face)</summary>

</br>

Com a biblioteca **transformers (Hugging Face)** podemos utilizar a fun√ß√£o `tokenizer.get_vocab()` para pegar o vocacul√°rio dos tokens:

[transformers_tokenizers.py](src/transformers_tokenizers.py)
```python
from transformers import AutoTokenizer

from utils import read_txt


# load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the and read the text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

tokens = tokenizer.tokenize(text)   # Tokenize the text
vocabulary = tokenizer.get_vocab()  # Get the vocabulary

print("Vocabulary type:", type(vocabulary))
for voc in list(vocabulary.items())[:10]:
    print(voc)
```

**OUTPUT:**
```bash
Vocabulary type: <class 'dict'>
('women', 2308)
('[unused452]', 457)
('sponsorship', 12026)
('devout', 26092)
('william', 2520)
('glide', 21096)
('referees', 25118)
('handball', 12378)
('1950s', 4856)
('treated', 5845)
```

</details>





















---

<div id="token-id-to-tensor"></div>

## Convertendo IDs de tokens em tensores de incorpora√ß√£o (Embeddings)

Converter os IDs de tokens em *tensores de incorpora√ß√£o ("Embeddings")* √© a √∫ltima etapa da prepara√ß√£o antes do modelo propriamente dito (LLM) processar os dados.

Vamos ver como fazer isso na pr√°tica:

<!--- ( PyTorch ) --->
<details>

<summary>Transformers (PyTorch)</summary>

<br/>

Para transformar os IDs de tokens em tensores, voc√™ s√≥ precisa ajustar o par√¢metro `return_tensors="pt" (para utilizar PyTorch)`: 

[embeddings_pytorch.py](src/embeddings_pytorch.py)
```python
# encode process = here we tokenize + convert to IDs
encoding = tokenizer.encode_plus(
    text,
    add_special_tokens=True,
    return_tensors="pt",      # pt = Pytorch
    truncation=True,
    padding=False
)

input_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
attention_mask = encoding["attention_mask"]

print("Tensor input_ids shape:", input_ids.shape)
print("Tensor token_type_ids shape:", token_type_ids.shape)
print("Tensor attention_mask shape:", attention_mask.shape)
```

**OUTPUT:**
```bash
Tensor input_ids shape: torch.Size([1, 512])
Tensor token_type_ids shape: torch.Size([1, 512])
Tensor attention_mask shape: torch.Size([1, 512])
```

> **NOTE:**  
> Vejam que agora n√≥s temos **PyTorch tensores**.

</details>

<!--- ( TensorFlow ) --->
<details>

<summary>Transformers (TensorFlow)</summary>

<br/>

Para transformar os IDs de tokens em tensores, voc√™ s√≥ precisa ajustar o par√¢metro `return_tensors="tf" (para utilizar TensorFlow)`: 

[embeddings_tensorflow.py](src/embeddings_tensorflow.py)
```python
encoding = tokenizer.encode_plus(
    text,
    add_special_tokens=True,
    return_tensors="tf",      # tf = Tensorflow
    truncation=True,
    padding=False
)

input_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
attention_mask = encoding["attention_mask"]

print("Tensor input_ids shape:", input_ids[0, :10])
print("Tensor token_type_ids shape:", token_type_ids[0, :10])
print("Tensor attention_mask shape:", attention_mask[0, :10])
```

**OUTPUT:**
```bash
Tensor input_ids shape: tf.Tensor([  101  1045  2018  2467  2245  2990 21025 19022 14287  2738], shape=(10,), dtype=int32)
Tensor token_type_ids shape: tf.Tensor([0 0 0 0 0 0 0 0 0 0], shape=(10,), dtype=int32)
Tensor attention_mask shape: tf.Tensor([1 1 1 1 1 1 1 1 1 1], shape=(10,), dtype=int32)
```

> **NOTE:**  
> Vejam que agora n√≥s temos **TensorFlow tensores**.

</details>





















---

<div id="creating-token-embeddings"></div>

## Criando token embeddings

Token Embedding √© o processo de transformar:

 - Tokens *inteiros* (IDs):
   - Que s√£o n√∫meros inteiros representando palavras ou subpalavras.
 - Em vetores densos de *n√∫meros reais*.

Esse processo √© essencial para que os modelos de linguagem (LLMs) possam trabalhar com texto de forma num√©rica.

Por exemplo:

**Exemplo-01:**  
```bash
token_id  = 1037     # "a" no BERT tokenizer.
embedding = [0.1, 0.5, ..., -0.2]  # vetor de 768 dimens√µes.
```

<br/>

**Exemplo-02:**  
![img](images/token-embedding-01.png)  

<br/>

**Exemplo-03:**  
![img](images/token-embedding-02.png)  

### üß† Por que isso √© importante?

Redes neurais n√£o entendem palavras ou n√∫meros inteiros diretamente ‚Äî elas precisam de *vetores cont√≠nuos* que capturam sem√¢ntica, contexto e rela√ß√£o entre palavras. Embeddings fazem essa ponte.

### üö´ Quando n√£o utilizar?

 - ‚ùå Se estiver usando um modelo pr√©-treinado completo (ex: AutoModel do Hugging Face) ‚Äî os embeddings j√° est√£o l√°.
 - ‚ùå Se estiver apenas tokenizando e n√£o treinando nenhum modelo.

Vamos ver como fazer isso na pr√°tica:

<details>

<summary>Transformer + PyTorch</summary>

<br/>

[token_embedding_pytorch.py](src/token_embedding_pytorch.py)
```python
import torch
from transformers import AutoTokenizer

from utils import read_txt


# load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the and read the text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

# encode process = here we tokenize + convert to IDs
encoding = tokenizer.encode_plus(
    text,
    add_special_tokens=True,  # Add [CLS], [SEP]
    return_tensors="pt",      # pt = Pytorch
    truncation=True,          # Truncates if too long
    padding=False             # Do not add padding
)

# Get the input IDs
input_ids = encoding["input_ids"]

vocab_size = tokenizer.vocab_size     # vocab size
embedding_dim = 768                   # embedding dim

# Create embedding layer
embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)

print("Layer dimensions (shape):", embedding_layer.weight.shape)
print("\nFirst token embedding dimension (shape):", embedding_layer.weight[0].shape)
print("\nFirst token embedding value (tensor):", embedding_layer.weight[0][:20])
```

#### Explica√ß√£o do C√≥digo

 - `vocab_size = tokenizer.vocab_size`
   - Obt√©m o n√∫mero total de tokens √∫nicos no vocabul√°rio do tokenizador `BERT (bert-base-uncased)`.
   - **Exemplo:** Se o `vocab_size` for 30.522 (que √© o nosso caso), significa que existem 30.522 tokens distintos (palavras, subpalavras, pontua√ß√µes, etc.) que o modelo entende.
 - `embedding_dim = 768`
   - Define a dimens√£o dos vetores de embedding.
   - **Por que 768?** Essa √© a dimens√£o usada pelo modelo BERT-base (cada token √© representado como um vetor de 768 n√∫meros).
 - `embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)`
   - **O que faz:** Cria uma camada de embedding do PyTorch.
   - **Como funciona internamente:** √â como uma *"tabela de lookup"* que, dado um *token_id (um n√∫mero entre 0 e vocab_size-1)*, retorna um vetor de tamanho *embedding_dim (nesse caso, 768)*.
   - **Inicializa√ß√£o:** Os vetores s√£o inicializados aleatoriamente (a menos que voc√™ carregue pesos pr√©-treinados).
 - `O que tem dentro de embedding_layer?`
   - A principal coisa que ele cont√©m √© a tabela de embeddings ‚Äì uma matriz de pesos onde cada linha representa o vetor associado a um token:
     - `Linha[0]:` 768 pesos (weight) para o token 0.
     - `Linha[1]:` 768 pesos (weight) para o token 1.
     - ...
     - `Linha[vocab_size-1]:` 768 pesos (weight) para o token `vocab_size-1`.

**OUTPUT:**
```bash
Layer dimensions (shape): torch.Size([30522, 768])

First token embedding dimension (shape): torch.Size([768])

First token embedding value (tensor): tensor([-0.2344, -0.0046, -2.6109, -1.0261,  0.7495, -0.0959,  2.9716, -1.6094,
        -0.1729,  0.0674, -0.6070, -1.8236,  1.1297,  0.4856, -1.7770, -0.8983,
        -1.3048, -1.2164, -0.1922, -0.0672], grad_fn=<SliceBackward0>)
```

Vejam que:

 - `print("Layer dimensions (shape):", embedding_layer.weight.shape)`
   - Aqui n√≥s temos uma camada embedding com 30.522 tokens, cada um com 768 valores (pesos/weights):
     - Como n√≥s sabemos esse valores (pesos/weights) foram inicializados aleatoriamente.
 - `print("\nFirst token embedding dimension (shape):", embedding_layer.weight[0].shape)`
   - Aqui n√≥s estamos exibindo as dimens√µes do primeiro token (palavra √∫nica) na tabela de embeddings:
     - Nada mas do que uma lista com 768 valores (pesos/weights), *reais*.
 - `print("First token embedding value (tensor):", embedding_layer.weight[0][:20])`
   - Aqui n√≥s estamos exibindo os 20 pesos (weights) do primeiro token (palavra √∫nica) da camada de embedding.

Voc√™ pode explorar mais detalhes da nossa camada embedding utilizando:

```python
print(embedding_layer.__dict__)
```

**OUTPUT:**
```bash
{'training': True, '_parameters': {'weight': Parameter containing:
tensor([[ 1.2361,  1.0225, -1.2596,  ...,  0.4168,  1.4311,  0.3879],
        [-0.7385,  0.5236, -1.4423,  ...,  1.0549, -2.3647,  1.2509],
        [ 0.8065,  0.5177, -0.0426,  ..., -0.4130,  0.5765, -0.4022],
        ...,
        [-1.2222, -0.8426,  0.2170,  ..., -0.6425,  0.9004, -1.2794],
        [-0.8202,  0.8905, -0.0465,  ..., -0.2120,  0.7153,  0.7043],
        [ 0.8098,  0.1132,  1.3992,  ...,  0.1763, -0.0457, -0.0235]],
       requires_grad=True)}, '_buffers': {}, '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': Order
edDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': O
rderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': {}, 'num_embeddings': 30522, 'embedding_dim': 768, 'p
adding_idx': None, 'max_norm': None, 'norm_type': 2.0, 'scale_grad_by_freq': False, 'sparse': False}
```

> **OBSERVA√á√ÉO:**  
> Vejam que aqui n√≥s podemos ver todos os par√¢metros e atributos da camada embedding.

</details>



<!--- ( TensorFlow ) --->
<details>

<summary>Transformers + TensorFlow</summary>

<br/>

[token_embedding_tensorflow.py](src/token_embedding_tensorflow.py)
```python
import tensorflow as tf
from transformers import AutoTokenizer

from utils import read_txt

# Load the BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Read the input text
file_path = "../datasets/the-verdict.txt"
text = read_txt(file_path)

# Tokenize and convert text to input IDs
encoding = tokenizer.encode_plus(
    text,
    add_special_tokens=True,   # Add [CLS] and [SEP] tokens
    return_tensors="tf",       # Use TensorFlow tensors
    truncation=True,           # Truncate if the text is too long
    padding=False              # Do not apply padding
)

# Get input IDs
input_ids = encoding["input_ids"]  # shape: (1, seq_len)

# Define vocabulary size and embedding dimension
vocab_size = tokenizer.vocab_size
embedding_dim = 768  # BERT-base uses 768-dimensional embeddings

# Create the embedding layer
embedding_layer = tf.keras.layers.Embedding(
    input_dim=vocab_size,
    output_dim=embedding_dim
)

# Apply embedding lookup on input IDs
embedded_tokens = embedding_layer(input_ids)  # shape: (1, seq_len, 768)

# Print information about the embedding layer
print("Layer dimensions (shape):", embedding_layer.weights[0].shape)
print("\nFirst token embedding dimension (shape):", embedding_layer.weights[0][0].shape)
print("\nFirst token embedding value (tensor):", embedding_layer.weights[0][0][:20])
```

**OUTPUT:**
```bash
Layer dimensions (shape): (30522, 768)

First token embedding dimension (shape): (768,)

First token embedding value (tensor): tf.Tensor(
[-0.04233279 -0.0479785  -0.00253046 -0.02014258  0.01921842 -0.02670938
  0.01929888  0.0185066   0.01841432 -0.04865185  0.01238776 -0.0382238
 -0.00538279  0.0330565  -0.01576235 -0.03277471  0.02024061 -0.03092581
 -0.02762043  0.00779605], shape=(20,), dtype=float32)
```

</details>







































































































<!--- ( Utils ) --->

---

<div id="read-txt"></div>

## read_txt()

Aqui n√≥s vamos aprender como ler um arquivo de texto no formato `.txt`:

<!--- ( Python (From Scratch) ) --->
<details>

<summary>Python (From Scratch)</summary>

</br>

[utils.py](src/utils.py)
```python
import re


def read_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
    return text


if __name__ == "__main__":

    file_path = "../datasets/the-verdict.txt"
    text = read_txt(file_path)

    print("Total number of characters:", len(text))
    print("Text type:", type(text), "\n")
```

**OUTPUT:**
```bash
Total number of characters: 20479
Text type: <class 'str'> 
```

> **OBSERVA√á√ÉO:**  
> Podemos usar o conceito **fatiamento (slicing)** para selecionar uma parte espec√≠fica do texto.

```python
print(text[:353])
```

**OUTPUT:**
```bash
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)

"The height of his glory"
```

</details>







































































































<!--- ( üöÄ Instala√ß√£o / Execu√ß√£o local ) --->

---

<div id="settings"></div>

## üöÄ Instala√ß√£o / Execu√ß√£o local

### Crie e ative o ambiente virtual (recomendado)

```bash
python -m venv environment
```

*LINUX:*  
```bash
source environment/bin/activate
```

*WINDOWS:*  
```bash
source environment/Scripts/activate
```

### Atualize o pip

```bash
python -m pip install --upgrade pip
```

### Instale as depend√™ncias

```bash
pip install -U -v --require-virtualenv -r requirements.txt
```










<!--- ( REFER√äNCIAS ) --->

---

<div id="ref"></div>

## REFER√äNCIAS

 - **AI Agents:**
   - [ChatGPT](https://chat.openai.com/)
 - **Books:**
   - [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**





<details>

<summary></summary>

<br/>

[](src/)
```python

```

**OUTPUT:**
```bash

```

</details>
