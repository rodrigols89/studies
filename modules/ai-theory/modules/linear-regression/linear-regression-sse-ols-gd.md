# Regress√£o Linear (SSE, OLS, GD)

## Conte√∫do

 - [01 - Um problema envolvendo Regress√£o Linear](#01)
 - [02 - M√©todo dos M√≠nimos Quadrados (Sum of Squared Errors: SSE)](#02)
 - [03 - M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)](#03)
 - [04 - M√©todo do Gradiente Descendente](#04)
 - [05 - Fun√ß√£o de Custo](#05)
 - [06 - Tentando minimizar a Fun√ß√£o de Custo](#06)
 - [07 - A Regra da Cadeia](#07)
   - [07.1 - Aplicando a Regra da Cadeia na Fun√ß√£o de Custo](#07-1)
 - [08 - Taxa de Aprendizagem (Learning Rate)](#08)
 - [09 - Aplicando o M√©todo do Gradiente Descendente na pr√°tica](#09)
 - [10 - Tentando fazer previs√µes (predict)](#10)
 - [11 - Interpola√ß√£o vs Extrapola√ß√£o](#11)
 - [12 - Regress√£o Linear com Scikit-Learn](#12)
   - [12.1 - Instalando o Scikit-Learn e salvando no ambiente virtual](#12-1)
   - [12.2 - Criando um conjunto de dados aleat√≥rio (para teste) com Scikit-Learn](#12-2)
   - [12.3 - Criando uma reta de melhor ajuste com Scikit-Learn (+Pegando os melhores valores para "m" e "b")](#12-3)
   - [12.4 - Dividindo os dados em Treino e Teste com Scikit-Learn](#12-4)
   - [12.5 - Tentando fazer previs√µes (predict) com Scikit-Learn](#12-5)

---

<div id='01'></div>

## 01 - Um problema envolvendo Regress√£o Linear

Ok, para come√ßar com nossos exemplos pr√°ticos em **Regress√µes Lineares** vamos imaginar o seguinte... Suponha que n√≥s estamos olhando a rela√ß√£o entre **notas** de alunos e seus **sal√°rios**.

O c√≥digo vai ser o seguinte:

[students.py](src/students.py)
```python
if __name__ =="__main__":

  from matplotlib import pyplot as plt
  import pandas as pd

  students_dic = {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }

  df = pd.DataFrame(students_dic)

  plt.figure(figsize=(10, 7))
  plt.scatter(df.Grade, df.Salary, color='g')
  plt.title('Grades vs Salaries')
  plt.xlabel('Grade')
  plt.ylabel('Salary')
  plt.savefig('../images/plot-01.png', format='png')
  plt.show()
```

**OUTPUT:**  

![image](images/plot-01.png)  

Essas vari√°veis s√£o conhecidas, respectivamente como:

 - **Grade (nota):** *Vari√°veis ‚Äã‚Äãindependentes*, *entradas* ou *preditoras*;
 - **Salary (sal√°rio):** *Vari√°veis ‚Äã‚Äãdependentes*, *sa√≠das* ou *respostas*.

Em uma **Regress√£o Linear** √©  comum denotar as sa√≠das com **ùë¶<sub>i</sub>** e as entradas com **ùë•<sub>i</sub>**. Se houver duas ou mais *Vari√°veis ‚ÄãIndependentes*, elas podem ser representadas como um vetor **ùê± = (ùë•‚ÇÅ,‚Ä¶, ùë•·µ£)**, onde **ùëü (ou n)** √© o n√∫mero de entradas.

Mas ent√£o, como eu consigo analisar as **notas** de alunos e seus **sal√°rios**? Bem, existem v√°rias maneiras, por√©m, vamos ver as mais comuns.

---

<div id='02'></div>

## 02 - M√©todo dos M√≠nimos Quadrados (Sum of Squared Errors: SSE)

> O m√©todo dos M√≠nimos Quadrados calcula o erro para cada ponto **(x<sub>i</sub>, y<sub>i</sub>)** em rela√ß√£o a m√©dia de todas as sa√≠das **y<sub>i</sub>**.

N√£o entendeu? Veja a f√≥rmula abaixo:

![images](images/formula-01.png)  

> Resumidamente, n√≥s estamos tirando a `variancia` de cada ponto **(x<sub>i</sub>, y<sub>i</sub>)** em rela√ß√£o a m√©dia de todos os meus **y**.

Agora vamos transformar tudo isso em Python para ficar algo mais automatizado:

[students_sse.py](src/students_sse.py)
```python
def SSE(dic):
  import pandas as pd

  df = pd.DataFrame(dic)

  df['Error'] = df['Salary'] - df['Salary'].mean()
  df['Squared Errors'] = df['Error']**2
  print(df)
  print("Sum of Squared Errors (SSE): ", round(sum(df['Squared Errors'])))

if __name__ =="__main__":

  students_dic = {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }

  SSE(students_dic)
```

**OUTPUT:**  
```python
    Grade  Salary     Error  Squared Errors
0      50   50000  -22400.0    5.017600e+08
1      50   54000  -18400.0    3.385600e+08
2      46   50000  -22400.0    5.017600e+08
3      95  189000  116600.0    1.359556e+10
4      50   55000  -17400.0    3.027600e+08
5       5   40000  -32400.0    1.049760e+09
6      57   59000  -13400.0    1.795600e+08
7      42   42000  -30400.0    9.241600e+08
8      26   47000  -25400.0    6.451600e+08
9      72   78000    5600.0    3.136000e+07
10     78  119000   46600.0    2.171560e+09
11     60   95000   22600.0    5.107600e+08
12     40   49000  -23400.0    5.475600e+08
13     17   29000  -43400.0    1.883560e+09
14     85  130000   57600.0    3.317760e+09
Sum of Squared Errors (SSE):  26501600000
```

**NOTE:**  
Vale ressaltar aqui que n√≥s estamos elevando todos os erros ao quadrado<sup>2</sup> porque alguns deles v√£o ser negativos, e como n√≥s queremos **somar todos os erros** isso acabaria modificando o resultado. Ou seja, estamos deixando todos positivos.

---

<div id='03'></div>

## 03 - M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)

> At√© ent√£o n√≥s estamos utilizando uma abordagem, onde n√≥s tiravamos a **varian√ßa** de cada ponto em rela√ß√£o a m√©dia de todos os resultados **y**.

Agora vamos utilizar uma abordagem que usa uma **reta de melhor ajuste** para ver se conseguimos um efeito melhor. Ou seja, vamos criar uma reta que fique o mais pr√≥ximo poss√≠vel de todos os pontos; Tanto os pontos acima da linha quanto os abaixo.

Para isso, n√≥s podemos utilizar a **Equa√ß√£o da Reta** para criar uma linha que passe o mais pr√≥ximo poss√≠vel de todos os dados:

![image](images/0001.png)  

Tenho certeza, todos voc√™s aprenderam essa f√≥rmula na escola. Para Regress√£o Linear, usamos s√≠mbolos como estes:

**Exemplo 01:**  
![image](images/0003.png)  

**Exemplo 02:**  
![image](images/0005.png)  

**Exemplo 03:**  
![image](images/0002.png)  

**Exemplo com Nota√ß√£o em Matriz:**  
![image](images/0004.png)  

Eu sei que isso pode acabar confundindo um pouco, mas √© s√≥ se lembrar da *Equa√ß√£o da Reta*, **y = mx + b**, onde:

 - O meu **m (ou a)** representa o Coeficiente Angular:
   - Que altera a inclina√ß√£o da reta;
   - E que √© representado por um valor constante.
 - O meu **b** representa o deslocamento da Reta:
   - Intercepta√ß√£o no eixo-y;
   - Que tamb√©m √© representado por um valor constante.

**NOTE:**  
Para criar essa **reta de melhor ajuste** n√≥s precisamos dos melhores valores poss√≠veis para os termos **m** e **b** `para a amostra que estamos trabalhando`. Isso porque os coeficientes **m** e **b** variam de valores de acordo com os dados que n√≥s temos.

> √ìtimo, entendemos a ideia por tr√°s do **Algoritmo de Regress√£o Linear** que usa uma **reta de melhor ajuste** para medir o **tamanho dos nossos erros** e **explicar a rela√ß√£o entre os dados**.

Uma maneira de tentar encontrar os valores para os coeficiente **m** e **b** √© utilizando as seguintes f√≥rmulas:

![image](images/formula-04.png)  
![image](images/formula-05.png)  

Agora vamos testar essa bruxaria em Python para praticar um pouco:

[students_ols.py](src/students_ols.py)
```python
def OLS(dic):
  import pandas as pd

  df = pd.DataFrame(dic)

  df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
  df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
  df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
  df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

  m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
  b = df['Salary'].mean() - (m * df['Grade'].mean())

  print("Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))

if __name__ =="__main__":

  students_dic = {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }

  OLS(students_dic)
```

**OUTPUT:**  
```python
Angular Coefficient (m): 1516
Linear Coefficient (b): -5732.0
```

Ok, agora que n√≥s j√° temos os melhores valores para os coeficientes **m** e **b** `para esse conjunto de dados`  podemos aplicar eles na **Equa√ß√£o da Reta** e criar a **reta de melhor ajuste**:

[students_ols_bestLineFit.py](src/students_ols_bestLineFit.py)  
```python
def OLS(dic):
  from matplotlib import pyplot as plt
  import pandas as pd

  df = pd.DataFrame(dic)

  df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
  df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
  df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
  df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

  m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
  b = df['Salary'].mean() - (m * df['Grade'].mean())

  print("Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))

  regression_line = [(m*x) + b for x in df['Grade']]

  plt.figure(figsize=(10, 7))
  plt.scatter(df.Grade, df.Salary, color='g')
  plt.plot(df.Grade, regression_line, color='b')
  plt.title('Grades vs Salaries | Ordinary Least Squares: OLS')
  plt.xlabel('Grade')
  plt.ylabel('Salary')
  plt.grid()
  plt.savefig('../images/plot-02.png', format='png')
  plt.show()

if __name__ =="__main__":

  students_dic = {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }

  OLS(students_dic)
```

**OUTPUT:**  

```python
Angular Coefficient (m): 1516
Linear Coefficient (b): -5732.0
```

![image](images/plot-02.png)  

---

![image](images/ohmygod.gif)  

√ìtimo, agora √© s√≥ tirar a `vari√¢ncia` de cada ponto **y<sub>i</sub>** em rela√ß√£o a **reta de melhor ajuste**:

[students_ols_error.py](src/students_ols_error.py)
```python
def OLS_error(dic):
  import pandas as pd

  df = pd.DataFrame(dic)

  df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
  df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
  df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
  df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

  m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
  b = df['Salary'].mean() - (m * df['Grade'].mean())

  df['y = mx + b'] = [(m*x) + b for x in df['Grade']]
  df['y_i - y = mx + b'] = df['Salary'] - df['y = mx + b']
  df['(y_i - y = mx + b)^2'] = df['y_i - y = mx + b'] ** 2

  newDF = df[['Grade', 'Salary', 'y = mx + b', 'y_i - y = mx + b', '(y_i - y = mx + b)^2']]

  print(newDF)
  print("Sum of Squared Errors (OLS): ", round(sum(newDF['(y_i - y = mx + b)^2'])))

if __name__ =="__main__":

  students_dic = {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }

  OLS_error(students_dic)
```

**OUTPUT**
```python
    Grade  Salary     y = mx + b  y_i - y = mx + b  (y_i - y = mx + b)^2
0      50   50000   70075.255242     -20075.255242          4.030159e+08
1      50   54000   70075.255242     -16075.255242          2.584138e+08
2      46   50000   64010.703700     -14010.703700          1.962998e+08
3      95  189000  138301.460094      50698.539906          2.570342e+09
4      50   55000   70075.255242     -15075.255242          2.272633e+08
5       5   40000    1849.050390      38150.949610          1.455495e+09
6      57   59000   80688.220441     -21688.220441          4.703789e+08
7      42   42000   57946.152157     -15946.152157          2.542798e+08
8      26   47000   33687.945987      13312.054013          1.772108e+08
9      72   78000  103430.288725     -25430.288725          6.466996e+08
10     78  119000  112527.116039       6472.883961          4.189823e+07
11     60   95000   85236.634098       9763.365902          9.532331e+07
12     40   49000   54913.876386      -5913.876386          3.497393e+07
13     17   29000   20042.705017       8957.294983          8.023313e+07
14     85  130000  123140.081238       6859.918762          4.705849e+07
Sum of Squared Errors (OLS):  6958885882
```

Agora a *Soma dos Erros* foi reduzida significativamente de **26.501.600.000** para **6.958.885.882**. Isso porque n√≥s estamos utilizando uma reta de melhor ajuste que tem os melhores valores de **m** e **b** `para esse conjunto de dados`, n√£o apenas subtraindo os pontos **(x<sub>i</sub>, y<sub>i</sub>)** pelo a m√©dia de todos os **y**.

**NOTE:**  
Mas essa solu√ß√£o n√£o √© escalon√°vel... Aplicar isso √† Regress√£o Linear foi bastante f√°cil, pois t√≠nhamos bons coeficientes e equa√ß√µes lineares, mas aplicar isso a algoritmos complexos e n√£o lineares como **Support Vector Machine** n√£o seria vi√°vel. 

> Ent√£o vamos encontrar a aproxima√ß√£o num√©rica desta solu√ß√£o por um **m√©todo iterativo**?.

---

<div id='04'></div>

## 04 - M√©todo do Gradiente Descendente

![image](images/scared.gif)  

> Calma gente, rss... Eu sei que esse nome **"Gradiente Descendente"** assusta todo mundo, mas voc√™s v√£o ver como √© simples. √â tudo quest√£o de saber quando e como ele √© aplicado.

Ent√£o, antes n√≥s estavamos utilizando o **M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)** que constru√≠a uma **reta de melhor ajuste** a partir de 2 f√≥rmulas... Agora n√≥s vamos ter que criar a mesma reta, por√©m sem aquelas f√≥rmulas. Isso porque n√≥s vamos utilizar uma **abordagem iterativa**.

Para come√ßar, vamos imaginar a seguinte hist√≥ria:

Considere que 20 pessoas (incluindo voc√™) s√£o lan√ßadas aleatoriamente no ar em uma cadeia de montanhas. Sua tarefa √© encontrar **o pico mais alto de todas as montanhas** em 30 dias.

![image](images/gd-01.jpg)  

Cada um de voc√™s tem um *walkie talkie* para se comunicar e um *alt√≠metro* para medir a altitude. Todos os dias voc√™s passam horas localizando o pico mais alto poss√≠vel e relatam sua altitude mais alta do dia a todos os outros.

![image](images/gd-02.png)  

Suponha que no dia 1 voc√™ relate 1000 p√©s; Outra pessoa informa 1230 p√©s e assim por diante. Depois, h√° uma pessoa que reporta 5000 p√©s, que √© o m√°ximo de todos.

**Lembre-se de que sua tarefa era atingir *coletivamente* o pico mais alto de todas as montanhas.**  
O que voc√™ faz a seguir no dia 2? No dia seguinte, todos se reunir√£o em dire√ß√£o √† √°rea onde a altitude m√°xima foi encontrada ontem. Eles pensar√£o que √© prov√°vel que o pico mais alto seja nesta √°rea.

> Por que algu√©m que reportou 500 p√©s ontem procuraria naquela √°rea de novo se h√° outra √°rea que j√° tem 5.000 p√©s?

**NOTE:**  
Portanto, todos os pesquisadores se movem **"rapidamente"** em dire√ß√£o ao ponto mais alto relatado. Agora, essa gan√¢ncia pode levar voc√™ ao pico mais alto de todas as montanhas, mas tamb√©m pode levar a um erro enorme... O cara que estava a 500 p√©s ontem poderia ter ido em dire√ß√£o a um pico que tinha uma altura de 10.000 p√©s! E ele o ignorou e foi em dire√ß√£o aquele de 5.000 p√©. Voc√™ realmente fica preso em um **M√°ximo Local**. E n√£o h√° como saber se voc√™ est√° preso no **M√°ximo Local**.

**Simulated Annealing:**  
Existe um Algoritmo que seria √∫til para n√≥s nesse caso, **Simulated Annealing:**, onde os pesquisadores teriam pesquisado todo o espa√ßo de pesquisa minuciosamente e sem serem tendenciosos para provavelmente encontrar os **m√°ximos globais**.

Vou deixar alguns exemplos visuais de **Simulated Annealing** para deixar mais claro:

![image](images/simulated-annealing-00.gif)  

---

![image](images/simulated-annealing-01.gif)  

---

![image](images/simulated-annealing-03.gif)  

---

![image](images/simulated-annealing-02.gif)  

Voltando para o problema das montanhas... Se tivesse como equacionar esse problema das Montanhas, ou seja, transforma isso em Matem√°tica n√≥s s√≥ precisar√≠amos tentar chegar (ou encontrar) o ponto **M√°ximo Global** da fun√ß√£o, tentando evitar ficar preso em *M√°ximos Locais*.

Agora vamos voltar para o nosso problema de **Regress√£o Linear**, onde, n√≥s quer√≠amos criar uma **reta de melhor ajuste** utilizando a *Equa√ß√£o da Reta*:

![image](images/0001.png)  

**NOTE:**  
Por√©m, vale lembrar que n√≥s n√£o vamos mais utilizar aquelas f√≥rmulas m√°gicas para tentar encontrar bons valores dos coeficientes **m** e **b** para o nosso conjunto de dados. Isso porque n√≥s vamos utilizar uma **abordagem iterativa**.

**Como assim uma Abordagem Iterativa?**  
Resumidamente, n√≥s vamos ficar tentando valores para os coeficientes **m** e **b** at√© achar a **reta de melhor ajuste** para o nosso conjunto de dados.

Um exemplo bem abstrado (e maluco) poderia ser o seguinte:

![image](images/example-01.png)  

Mas se pensarmos bem, n√≥s resolvemos um problema e agora temos outro *(mesmo que mais simples)*, que √© ficar tentando v√°rios valores para os coeficientes **m** e **b** at√© achar a **reta de melhor ajuste** para o nosso conjunto de dados.

![image](images/ELVN.gif)  

> Ent√£o, temos um problema hein? J√° pensou ter que criar 1 milh√£o de retas o tamanho do recurso computacional que seria gasto?

Ok temos um problema, como escolher os melhores valores poss√≠veis para os meus **m** e **b**? Bem, pensem comigo nas seguintes abordagens:

 - **Primeira abordagem:** N√≥s podemos ir alterando valores de pouco em pouco, por exemplo, 1 unidade por teste em **m** e **b**:
   - O problema √© que se tivermos muito longe dos melhores valores para **m** e **b** isso vai exigir muito recurso computacional.
 - **Segunda abordagem:** Essa abordagem √© muito simples, vamos aumentar valores muito grandes para testes em **m** e **b**. Por exemplo, 1000, 500, 300, 100...

**Qual a melhor abordagem? AS DUAS!**  
√â isso mesmo... Pense comigo, se eu aumentar grandes unidades (valores) para os meus **m** e **b**; E a medida que eles v√£o se aproximando da melhor reta poss√≠vel eu posso ir diminuindo esses valores at√© chegar na **reta de melhor ajuste** para o nosso conjunto de dados.

Ou seja:

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto.

![image](images/problem.gif)  

Ent√£o, o **Gradiente Descendente** √© o bixo de sete cabe√ßas que consegue aplicar esse conceito que n√≥s aprendemos agora. Mas como?

Primeiro vamos ver aqui a matem√°tica das coisas n√©:

**Equa√ß√£o da Reta - (Regress√£o Linear)**  
![image](images/linear-regression-formule-02.png)  

Agora vamos pegar o nosso gr√°fico de compara√ß√£o entre **notas** de alunos e seus **sal√°rios**:

![image](images/plot-01.png)  

Suponha que n√≥s selecionamos um desses pontos, o ponto **y<sub>i</sub>**, algo parecido com isso:

![image](images/example-02.png)  

Agora suponha que n√≥s pegamos a *equa√ß√£o da reta* e desenhamos uma reta qualquer:

![image](images/example-03.png)  

Ent√£o, agora n√≥s estamos com a seguinte situa√ß√£o:
 - Um ponto **y<sub>i</sub>** selecionado;
 - E criamos uma *equa√ß√£o reta* com valores aleat√≥rios para os coeficientes **m** e **b**.

Agora vem a pergunta chave:

> Como eu sei qu√£o distante essa reta est√° dos meus dados? - **Todos eles!**

Pode parecer muito dif√≠cil, mas √© simples:

 - **1¬™ -** Eu vou pegar o meu **x<sub>i</sub>**
 - **2¬™ -** Ver qual foi o resultado da equa√ß√£o da reta nesse ponto.
 
N√£o entendeu? Vamos ver isso visualmente:

![image](images/example-04.png)  

Veja que agora n√≥s temos outro ponto de intersec√ß√£o, por√©m esse vai representar minha **(regress√£o<sub>i</sub>)**. Mas o que isso tem a ver com a nossa pergunta chave?

> Como eu sei qu√£o distante essa reta est√° dos meus dados? - **Todos eles!**

Bem, o ideal seria que essa *equa√ß√£o da reta* passasse bem no nosso ponto **y<sub>i</sub>**,mas como podemos ver tem uma dist√¢ncia entre o meu ponto **y<sub>i</sub>** e a minha **regress√£o<sub>i</sub>**.

U√©, mas se temos uma dist√¢ncia entre esses dois pontos √© s√≥ calcular essa dist√¢ncia n√£o √©?

![image](images/genius.gif)

Ou seja, a diferen√ßa entre a minha **regress√£o<sub>i</sub>** e o meu ponto **y<sub>i</sub>**:

![image](images/example-05.png)  

---

<div id="05"></div>

## 05 - Fun√ß√£o de Custo

**Agora vamos ver alguns detalhes aqui:**

 - **1¬™ -** Essa dist√¢ncia entre o meu ponto **y<sub>i</sub>** e a minha **regress√£o<sub>i</sub>** √© o que n√≥s conhecemos como:
   - **Erro para esse ponto y<sub>i</sub>.**
 - **2¬™ -** N√≥s vamos ter que sair medindo esse erro para todos os pontos do nosso gr√°fico:
   - Ou seja, n√≥s vamos tirar o erro para todos os nossos **y<sub>i</sub>** em rela√ß√£o a essa *reta que n√≥s criamos*.
 - **3¬™ -** E por fim, fazer a soma de todos os erros para essa reta:
   - A soma de todos os erros vai n√≥s da o ***tamanho do nosso erro para essa reta***:
     - Quanto *maior* esse valor, *maior vai ser nosso erro*;
     - Quanto *menor* esse valor, *menor vai ser nosso erro*.

**Ok, mas como eu posso equacionar isso?**  
Bem isso √© o que n√≥s conhecemos como **Fun√ß√£o de Custo**, **Loss Function**, **L**, **J**... Existem v√°rias abordagens.

Alguns exemplos s√£o:

**Example-01:**  
![image](images/01.png)  

**Example-02:**  
![image](images/cf-function-01.png)

**Example-03:**  
![image](images/cf-2m.png)

Essas fun√ß√µes v√£o variar de acordo com o seu problema, Algumas utilizam:

 - **RMSE**;
 - **MSE**;
 - **MAE**
 - **ErroM√©dio**... etc.

Como n√≥s estamos focando apenas em somar todos os nossos erros e mais nada, vamos ficar com o primeiro exemplo **(Example-01)** de **Fun√ß√£o de Custo**.

---

<div id='06'></div>

## 06 - Tentando minimizar a Fun√ß√£o de Custo

Ok, n√≥s j√° temos a nossa **Fun√ß√£o de custo** que vai fazer a soma de todos os nossos erros:

![image](images/01.png)  

Recapitulando:

 - 1¬™ - N√≥s tra√ßamos uma reta;
 - 2¬™ - Calculamos o *erro* para cada ponto na reta;
 - 3¬™ - Por fim, fizemos a *soma de todos os erros* e conseguimos ***tamanho do nosso erro***.
   - N√≥s estamos elevando a subtra√ß√£o de **(reg<sub>i</sub> - y<sub>i</sub>)** PARA CADA ITERA√á√ÉO DO SOMAT√ìRIO ao quadrado<sup>2</sup> porque alguns desses valores podem ser negativos e n√≥s queremos a SOMA DE TODOS ELES.

At√© ai tudo bem, calculamos o ***tamanho do nosso erro*** para uma reta, mas lembra que n√≥s tinhamos uma abordagem?

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto - Para encontrar os melhores valores de **m** e **b**.

Ou seja, n√≥s vamos construir outra reta que n√≥s d√™ um valor menor para a nossa **Fun√ß√£o de Custo**.  
Mas como? Simples, n√≥s vamos passar novos valores para os coeficientes **m** e **b**: 

![image](images/02.png)  

Olhando para a abstra√ß√£o acima n√≥s temos que as nossas vari√°veis **m** e **b** na *equa√ß√£o da reta* v√£o receber novos valores; que v√£o ser *subtra√≠dos* de **algo (something)**. Mas que algo √© esse?  

> Ent√£o, √© ai que entra o conceito de **Derivadas**, **Derivadas Parciais** e **M√≠nimos de uma Fun√ß√£o**.

Suponha que a nossa *equa√ß√£o da reta* tem o seguinte gr√°fico (√© s√≥ um exemplo):

![image](images/example-06.png)  

Bem, no exemplo acima n√≥s temos uma par√°bola (√© s√≥ um exemplo) com v√°rias Taxas de Varia√ß√£o e n√≥s estamos interessando nos **pontos m√≠nimos da fun√ß√£o** para as nossas vari√°veis **m** e **b**.

Agora sabendo disso e da nossa abordagem n√≥s vamos:

 - Derivar a nossa **Fun√ß√£o de Custo** para **m** e para **b**:
   - Dando grandes passos quando estivermos muito longe do ponto m√≠nimo;
   - E passos curtos quando estivermos muito pr√≥ximos do ponto m√≠nimo.

Ou seja, como n√≥s temos mais de uma vari√°vel, n√≥s vamos aplicar o conceito de **Derivadas Parciais**, onde, n√≥s derivamos para uma vari√°vel e deixamos a outra como constante e depois fazemos o mesmo para a outra e vamos diminuindo at√© achar os pontos m√≠nimos poss√≠veis para os coeficientes **m** e **b**:

![image](images/der01.png)  

Ahh, entendido, mas tem 2 observa√ß√µes na abstra√ß√£o acima:

 - 1¬™ - N√≥s temos 2 constantes **Œ± (Alpha)** e **Œ≤ (Beta)** que est√£o multiplicando as nossas Derivadas Parciais;
 - 2¬™ - N√≥s estamos Derivando a nossa **Fun√ß√£o de Custo** para as vari√°veis **m** e **b** da *equa√ß√£o da reta* - **(What?)**.

<div id='07'></div>

## 07 - A Regra da Cadeia

> Por enquanto (mas s√≥ por enquanto) vamos ignorar as constantes **Œ± (Alpha)** e **Œ≤ (Beta)** e vamos relembrar um conceito matem√°tico chamado: **Regra da Cadeia**.

Em c√°lculo, a **Regra da Cadeia** √© uma f√≥rmula para a derivada da fun√ß√£o composta de duas fun√ß√µes - **What?**  
Ok, vamos para os exemplos, suponha que n√≥s temos as seguintes fun√ß√µes:

![image](images/der02.png)  

Veja que agora n√≥s temos 2 fun√ß√µes **y** e **z**, onde:

 - O meu **y** depende de **z**;
 - E o meu **z** depende de **x**.

Agora eu quero Derivar a minha fun√ß√£o **y** para **x**:

![image](images/der03.png)  

U√©, como vai ser? Se o meu **y** depende do **z**; e o meu **z** depende do **x**?

**A REGRA DA CADEIA:**  
√â ai que entra o conceito da **Regra da Cadeia**. Basta seguir a f√≥rmula abaixo:

![image](images/der04.png)  

Agora √© s√≥ tirar a Derivada de **y** para **z**; e de **z** para **x**; e depois multiplicar elas... Vai ficar assim:

![image](images/der05.png)  

**NOTE:**  
√â importante lembrar que algumas dessas vari√°veis tem rela√ß√µes com outra, por exemplo, o meu **z = 3x**, por isso n√≥s mudamos isso na hora de trabalhar a equa√ß√£o.

Mas no fim o que importa √© que a Derivada da fun√ß√£o **y** em rela√ß√£o a **x** √© **18x**.

<div id='07-1'></div>

## 07.1 - Aplicando a Regra da Cadeia na Fun√ß√£o de Custo

Ok, agora que n√≥s j√° revisamos o conceito da **Regra da Cadeia**, como eu posso aplicar ela na minha **Fun√ß√£o de Custo** para os coeficientes **m** e **b**? Bem, vamos ver as rela√ß√µes n√©?

Primeiro vamos pegar nossa **Fun√ß√£o de Custo**:

![image](images/01.png)  

Ok, dentro da *Fun√ß√£o de Custo* n√≥s temos a fun√ß√£o **reg<sub>i</sub>** que depende de **m**; E a minha *Fun√ß√£o de Custo* que depende de **reg<sub>i</sub>**. U√©, s√≥ aplicar a Regra da Cadeia ent√£o...

**NOTE:**  
Primeiro vamos apelidar na nossa fun√ß√£o **(reg<sub>i</sub> + y<sub>i</sub>)<sup>2</sup>** de ***error*** para ficar uma abstra√ß√£o mais nominal.

Agora √© s√≥ tirar a Derivada da **Fun√ß√£o de Custo** para **m** seguindo a *Regra da Cadeia*:

![image](images/der06.png)  

 - Veja que a minha **Fun√ß√£o de Custo** depende do **error**; e o meu **error** depende de **m**;
 - Agora √© s√≥ tirar a Derivada da minha **Fun√ß√£o de Custo** para o **erro**; e do meu **erro** para **m** e depois multiplicar:

![image](images/der07.png)  

Simplificando, a Derivada da minha **Fun√ß√£o de Custo** para **m** vai ser :

![image](images/der08.png)  

√ìtimo, agora √© s√≥ tirar a Derivada da **Fun√ß√£o de Custo** em rela√ß√£o a **b**:

![image](images/der09.png)  

Simplificando novamente:

![image](images/der10.png)  

**Pronto, resolvido!**  
Agora n√≥s j√° temos as Derivadas da **Fun√ß√£o de Custo** para **m** e **b**:

![image](images/der11.png)  

Como os nossos **x<sub>i</sub>** e **y<sub>i</sub>** s√£o valores que n√≥s j√° temos no gr√°fico (nossos dados) n√≥s s√≥ vamos Derivando os coeficientes **m** e **b** at√© chegar o mais pr√≥ximo poss√≠vel do m√≠nimo da **Fun√ß√£o de Custo**, seguindo a nossa abordagem:

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto.

![image](images/gd-graph.gif)  

---

<div id='08'></div>

## 08 - Taxa de Aprendizagem (Learning Rate)

Ok pessoas... Voltando alguns passos atr√°s, lembram que n√≥s tinhamos umas constantes que multiplicavam nossas Derivadas da **Fun√ß√£o de Custo** para **m** e **b**? Eram as constantes **Œ± (Alpha)** e **Œ≤ (Beta)**, mas o que elas significavam?

Ent√£o, isso √© o que n√≥s chamamos de **Taxa de Aprendizado (Learning Rate)**, respons√°veis por ajuste e determina o tamanho da etapa em cada itera√ß√£o enquanto se move em dire√ß√£o ao **M√≠nimo da Fun√ß√£o de Custo**.

A **Taxa de Aprendizado (Learning Rate)** √© um hiperpar√¢metro que controla o quanto alterar o modelo em resposta ao erro estimado cada vez que os pesos do modelo s√£o atualizados. Escolher a taxa de aprendizado √© desafiador, pois um valor muito pequeno pode resultar em um longo processo de treinamento que pode travar, enquanto um valor muito grande pode resultar no aprendizado de um conjunto sub-√≥timo de pesos muito r√°pido ou um processo de treinamento inst√°vel.

Seguindo a nossa abordagem de *Gradiente Descendente* a *Taxa de Veria√ß√£o* vai determina o tamanho dos nossos passos em dire√ß√£o ao **M√≠nimo** da **Fun√ß√£o de Custo**.

Veja os exemplos abaixo:

**Exemplo-01:**  
![image](images/learning-rate-01.jpg)  

**Exemplo-02:**  
![image](images/learning-rate-02.jpg)  

---

<div id='09'></div>

## 09 - Aplicando o M√©todo do Gradiente Descendente na pr√°tica

Ok, mas depois de todas essas **bruxarias te√≥ricas**, como colocar tudo isso em pr√°tica? Bem, vamos ver isso agora com Python sem precisar importar nenhuma biblioteca (mas s√≥ por agora):

[students_gd_bestLine.py](src/students_gd_bestLine.py)  
```python
def GD(dic):
  from matplotlib import pyplot as plt
  import pandas as pd

  df = pd.DataFrame(dic)

  # Start coefficients (Angular and Linear).
  m = 7
  b = 1

  # Learning rate
  learning_rate = 0.000001

  # Search best value for coefficients m and b.
  for i in range(1, 1000+1, 1):
    y_pred = m*df['Grade'] + b
    m_derivative = sum(2*df['Grade']*(y_pred - df['Salary']))
    b_derivative = sum(2*(y_pred - df['Salary']))
    m = m - (learning_rate * m_derivative)
    b = b - (learning_rate * b_derivative)
    # print(m, b) # Remove comments to view step-by-step.

  print("\nAngular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))

  regression_line = [(m*x) + b for x in df['Grade']]

  plt.figure(figsize=(10, 7))
  plt.scatter(df.Grade, df.Salary, color='g')
  plt.plot(df.Grade, regression_line, color='b')
  plt.title('Grades vs Salaries | Gradient descent Approach')
  plt.xlabel('Grade')
  plt.ylabel('Salary')
  plt.grid()
  plt.savefig('../images/plot-03.png', format='png')
  plt.show()

if __name__ =="__main__":

  students_dic = {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }

  GD(students_dic)
```

**OUTPUT:**  
```python
Angular Coefficient (m): 1425
Linear Coefficient (b): -7
```

![image](images/plot-03.png)

**NOTE:**  
Veja que com o **M√©todo do Gradiente Descendente** n√≥s conseguimos melhores valores para os coeficientes **m** e **b** em rela√ß√£o ao **M√©todo dos M√≠nimos Quadrados Ordin√°rios (OLS)**:

 - **M√©todo dos M√≠nimos Quadrados Ordin√°rios (OLS)**:
   - Angular Coefficient (m): 1516
   - Linear Coefficient (b): -5732.0
 - **M√©todo do Gradiente Descendente**:
   - Angular Coefficient (m): 1425
   - Linear Coefficient (b): -7

**NOTE:**  
Voc√™ tamb√©m pode acompanhar todo esse processo passo a passo (step-by-step) removendo o coment√°rio l√° no c√≥digo na linha 26.

[students_gd_bestLine.py](src/students_gd_bestLine.py)
```python
# print(m, b) # Remove comments to view step-by-step.
```

---

<div id="10"></div>

## 10 - Tentando fazer previs√µes (predict)

At√© aqui est√° tudo √≥timo, mas como eu posso `tentar fazer previs√µes (predict)` com esses dados?

Primeiro, vamos recapitular nossa **Equa√ß√£o da Reta (Regress√£o Linear)**:

![img](images/0001.png)  

**NOTE:**  
Lembrem que agora n√≥s temos os os melhores valores para os coeficientes **"m"** e **"b"** que conseguimos com o **Gradiente Descendente**. Agora √© s√≥ tentar prever o valor da **Vari√°vel Dependente "Salary"** com base em algum valor da **Vari√°vel Independente "Grade"**.

Por exemplos, suponha que n√≥s queremos saber quanto vai ganhar um aluno que teve a nota (grade) 85 (vamos come√ßar tentando com um valor j√° existente no nosso conjunto de dados para ver se ele chega perto):

![img](images/predict-01.png)  

**NOTE:**  
Vejam que n√£o ficou muito longe do sal√°rio (Salary) do nosso aluno que tinha a nota (grade) 85 - **130000**  
Lembrem tamb√©m que essas tentativas de previs√µes n√£o s√£o exatas. Nenhum modelo de Machine Learning vai te dar uma predi√ß√£o exata **(S√≥ se for muita coincid√™ncia)**.

Agora vamos tentar aplicar isso em Python para ver como fica?

[predict.py](src/predict.py)
```python
def predict_salary(dic, x_value):
  import pandas as pd

  df = pd.DataFrame(dic)

  # Start coefficients (Angular and Linear).
  m = 7
  b = 1

  # Learning rate
  learning_rate = 0.000001

  # Search best value for coefficients m and b.
  for i in range(1, 1000+1, 1):
    y_pred = m*df['Grade'] + b
    m_derivative = sum(2*df['Grade']*(y_pred - df['Salary']))
    b_derivative = sum(2*(y_pred - df['Salary']))
    m = m - (learning_rate * m_derivative)
    b = b - (learning_rate * b_derivative)
    # print(m, b) # Remove comments to view step-by-step.

  # Linear Regression formula.
  predict_y = m*x_value + b

  print("\nAngular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))
  print("Student with grade {0} may have {1} salary approximately".format(round(x_value), round(predict_y)))



if __name__ =="__main__":

  students_dic = {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }

  predict_salary(students_dic, 85)
```

**OUTPUT:**  
```python
Angular Coefficient (m): 1425
Linear Coefficient (b): -7
Student with grade 85 may have 121083 salary approximately
```

---

<div id="11"></div>

## 11 - Interpola√ß√£o vs Extrapola√ß√£o

No exemplo acima n√≥s estamos procurando um valor dentro do nosso conjunto de dados. Isso √© o que n√≥s conhecemos como **Interpola√ß√£o**.

**N√£o entendeu?**  
Vamos ver isso visualmente ent√£o..

**Interpola√ß√£o:**  
Interpola√ß√£o √© quando encontramos um valor dentro de nosso conjunto de pontos de dados:

![image](images/interpolate.svg)

Aqui, usamos *interpola√ß√£o* linear para estimar as vendas a 21 ¬∞C.

**Extrapola√ß√£o:**  
Extrapola√ß√£o √© quando encontramos um valor fora de nosso conjunto de pontos de dados:

![image](images/extrapolate.svg)  

Aqui, usamos a *extrapola√ß√£o* linear para estimar as vendas a 29 ¬∞C (que √© maior do que qualquer valor que temos).

**Cuidado:**  
A extrapola√ß√£o pode dar resultados enganosos porque estamos em um "territ√≥rio desconhecido".

---

<div id="12"></div>

## 12 - Regress√£o Linear com Scikit-Learn

Ok, at√© aqui j√° vimos todas essas bruxarias matem√°ticas e te√≥ricas, mas como resolver isso de uma maneira mais simples?

Gra√ßas ao Python e a maravilhosa comunidade **Open-Source** n√≥s temos a biblioteca [Scikit-Learn](https://scikit-learn.org/stable/index.html) que deixa todo esse trabalho **MUITO F√ÅCIL**!

<div id="12-1"></div>

## 12.1 - Instalando o Scikit-Learn e salvando no ambiente virtual

Vamos come√ßar instalando a biblioteca:

```
pip install scikit-learn --upgrade
```

**NOTE:**  
Se voc√™ estiver utilizando um ambiente virtual (assim como eu) √© recomendado salvar essa depend√™ncia:

```
pip freeze > requirements.txt
```

Ok, agora vamos ver a nossa vers√£o atual do Scikit-Learn?

[checkVersion.py](src/checkVersion.py)
```python
def checkVersion():
  import sklearn
  print('Scikit-Learn Version: {0}'.format(sklearn.__version__))

if __name__ =='__main__':
  checkVersion()
```

**OUTPUT:**  
```
Scikit-Learn Version: 0.23.1
```

<div id="12-2"></div>

## 12.2 - Criando um conjunto de dados aleat√≥rio (para teste) com Scikit-Learn

√ìtimo, tudo lindo e maravilhoso a nossa disposi√ß√£o! Mas como eu crio um exemplo de **Regress√£o Linear**?  
Simples, primeiro vamos criar um conjunto de dados aleat√≥rios (mesmo sem sentido) para representar o nosso conjunto de dados:

[make_sample.py](src/make_sample.py)
```python
def createRegression(samples, variavel_numbers, n_noise):
  from sklearn.datasets import make_regression
  x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
  return x, y

if __name__ =='__main__':
  from matplotlib import pyplot as plt

  reg = createRegression(200, 1, 30)

  plt.figure(figsize=(10, 7))
  plt.scatter(*reg)
  plt.title('Linear Regression Sample')
  plt.savefig('../images/plot-04.png', format='png')
  plt.show()
```

**OUTPUT:**  
![IMAGE](images/plot-04.png)  

A biblioteca **Scikit-Learn** tem um m√©todo chamado [make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) que pode ser usado para criar um conjunto de dados para que possamos trabalhar com eles. Os argumentos mais comuns (que foram o que n√≥s utilizamos) para esse m√©todo s√£o:

 - **n_samples:** O n√∫mero de Amostra de Dados;
 - **n_features:** O n√∫mero de vari√°veis/caracter√≠sticas;
 - **noise:** Quanto ru√≠do ter√° nosso gr√°fico, ou seja, qu√£o bagun√ßado vai est√° os dados.

**NOTE:**  
Toda vez que voc√™ rodar o c√≥digo [make_sample.py](src/make_sample.py) vai ser gerado um conjunto de dados diferentes. Ou seja, vai ser outro gr√°fico/plot.

**NOTE:**  
Outra observa√ß√£o importante que voc√™ deve prestar aten√ß√£o √© que a fun√ß√£o [make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) nos retornou 2 conjuntos de dados - **x** e **y**, ou seja:

```python
x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
```

 - Os valores aleat√≥rios criados para o nosso **eixo-x**;
 - E os seus correspondente no **eixo-y**.

<div id="12-3"></div>

## 12.3 - Criando uma reta de melhor ajuste com Scikit-Learn (+Pegando os melhores valores para "m" e "b")

√ìtimo, agora voltando para o nosso problema (Regress√£o Linear):

> Como crio uma **reta de melhor ajuste** com Scikit-Learn?

D√° para fazer isso de forma autom√°tica e simples? Claro, com **Scikit-Learn** e suas bruxarias:

[reg-v1.py](src/reg-v1.py)
```python
def createRegression(samples, variavel_numbers, n_noise):
  from sklearn.datasets import make_regression
  x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
  return x, y

if __name__ =='__main__':

  from sklearn.linear_model import LinearRegression
  from matplotlib import pyplot as plt

  reg = createRegression(200, 1, 30)
  model = LinearRegression() # Linear Regression Instance.

  model.fit(*reg)

  a_coeff = model.coef_ # Angular Coefficient - m
  l_coeff = model.intercept_ # Linear Coefficient - b

  print('Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}'.format(a_coeff, l_coeff))

  plt.figure(figsize=(10, 7))
  plt.scatter(*reg)
  plt.plot(reg[0], a_coeff*reg[0] + l_coeff,color='red')
  plt.savefig('../images/plot-05.png', format='png')
  plt.show()
```

**OUTPUT:**  
```python
Angular Coefficient (m): [34.34928509]
Linear Coefficient (b): -0.23447017542042375
```

![IMAGE](images/plot-05.png)  

**Lindo n√£o?**  
Vou comentar apenas as partes cruciais de agora em diante... Come√ßando por aqui:

```python
model.fit(*reg)
```

√â aqui que entra a m√°gica de achar os melhores valores para **m** e **b**. J√° est√° tudo pronto, n√≥s s√≥ precisamos importar a classe [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) e depois utilizar o m√©todo **fit()** que √© respons√°vel por conseguir os melhores valores para os nossos *Coeficientes* **Angular** e **Linear**.

Pronto, agora que j√° temos os melhores valores para **m** e **b**, vamos pegar esses valores com os atributos **coef_** e **intercept_** e imprimir os mesmos:

```python
a_coeff = model.coef_ # Angular Coefficient - m
l_coeff = model.intercept_ # Linear Coefficient - b
print('Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}'.format(a_coeff, l_coeff))
```

E por fim, vamos gerar a parte visual:

 - Primeiro vamos criar um Scatter Plot para exibir nosso conjunto de dados (os dados de amostra);
 - E depois adiciona uma **reta de melhor ajuste**.

```python
plt.scatter(*reg)
plt.plot(reg[0], a_coeff*reg[0] + l_coeff,color='red')
```

**NOTE:**  
Lembrem que para criar essa **reta de melhor ajuste** n√≥s utilizamos a *Equa√ß√£o da Reta*:

![image](images/linear-regression-formule.png)  

```python
m = a_coeff
x = x[0]
b = l_coeff
```

<div id='12-4'></div>

## 12.4 - Dividindo os dados em Treino e Teste com Scikit-Learn

Uma coisa que voc√™s tem que entender primeiro √© que os modelos de **Machine Learning** aprendem a partir de dados. Sabendo disso √© interessante dividir nosso conjunto de dados em **Dados de Treino** & **Dados de Teste**.

**DADOS DE TREINO:**  
Ok, suponha que n√≥s queremos desenvolver um programa (modelo) que identifique se uma imagem **√© um cachorro** ou **n√£o √© um cachorro**.

De in√≠cio n√≥s vamos receber um conjunto (amostra) com v√°rias imagens de cachorros, depois n√≥s vamos pegar uma parte desse conjunto (normalmente 70%) e dar para o nosso modelo aprender identificando caracter√≠sticas comuns entre cachorros.

**DADOS DE TESTE:**  
Ok, N√≥s reservamos 70% do nosso conjunto de dados (amostra) para o nosso algoritmo aprender e os outros 30%?  
Ent√£o, esses s√£o os **Dados de Testes**. N√≥s vamos passar os dados de testes para o nosso modelo e ver qu√£o bem ele est√° aprendendo. Por exemplo:

> Isso aqui √© um cachorro?

![image](images/dog.gif)  

E o nosso modelo vai ter que dar um retorno dizendo se √© um cachorro ou n√£o.

**NOTE:**  
Viram como √© interessante dividir o conjunto de dados (amostra) em **treino** e **teste**? Outro exemplo, seria identificar uma doen√ßa em pacientes, como n√≥s saber√≠amos se nosso modelo aprendeu (ou est√° aprendendo) bem se deixar ele aprender com todo o conjunto de dados?

> Por isso, ele vai aprender com uma parte (70% no nosso caso) e vamos reserva outra parte (30% no nosso caso) para testar e ver qu√£o bem ele (nosso modelo) est√° aprendendo.

[reg-v2.py](src/reg-v2.py)
```python
def createRegression(samples,variavel_numbers, n_noise):
  from sklearn.datasets import make_regression
  x, y = make_regression(n_samples=samples, n_features=variavel_numbers, noise=n_noise)
  return x, y

if __name__ =='__main__':

  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import train_test_split
  from matplotlib import pyplot as plt

  reg = createRegression(200, 1, 30)
  model = LinearRegression()

  # Divide the data into Training and Testing - 30% for testing.
  x_train, x_test, y_train, y_test = train_test_split(reg[0], reg[1], test_size=0.30)

  # Just the training data is transferred to the fit() function (which finds the best values ‚Äã‚Äãfor m and b).
  model.fit(x_train, y_train)

  a_coeff = model.coef_ # Angular Coefficient - m
  l_coeff = model.intercept_ # Linear Coefficient - b
  print('Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}'.format(a_coeff, l_coeff))

  # Create plot.
  plt.figure(figsize=(10, 7))
  plt.subplot(211)
  plt.scatter(reg[0], reg[1])
  plt.title('Complete Sample')
  plt.plot(x_train, a_coeff*x_train + l_coeff,color='red')
  plt.subplot(223)
  plt.scatter(x_train, y_train)
  plt.title('Training Set (70%)')
  plt.plot(x_train, a_coeff*x_train + l_coeff,color='blue')
  plt.subplot(224)
  plt.scatter(x_test, y_test)
  plt.title('Testing set (30%)')
  plt.plot(x_train, a_coeff*x_train + l_coeff,color='green')
  plt.savefig('../images/plot-06.png', format='png')
  plt.show()
```

**OUTPUT:**  
```python
Angular Coefficient (m): [4.65851642]
Linear Coefficient (b): 0.760506738734484
```

![image](images/plot-06.png)

Agora vamos comentar s√≥ as partes cruciais que foram utilizadas para dividir os dados em **treino** e **teste**. Primeiro n√≥s importamos o m√©todo **train_test_split()**.

```python
from sklearn.model_selection import train_test_split
```

Depois n√≥s passamos os seguintes argumentos para esse m√©todo:

 - **1¬™ -** Os dados no eixo-x do conjunto de dados;
 - **2¬™ -** Os seus correspondentes no eixo-y;
 - **3¬™ -** Por fim, quanto n√≥s reservamos dos dados para teste: **test_size=0.30 = 30%**.

```python
x_train, x_test, y_train, y_test = train_test_split(reg[0], reg[1], test_size=0.30)
```

**NOTE:**  
Veja que o m√©todo **train_test_split()** retorna os dados j√° separados *(aleatoriamente)* em dados de treino e teste.

Agora por fim, n√≥s vamos treinar o nosso m√≥delo apenas com os dados de treino *(como foi explicado anteriormente)*:

```python
model.fit(x_train, y_train)
```

<div id='12-5'></div>

## 12.5 - Tentando fazer previs√µes (predict) com Scikit-Learn

√ìtimo, at√© ent√£o n√≥s fizemos v√°rios exemplos de **Regress√£o Linear**, mas como tentar fazer previs√µes com os nossos modelos com Scikit-Learn?

Vamos come√ßar com uma nova abordagem... Suponha que n√≥s amamos pizzas e nos √∫ltimos dias n√≥s compramos 15. Sabendo que n√≥s compramos muitas pizzas vamos tentar prever qual o pre√ßo de uma pizza de acordo com o seu di√¢metro.

Ent√£o, os dados das nossas 15 pizzas foram os seguintes:


|Inst√¢ncia | Di√¢metro(cm) | Pre√ßo(R$) |
|----------|--------------|-----------|
|   1      |    	7       |   	8     |
|   2      |      10      |     11    |
|   3	     |      15	    |     16    |
|   4      |      30      |     38.5  |
|   5	     |      45      |     52    |
|   6      |    	13      |   	14    |
|   7      |      60      |     70    |
|   8	     |      100     |     90    |
|   9      |      5       |     6     |
|   10     |      30      |     38.5  |
|   11     |    	90      |   	102   |
|   12     |      18      |     20    |
|   13     |      70	    |     85    |
|   14     |      110     |     100   |
|   15     |      25      |     34    |

√ìtimo, primeiro vamos fazer todo aquele paranau√™:

 - Dividir os dados em treino e teste;
 - Criar uma reta de Regress√£o Linear que melhor explique essa rela√ß√£o;
 - E por fim, criar um gr√°fico com tudo isso.

Vai ficar assim:

[pizza_predict.py](src/pizza_predict.py)  
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt


diameter = [[7], [10], [15], [30], [45], [13], [60], [100], [5], [30], [90], [18], [70], [110], [25]] 
prices   = [[8], [11], [16], [38.5], [52], [14], [70], [90], [6], [38.5], [102], [20], [85], [100], [34]]

model = LinearRegression()

x_train, x_test, y_train, y_test = train_test_split(diameter, prices, test_size=0.30, random_state=10)

model.fit(x_train, y_train)

a_coeff = model.coef_ # Angular Coefficient - m
l_coeff = model.intercept_ # Linear Coefficient - b

plt.figure(figsize=(10, 7))
plt.subplot(211)
plt.scatter(diameter, prices)
plt.title('Complete Sample')
plt.plot(x_train, a_coeff*x_train + l_coeff,color='red')
plt.subplot(223)
plt.scatter(x_train, y_train)
plt.title('Training Set (70%)')
plt.plot(x_train, a_coeff*x_train + l_coeff,color='blue')
plt.subplot(224)
plt.scatter(x_test, y_test)
plt.title('Testing set (30%)')
plt.plot(x_train, a_coeff*x_train + l_coeff,color='green')
plt.savefig('../images/plot-07.png', format='png')
plt.show()
```

**OUTPUT:**  

![image](images/plot-07.png)  

Ok, isso n√≥s j√° aplicamos v√°rias vezes, mas agora eu quero saber o seguinte:

> Se eu desejar comprar uma pizza de 20cm di√¢metro qual vai ser o pre√ßo?

A classe [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) al√©m do m√©todo **fit()** utilizado para treinar nosso modelo, tamb√©m tem o m√©todo **predict()** que utiliza o nosso modelo linear para tentar fazer previs√µes.

Veja como fica isso com o c√≥digo abaixo:

[pizza_predict-v2.py](src/pizza_predict-v2.py)
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt

diameterPassed = float(input("What's the diameter(cm) of the pizza you want? "))

diameters = [[7], [10], [15], [30], [45], [13], [60], [100], [5], [30], [90], [18], [70], [110], [25]]
prices   = [[8], [11], [16], [38.5], [52], [14], [70], [90], [6], [38.5], [102], [20], [85], [100], [34]]

model = LinearRegression()

x_train, x_test, y_train, y_test = train_test_split(diameters, prices, test_size=0.30, random_state=10)

model.fit(x_train, y_train)
price = model.predict([[diameterPassed]])

print("A {0} cm diameter pizza should cost:: R${1}".format(diameterPassed, round(price[0][0])))
```

**NOTE:**  
Na verdade n√≥s fizemos ainda melhor do que tentar prever s√≥ para uma pizza de 20cm de di√¢metro. N√≥s criamos um programa de Machine Learning que para **qualquer di√¢metro passado**, ele vai retorna qual ser√° mais ou menos o **pre√ßo da pizza** para esse conjunto (amostra) de dados.

![image](images/genius.gif)  

---

**REFERENCES:**  
[Didatica Tech - M√ìDULO - I](https://didatica.tech/)  
[A matem√°tica do Gradiente Descendente & Regress√£o Linear (machine learning)](https://www.youtube.com/watch?v=htfh2xrnlaE)  
[Metodo da Descida do Gradiente](https://www.youtube.com/watch?v=s0VhfvCB0Vw)  
[AULA 5 - GRADIENTE DESCENDENTE EXPLICADO - CURSO DE INTELIG√äNCIA ARTIFICIAL PARA TODOS](https://www.youtube.com/watch?v=joaYDx1HTcA)  
[Optimization: Ordinary Least Squares Vs. Gradient Descent ‚Äî from scratch](https://towardsdatascience.com/https-medium-com-chayankathuria-optimization-ordinary-least-squares-gradient-descent-from-scratch-8b48151ba756)   
