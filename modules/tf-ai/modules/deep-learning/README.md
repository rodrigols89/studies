# Deep Learning

## ConteÃºdo

 - **Projetos:**
   - [ğŸŒ¸ Iris flower data set](#iris-data-set)
 - [**REFERÃŠNCIAS**](#ref)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->






































































































<!--- ( Projetos ) --->

---

<div id="iris-data-set"></div>

## ğŸŒ¸ Iris flower data set

O **Iris Dataset** Ã© um dos conjuntos de dados mais famosos da histÃ³ria da ciÃªncia de dados e aprendizado de mÃ¡quina. Ele foi publicado em 1936 pelo estatÃ­stico e biÃ³logo Ronald A. Fisher.

**ğŸ“Š O que ele contÃ©m?**  
O conjunto de dados contÃ©m 150 amostras de flores da planta Iris, divididas igualmente entre 3 espÃ©cies:

 - Iris setosa;
 - Iris versicolor;
 - Iris virginica.

![img](images/iris-flowers-01.png)  

Para cada flor, foram medidas 4 caracterÃ­sticas:

 - **ğŸŒ¸ 1. Comprimento da sÃ©pala (Sepal length):**
   - **O que Ã© sÃ©pala?**
     - Ã‰ a parte verde que protege a flor quando ainda estÃ¡ em botÃ£o (antes de abrir).
     - Medida: O comprimento da sÃ©pala em centÃ­metros (cm).
     - Exemplo: 5.1 cm.
 - **ğŸŒ¸ 2. Largura da sÃ©pala (Sepal width):**
   - **O que mede?**
     - A largura (horizontal) da sÃ©pala, tambÃ©m em centÃ­metros.
     - ImportÃ¢ncia: Algumas espÃ©cies tÃªm sÃ©palas mais largas ou estreitas, o que ajuda a diferenciÃ¡-las.
 - **ğŸŒº 3. Comprimento da pÃ©tala (Petal length):**
   - **O que Ã© pÃ©tala?**
     - Ã‰ a parte colorida da flor, que atrai polinizadores.
     - Medida: O comprimento da pÃ©tala em centÃ­metros.
     - RelevÃ¢ncia: Essa Ã© uma das features mais Ãºteis para separar as espÃ©cies.
 - **ğŸŒº 4. Largura da pÃ©tala (Petal width):**
   - Medida: A largura da pÃ©tala em centÃ­metros.
   - ImportÃ¢ncia: TambÃ©m Ã© muito Ãºtil â€” por exemplo, a Iris setosa tende a ter pÃ©talas bem estreitas.

![img](images/iris-flowers-02.png)  

**ğŸ§  Para que ele Ã© usado?**  
O Iris Dataset Ã© amplamente usado para aprender conceitos de **"classificaÃ§Ã£o"**:

 - Redes Neurais;
 - SVM;
 - K-NN.

**âœ… Por que ele Ã© tÃ£o popular?**

 - Pequeno e fÃ¡cil de entender;
 - Bem balanceado (50 amostras por classe);
 - Perfeito para iniciantes praticarem classificaÃ§Ã£o multiclasse.

Ã“timo, agora que jÃ¡ entendemos o dataset, vamos planejar como serÃ¡ nossa Rede Neural utilizando o mesmo. De inÃ­cio vamos revisar quais sÃ£o as entradas:

 - **ğŸŒ¸ 1. Comprimento da sÃ©pala (Sepal length):**
 - **ğŸŒ¸ 2. Largura da sÃ©pala (Sepal width):**
 - **ğŸŒº 3. Comprimento da pÃ©tala (Petal length):**
 - **ğŸŒº 4. Largura da pÃ©tala (Petal width):**

> **E a saÃ­da?**  
> A saÃ­da (output) do Iris dataset Ã© a espÃ©cie da flor, ou seja, a classe Ã  qual cada flor pertence.

| CÃ³digo | Classe (EspÃ©cie)  | DescriÃ§Ã£o                                   |
| ------ | ----------------- | ------------------------------------------- |
| 0      | *Iris setosa*     | SÃ©pala larga e pÃ©tala curta                 |
| 1      | *Iris versicolor* | Medidas intermediÃ¡rias entre as outras duas |
| 2      | *Iris virginica*  | PÃ©talas mais longas e largas                |

Sabendo de tudo isso vamos imaginar que a nossa Rede Neural vai ter a seguinte estrutura:

![img](images/iris-ann.png)  

> **NOTE:**  
> Vejam que a saÃ­da vai ser uma das 3 classes (Iris setosa, Iris versicolor e Iris virginica).

Para comeÃ§ar a implementaÃ§Ã£o da nossa Rede Neural vamos importar as bibliotecas necessÃ¡rias:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
import os
import tensorflow as tf
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
```

</details>

<br/>

Agora vamos carregar o conjunto de dados:

 - X recebe as 4 caracterÃ­sticas das flores (sÃ©palas e pÃ©talas).
 - y recebe os rÃ³tulos das classes (0=setosa, 1=versicolor, 2=virginica).

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Carrega o dataset Iris
iris = load_iris()
X = iris.data  # 4 features
y = iris.target  # 3 classes
```

</details>

<br/>

Agora vamos aplicar uma **normalizaÃ§Ã£o (ou padronizaÃ§Ã£o)** nos dados:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# PrÃ©-processamento
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

</details>

<br/>

Continuando, um processo comum em *Machine Learning (Deep Learning)* Ã© dividir os **dados em dados de treinamento** e **"dados de teste"**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# DivisÃ£o treino/teste
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
```

 - Divide os dados em `80%` para treino e `20%` para teste.
 - `random_state=42` garante que a divisÃ£o seja reprodutÃ­vel.

</details>

<br/>

Agora vamos aplicar **"One-hot encoding"** nas classes.

> **What? O que Ã© isso?**

**ğŸŸ¡ O problema:**  
Por padrÃ£o, as classes do Iris dataset sÃ£o representadas como nÃºmeros inteiros:

| Flor | Classe |
| ---- | ------ |
| A    | 0      |
| B    | 1      |
| C    | 2      |

Por exemplo, se vocÃª estiver utilizando o Python a saÃ­da deve ser algo parecido com isso:

```python
print(y)
print("DimensÃ£o de y:", y.shape)
```

**OUTPUT:**
```bash
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
DimensÃ£o de y: (150,)
```

> **NOTE:**  
> Vejam que nÃ³s temos como saÃ­da um array 1-dimensinal com 150 elementos.

**NOTE:**  
SÃ³ que esses nÃºmeros **nÃ£o sÃ£o interpretados corretamente** por redes neurais quando usamos funÃ§Ãµes de saÃ­da como **Softmax**, que retornam **"probabilidades"** para cada classe.

**ğŸ” A soluÃ§Ã£o: One-Hot Encoding:**  
Em vez de representar a classe como um Ãºnico nÃºmero inteiro, vamos representar cada classe como um vetor binÃ¡rio onde apenas uma posiÃ§Ã£o Ã© 1 (indicando a classe), e as demais sÃ£o 0:

| Classe original | One-hot encoded |
| --------------- | --------------- |
| 0 (Setosa)      | \[1, 0, 0]      |
| 1 (Versicolor)  | \[0, 1, 0]      |
| 2 (Virginica)   | \[0, 0, 1]      |

**âœ… Aplicando Softmax:**
A funÃ§Ã£o *Softmax* vai transformar esses valores em algo assim:

```python
[0.65, 0.24, 0.11]
```

Ou seja:

 - **Classe 0 (Setosa):** 65% de chance;
 - **Classe 1 (Versicolor):** 24% de chance;
 - **Classe 2 (Virginica):** 11% de chance.

**ğŸ“Œ O que a rede estÃ¡ dizendo?**  
A *Rede Neural* acredita que:

 - A flor provavelmente Ã© Setosa (classe 0), com 65% de confianÃ§a.
 - HÃ¡ uma chance menor de ser Versicolor (24%) ou Virginica (11%).

**ğŸ“Š Por que Ã© Ãºtil?**  

 - Isso permite que o modelo **"escolha a classe"** com a **maior probabilidade** como resposta.
 - TambÃ©m permite medir o nÃ­vel de incerteza nas previsÃµes (ex: se todas as classes tiverem probabilidade prÃ³xima, a rede estÃ¡ em dÃºvida).

Ã“timo, agora que nÃ³s jÃ¡ entendemos o que Ã© **"One-hot encoding"**, vamos aplicar isso na prÃ¡tica nos nossos `y_train` e `y_test`:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# One-hot encoding das classes
y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=3)
y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=3)
```

 - **Classe 0** â†’ [1, 0, 0]
 - **Classe 1** â†’ [0, 1, 0]
 - **Classe 2** â†’ [0, 0, 1]

</details>

<br/>

Continuando, agora nÃ³s precisamos definir o nÃºmero de **neuronios de entrada** da nossa Rede Neural:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

Como nÃ³s sabemos a nossa entrada vai ser **"X"**:

```python
print(X.shape)
```

**OUTPUT:**
```bash
(150, 4)
```

> **NOTE:**  
> Vejam que nÃ³s temos 150 amostras, com 4 caracterÃ­sticas (features) cada. Ou seja, nossa rede neural terÃ¡ 4 neuronios de entrada.

[iris-v1.py](src/iris-v1.py)
```python
# Input com 4 features
n_inputs = X.shape[1]  # 4
inputs = tf.keras.Input(shape=(n_inputs,), name="input_layer")
```

</details>

<br/>

Agora nÃ³s vamos definir as **Camadas Ocultas (Hidden layers)** da nossa Rede Neural:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Camadas Ocultas (Hidden Layers)
hidden1 = tf.keras.layers.Dense(5, activation="relu", name="hidden_layer_1")(inputs)
hidden2 = tf.keras.layers.Dense(3, activation="relu", name="hidden_layer_2")(hidden1)
```

</details>

<br/>

Agora nÃ³s vamos conectar nossas **Camadas Ocultas (Hidden layers)** com nossa **Camada de SaÃ­da (Output layer)**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Camada de SaÃ­da (Output Layer)
output = tf.keras.layers.Dense(3, activation="softmax", name="output_layer")(hidden2)
```

</details>

<br/>

Para finalizar essa parte de camadas, vamos criar o nosso **Modelo (conectar as camadas)** e **visualizar sua estrutura**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Cria o modelo (conecta as camadas)
model = tf.keras.Model(inputs=inputs, outputs=output)
model.summary()  # Visualiza a estrutura da Rede Neural
```

**OUTPUT:**
```bash
Model: "functional"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                         â”ƒ Output Shape                â”ƒ         Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ input_layer (InputLayer)             â”‚ (None, 4)                   â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hidden_layer_1 (Dense)               â”‚ (None, 5)                   â”‚              25 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hidden_layer_2 (Dense)               â”‚ (None, 3)                   â”‚              18 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ output_layer (Dense)                 â”‚ (None, 3)                   â”‚              12 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 55 (220.00 B)
 Trainable params: 55 (220.00 B)
 Non-trainable params: 0 (0.00 B)
```

</details>

<br/>

Agora nÃ³s vamos **"preparar o modelo para treino"**. Essa parte conhecida como **"compilaÃ§Ã£o"** Ã© o momento em que vocÃª diz ao modelo:

> Aqui estÃ¡ **"como vocÃª vai aprender"**, **"como calcular o erro"**, e **"como vamos medir seu desempenho"**.


<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# CompilaÃ§Ã£o
model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)
```

</details>

<br/>

Agora que nÃ³s jÃ¡ orientamos o nosso modelo quais mÃ©tricas utilizar vamos treinar o nosso modelo:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Treinamento
model.fit(
    X_train,
    y_train_ohe,
    epochs=50,
    batch_size=8,
    verbose=0
)
```

 - `X_train`
   - Os dados de entrada (tambÃ©m chamados de features ou atributos) usados para treinar o modelo.
   - Formato: Uma matriz com vÃ¡rias linhas (amostras) e colunas (4 colunas no caso do Iris: sÃ©pala e pÃ©tala).
 - `y_train_ohe`
   - Os rÃ³tulos das classes, ou seja, a **"resposta certa"** para cada exemplo.
   - Por que estÃ¡ em formato ohe (One-Hot Encoding)?
     - Porque a saÃ­da do modelo Ã© uma distribuiÃ§Ã£o de probabilidade (com Softmax), entÃ£o o rÃ³tulo tambÃ©m precisa estar nesse formato compatÃ­vel.
 - `epochs=50`
   - Define **quantas vezes o modelo verÃ¡ todos os dados de treino**.
   - Exemplo: Se vocÃª tem 120 amostras e epochs=50, o modelo verÃ¡ todas essas 120 amostras 50 vezes, tentando melhorar a cada repetiÃ§Ã£o.
   - Quanto maior o nÃºmero, maior a chance do modelo aprender bem (mas cuidado com o overfitting).
 - `batch_size=8`
   - O nÃºmero de amostras que serÃ£o usadas de cada vez antes de o modelo atualizar os pesos.
   - Exemplo: Se `batch_size=8`, o modelo pega 8 amostras, calcula o erro, ajusta os pesos, e sÃ³ entÃ£o passa para as prÃ³ximas 8.
   - Por que isso Ã© Ãºtil?
     - Treinar com batches pequenos ajuda a reduzir o uso de memÃ³ria e pode melhorar a generalizaÃ§Ã£o.
 - `verbose=0`
   - Controla o nÃ­vel de mensagens mostradas durante o treinamento.
   - Valores possÃ­veis:
     - 0: nenhum output (silencioso);
     - 1: barra de progresso por Ã©poca;
     - 2: uma linha por Ã©poca (resumo).
     - Por que usar 0?
       - Ãštil para evitar poluiÃ§Ã£o visual quando nÃ£o queremos ver o log de treino.

</details>

<br/>

Agora nÃ³s vamos **avalia (validar)** o modelo nos dados de teste:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# AvaliaÃ§Ã£o
loss, accuracy = model.evaluate(X_test, y_test_ohe, verbose=0)
print(f"\nAcurÃ¡cia do modelo: {accuracy:.2f}")
```

**OUTPUT:**
```bash
AcurÃ¡cia do modelo: 0.70
```

**O que esses 0.70 significa?**  
Uma **acurÃ¡cia de 0.70 (ou 70%)** significa que o nosso modelo **acertou 70% das previsÃµes** no conjunto de teste.

</details>

<br/>

Ã“timo, que temos um modelo treinado (nÃ£o Ã© perfeito 70%, mas Ã© o que temos por agora) vamos tentar fazer algumas previsÃµes:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# PrevisÃ£o com um exemplo real
sample = X_test[0].reshape(1, -1)
prediction = model(sample)
print("\nPrediÃ§Ã£o (probabilidades):", prediction.numpy())
print("Classe prevista:", tf.argmax(prediction, axis=1).numpy())
print("Classe Real:", y_test[0])
```

**OUTPUT:**
```bash
PrediÃ§Ã£o (probabilidades): [[0.12156641 0.6814825  0.19695109]]
Classe prevista: [1]
Classe Real: 1
```

</details>

<br/>







































































































---

<div id="settings"></div>

## ğŸš€ InstalaÃ§Ã£o / ExecuÃ§Ã£o local

*Crie e ative o ambiente virtual (recomendado):**  

```bash
python -m venv environment
```

**LINUX:**  
```bash
source environment/bin/activate
```

**WINDOWS:**  
```bash
source environment/Scripts/activate
```

**ATUALIZE O PIP:**
```bash
python -m pip install --upgrade pip
```

**Instale as dependÃªncias:**  

```bash
pip install -U -v --require-virtualenv -r requirements.txt
```










<!--- ( REFERÃŠNCIAS ) --->

---

<div id="ref"></div>

## REFERÃŠNCIAS

 - [ChatGPT](https://chat.openai.com/)

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**