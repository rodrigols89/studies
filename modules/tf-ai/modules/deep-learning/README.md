# Deep Learning

## Conte√∫do

 - [**Fun√ß√µes de Perda (Loss Functions):**](#loss-functions)
   - **Problemas de Regress√£o:**
     - Mean Squared Error (MSE | Problemas com sa√≠da cont√≠nua, ex: pre√ßo, temperatura)
     - Mean Absolute Error (MAE | Quando voc√™ quer menos sensibilidade a outliers)
     - Huber Loss (Regress√£o robusta, Combina√ß√£o entre MSE e MAE, mais robusta a outliers)
   - **Problemas de Classifica√ß√£o:**
     - Binary Cross Entropy (Quando h√° apenas 2 classes, ex: spam/n√£o-spam)
     - Categorical Cross-Entropy (Sa√≠das em one-hot (ex: [0, 0, 1])
     - Sparse Categorical Crossentropy (Sa√≠das como r√≥tulo inteiro, ex: 2 ao inv√©s de [0, 0, 1])
   - **Compara√ß√£o entre distribui√ß√µes de probabilidade:**
     - Kullback-Leibler Divergence (Quando se quer medir a diferen√ßa entre duas distribui√ß√µes)
 - [**Optimizers (ou Otimizadores):**](#optimizers)
   - Stochastic Gradient Descent (SGD)
   - SGD com Momentum
   - Learning Rate
   - Learning Rate Decay
   - Adam
   - RMSprop
   - Adagrad
   - Adadelta
   - Nadam
   - FTRL
 - **Gr√°ficos (Plots):**
   - **ü§ñ Gr√°ficos relacionados ao treinamento do modelo:**
     - [Gr√°fico de Perda (Loss) por √âpoca em Redes Neurais](#loss-by-epoch-plot)
     - [Gr√°fico de Acur√°cia por √âpoca em Redes Neurais](#accuracy-by-epoch-plot)
     - [Interpretando os Gr√°ficos de Perda e Acur√°cia](#loss-vs-accuracy)
   - **‚úÖ Gr√°ficos relacionados ao desempenho do modelo:**
     - [Matriz de Confus√£o (Confusion Matrix) ‚Äì Entendendo os Acertos e Erros do Modelo](#confusion-matrix)
 - **Projetos:**
   - [üå∏ Iris flower data set](#iris-data-set)
 - [**REFER√äNCIAS**](#ref)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->






































































































<!--- ( Fun√ß√µes de Perda (Loss Functions) ) --->

---

<div id="loss-functions"></div>

## Fun√ß√µes de Perda (Loss Functions)

> Fun√ß√µes de perda (Loss Functions) s√£o f√≥rmulas matem√°ticas que medem o erro entre a **"sa√≠da prevista"** pela rede neural e a **"sa√≠da real"**.

Em outras palavras, elas dizem:

 - Qu√£o ruim est√° a previs√£o da rede;
 - **NOTE:** Quanto menor o valor da perda, melhor o modelo est√° aprendendo.

### üéØ Para que servem?

As *fun√ß√µes de perda (loss functions)** servem para orientar o processo de aprendizagem da rede neural. Durante o treinamento, a rede ajusta seus pesos internos para minimizar essa perda.

> **NOTE:**  
> ‚û°Ô∏è Os otimizadores (como Adam, SGD, etc.) usam essa perda para saber como e quanto alterar os pesos (e bias) da rede.

### ‚è±Ô∏è Quando s√£o usadas?

As fun√ß√µes de perda **s√£o usadas em todo treino da rede neural**:

 - A cada batch de dados;
 - Em cada √©poca;
 - **NOTE:** S√£o indispens√°veis ‚Äî sem ela, o modelo n√£o aprende.

### üì¶ Tipos de Fun√ß√µes de Perda

> A fun√ß√£o de perda muda dependendo do tipo de problema (Regress√£o ou Classifica√ß√£o).

**üîµ 1. Classifica√ß√£o:**  
Para tarefas em que o modelo precisa prever classes (ex: detectar se uma flor √© iris-setosa, iris-versicolor ou iris-virginica).

| Fun√ß√£o de Perda                   | Quando usar                                                                |
| --------------------------------- | -------------------------------------------------------------------------- |
| `categorical_crossentropy`        | Classifica√ß√£o multiclasse com one-hot encoding                             |
| `sparse_categorical_crossentropy` | Classifica√ß√£o multiclasse com labels inteiros (sem one-hot)                |
| `binary_crossentropy`             | Classifica√ß√£o bin√°ria (ex: 0 ou 1)                                         |
| `Kullback-Leibler divergence`     | Quando se quer medir a diferen√ßa entre duas distribui√ß√µes de probabilidade |

**üü† 2. Regress√£o:**  
Para tarefas em que o modelo precisa prever n√∫meros cont√≠nuos (ex: pre√ßo de uma casa, temperatura, etc.)

| Fun√ß√£o de Perda       | Quando usar                                |
| --------------------- | ------------------------------------------ |
| `mean_squared_error`  | Regress√£o ‚Äì penaliza mais os grandes erros |
| `mean_absolute_error` | Regress√£o ‚Äì menos sens√≠vel a outliers      |
| `huber_loss`          | Regress√£o robusta ‚Äì mistura dos dois acima |


### ‚öôÔ∏è Como implementar?

<details>

<summary>TensorFlow (Python)</summary>

<br/>

Na pr√°tica, voc√™ escolhe a **fun√ß√£o de perda (loss function)** ao compilar o modelo com TensorFlow:

<br/>

```python
model.compile(
    loss='categorical_crossentropy',  # <- Fun√ß√£o de perda
)
```

</details>







































































































<!--- ( Optimizers (ou Otimizadores) ) --->

---

<div id="optimizers"></div>

## Optimizers (ou Otimizadores)

> **"Otimizadores"** s√£o algoritmos usados para **ajustar os pesos de uma rede neural durante o treinamento**, **minimizando a fun√ß√£o de perda (loss function)**.

### üéØ Para que servem?

 - Encontrar os melhores valores de pesos e bias;
 - Reduzir o erro (loss) entre as previs√µes e os valores reais;
 - Ajudar o modelo a convergir para uma solu√ß√£o precisa;
 - Tornar o treinamento mais r√°pido e est√°vel.

### ‚öôÔ∏è Como implementar?

<details>

<summary>TensorFlow (Python)</summary>

<br/>

Na pr√°tica, voc√™ escolhe o **otimizador** ao compilar o modelo com TensorFlow:

<br/>

```python
model.compile(
    optimizer="adam",  # <- Otimizador
)
```

</details>

<br/>

| üîß Nome                                  | üìù Breve descri√ß√£o                                                                                                                                                    |
| ---------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Stochastic Gradient Descent (SGD)**    | M√©todo cl√°ssico. Atualiza pesos a cada amostra. Simples e eficiente, mas pode oscilar.                                                                                |
| **SGD com Momentum**                     | Variante do SGD. Adiciona uma "mem√≥ria" do gradiente anterior (momentum), o que ajuda a evitar oscila√ß√µes e acelera em dire√ß√£o √† solu√ß√£o.                             |
| **Learning Rate**                        | Hiperpar√¢metro que controla o "tamanho do passo" que o otimizador d√° ao atualizar os pesos. Um valor muito alto pode pular o m√≠nimo; muito baixo pode demorar demais. |
| **Learning Rate Decay**                  | Estrat√©gia para diminuir gradualmente o learning rate conforme o treinamento avan√ßa. Permite aprendizado r√°pido no in√≠cio e refinamento no final.                     |
| **Adam**                                 | Muito usado. Combina Momentum + RMSprop. Adapta os passos automaticamente e funciona bem em muitos problemas.                                                         |
| **RMSprop**                              | Mant√©m hist√≥rico de gradientes com m√©dia exponencial. Ideal para problemas com dados n√£o estacion√°rios (como s√©ries temporais).                                       |
| **Adagrad**                              | Adapta o tamanho do passo para cada par√¢metro com base no hist√≥rico de gradientes. Funciona bem com dados esparsos.                                                   |
| **Adadelta**                             | Variante do Adagrad que limita a acumula√ß√£o dos gradientes passados. Melhora a estabilidade.                                                                          |
| **Nadam**                                | Combina Adam com Nesterov momentum. Pode oferecer converg√™ncia mais r√°pida e suave.                                                                                   |
| **FTRL (Follow The Regularized Leader)** | Otimizador espec√≠fico para problemas com muitos dados esparsos (ex: grandes sistemas de recomenda√ß√£o). Muito usado no Google.                                         |







































































































<!--- ( Gr√°ficos ) --->

---

<div id="loss-by-epoch-plot"></div>

## Gr√°fico de Perda (Loss) por √âpoca em Redes Neurais

**üîç O que √© "Perda (Loss) por √âpoca"?**  
Durante o treinamento de uma rede neural, o modelo tenta prever os resultados certos com base nos dados de entrada. A **fun√ß√£o de perda (loss function)** √© usada para medir o quanto o modelo erra em cada etapa.

 - **Perda (Loss):** Um n√∫mero que representa o erro do modelo ‚Äî quanto menor, melhor.
 - **√âpoca (Epoch):** Uma passada completa por todos os dados de treino. Se voc√™ treina por 50 √©pocas, o modelo ver√° o mesmo conjunto de dados 50 vezes.
 - **Gr√°fico de perda por √©poca:** Mostra como o *erro (loss)* muda ao longo do treinamento. Ele ajuda a entender se o modelo est√° aprendendo, estagnado ou at√© piorando.

> **Por que esse gr√°fico √© importante?**

 - **Ele responde perguntas como:**
   - O modelo est√° aprendendo com o tempo?
   - A perda est√° diminuindo ou aumentando?
   - O modelo est√° sofrendo overfitting?

√ìtimo, agora que n√≥s j√° entendemos o que √© **"Perda (Loss) por √âpoca"**, vamos vamos criar uma fun√ß√£o que faz isso para n√≥s para qualquer tipo de rede neural e salva o gr√°fico em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
import matplotlib.pyplot as plt
import os

def plot_loss(history, filename="loss_plot.png"):
    """
    Gera e salva um gr√°fico de perda (loss) por √©poca com base no objeto `history` do Keras.
    
    Par√¢metros:
    - history: objeto retornado por model.fit()
    - filename: nome do arquivo de imagem que ser√° salvo
    """
    loss = history.history.get("loss")
    val_loss = history.history.get("val_loss")

    epochs = range(1, len(loss) + 1)

    plt.figure(figsize=(8, 5))
    plt.plot(epochs, loss, "b-", label="Loss de Treinamento")
    
    if val_loss:
        plt.plot(epochs, val_loss, "r--", label="Loss de Valida√ß√£o")

    plt.title("Perda (Loss) por √âpoca")
    plt.xlabel("√âpoca")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora √© s√≥ treinar o modelo com o m√©todo `fit()` e pegar o objeto retornado pelo mesmo e passar para a fun√ß√£o `plot_loss()`:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    epochs=50,
    batch_size=8,
    verbose=0
)

plot_loss(history, filename="loss_plot-01.png")
```

**GR√ÅFICO:**

![img](images/loss_plot-01.png)  

</details>




















---

<div id="accuracy-by-epoch-plot"></div>

## Gr√°fico de Acur√°cia por √âpoca em Redes Neurais

**üéØ O que √© "Acur√°cia por √âpoca"?**  
Durante o treinamento de uma rede neural, al√©m de medir quanto o modelo est√° errando (com a perda), tamb√©m queremos saber o quanto ele est√° acertando. Para isso usamos a acur√°cia.

 - **Acur√°cia (accuracy):** A porcentagem de acertos que o modelo teve em rela√ß√£o ao total de exemplos avaliados.
 - **√âpoca (Epoch):** Uma rodada completa de treinamento, onde o modelo v√™ todos os dados uma vez.
 - **Gr√°fico de acur√°cia por √©poca:** Mostra como a porcentagem de acertos do modelo muda a cada √©poca.

> **Para que serve esse gr√°fico?**

 - **Esse gr√°fico ajuda a responder:**
   - O modelo est√° melhorando ao longo do tempo?
   - O modelo est√° memorizando os dados de treino (overfitting)?
   - O desempenho est√° estagnado?

√â comum usar dois tipos de acur√°cia no gr√°fico:

 - **Acur√°cia de treino:** desempenho nos dados usados para treinar.
 - **Acur√°cia de valida√ß√£o:** desempenho em dados que o modelo nunca viu durante o treino.

√ìtimo, agora que n√≥s j√° entendemos o que √© **"Acur√°cia por √âpoca"**, vamos vamos criar uma fun√ß√£o que faz isso para n√≥s para qualquer tipo de rede neural e salva o gr√°fico em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
def plot_accuracy(history, filename="accuracy_plot.png"):
    """
    Gera e salva um gr√°fico de acur√°cia por √©poca com base no objeto `history` do Keras.

    Par√¢metros:
    - history: objeto retornado por model.fit()
    - filename: nome do arquivo de imagem que ser√° salvo
    """
    acc = history.history.get("accuracy")
    val_acc = history.history.get("val_accuracy")

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(8, 5))
    plt.plot(epochs, acc, "b-", label="Acur√°cia de Treinamento")
    
    if val_acc:
        plt.plot(epochs, val_acc, "g--", label="Acur√°cia de Valida√ß√£o")

    plt.title("Acur√°cia por √âpoca")
    plt.xlabel("√âpoca")
    plt.ylabel("Acur√°cia")
    plt.legend()
    plt.grid(True)

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora √© s√≥ treinar o modelo com o m√©todo `fit()` e pegar o objeto retornado pelo mesmo e passar para a fun√ß√£o `plot_accuracy()`:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    epochs=50,
    batch_size=8,
    verbose=0
)

plot_accuracy(history, filename="accuracy_plot-01.png")
```

**GR√ÅFICO:**

![img](images/accuracy_plot-01.png)  

</details>

<br/>




















---

<div id="loss-vs-accuracy"></div>

## Interpretando os Gr√°ficos de Perda e Acur√°cia

Quando treinamos uma rede neural, √© essencial acompanhar como o modelo est√° se comportando a cada √©poca. Os dois principais gr√°ficos para isso s√£o:

 - Gr√°fico de Perda por √âpoca (Loss)
 - Gr√°fico de Acur√°cia por √âpoca (Accuracy)

Mas, mais importante que olhar separadamente, √© analisar os dois juntos.

### üîÅ Por que observar os dois gr√°ficos juntos?

A combina√ß√£o dos dois gr√°ficos ajuda voc√™ a entender se:

 - O modelo est√° aprendendo bem.
 - O modelo est√° sofrendo com overfitting (ajuste exagerado aos dados de treino).
 - O modelo est√° subajustado (underfitting), ou seja, n√£o aprendeu o suficiente.

### ‚úÖ Situa√ß√£o 1: O Modelo Est√° Aprendendo Bem

Comportamento esperado:

| Gr√°fico         | O que acontece?     |
| --------------- | ------------------- |
| üìâ **Loss**     | Diminui com o tempo |
| üìà **Accuracy** | Aumenta com o tempo |

> **‚û°Ô∏è Interpreta√ß√£o:** 
> O modelo est√° aprendendo a generalizar e se adaptar aos dados.

### ‚ö†Ô∏è Situa√ß√£o 2: Overfitting (Sobreajuste)

Comportamento t√≠pico:

| Gr√°fico         | Treinamento      | Valida√ß√£o         |
| --------------- | ---------------- | ----------------- |
| üìâ **Loss**     | Diminui          | Come√ßa a subir üìà |
| üìà **Accuracy** | Sobe bastante üìà | Come√ßa a cair üìâ  |

> **‚û°Ô∏è Interpreta√ß√£o:**  
> O modelo est√° aprendendo bem os dados de treino, mas est√° memorizando demais e n√£o consegue generalizar para novos dados (test/valida√ß√£o). Isso √© *"overfitting"*.

 - **üõ†Ô∏è O que fazer nesse caso:**
   - Reduza o n√∫mero de √©pocas.
   - Use t√©cnicas como regulariza√ß√£o (Dropout, L2, etc.).
   - Adicione mais dados ou use Data Augmentation.

### üßä Situa√ß√£o 3: Underfitting (Subajuste)

Comportamento t√≠pico:

| Gr√°fico         | O que acontece?             |
| --------------- | --------------------------- |
| üìâ **Loss**     | Fica alto ou n√£o muda muito |
| üìà **Accuracy** | Baixa ou n√£o sobe muito     |

> **‚û°Ô∏è Interpreta√ß√£o:**  
> O modelo n√£o est√° aprendendo o suficiente nem no treino nem na valida√ß√£o.

 - **üõ†Ô∏è O que fazer nesse caso:**
   - Aumente a complexidade do modelo (mais camadas ou neur√¥nios).
   - Treine por mais √©pocas.
   - Melhore o pr√©-processamento dos dados.

√ìtimo, agora que n√≥s j√° entendemos qual a vantagem de comparar os gr√°ficos de **Perda (Loss)** e **Acur√°cia (Accuracy)**, vamos criar um gr√°fico que gera essa compara√ß√£o para qualquer Rede Neural e salve em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
def loss_vs_accuracy_plot(history, filename="training_metrics.png"):
    """
    Gera e salva um gr√°fico com a perda (loss) e acur√°cia (accuracy) por √©poca, usando o hist√≥rico de treinamento.
    
    Par√¢metros:
    - history: objeto retornado por model.fit()
    - filename: nome do arquivo de imagem a ser salvo
    """
    loss = history.history.get("loss")
    val_loss = history.history.get("val_loss")
    acc = history.history.get("accuracy")
    val_acc = history.history.get("val_accuracy")

    epochs = range(1, len(loss) + 1)

    plt.figure(figsize=(12, 5))

    # üîª Subplot 1: Perda
    plt.subplot(1, 2, 1)
    plt.plot(epochs, loss, "b-", label="Perda de Treinamento")
    if val_loss:
        plt.plot(epochs, val_loss, "r--", label="Perda de Valida√ß√£o")
    plt.title("Perda por √âpoca")
    plt.xlabel("√âpoca")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # üî∫ Subplot 2: Acur√°cia
    plt.subplot(1, 2, 2)
    plt.plot(epochs, acc, "g-", label="Acur√°cia de Treinamento")
    if val_acc:
        plt.plot(epochs, val_acc, "m--", label="Acur√°cia de Valida√ß√£o")
    plt.title("Acur√°cia por √âpoca")
    plt.xlabel("√âpoca")
    plt.ylabel("Acur√°cia")
    plt.legend()
    plt.grid(True)

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora √© s√≥ treinar o modelo com o m√©todo `fit()` e pegar o objeto retornado pelo mesmo e passar para a fun√ß√£o `loss_vs_accuracy_plot()`:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    validation_data=(X_test, y_test_ohe),  # obrigat√≥rio para ter val_loss e val_accuracy
    epochs=50,
    batch_size=8,
    verbose=0
)

loss_vs_accuracy_plot(history, filename="loss-vs-accuracy-01.png")
```

**GR√ÅFICO:**

![img](images/loss-vs-accuracy-01.png)  

</details>




















---

<div id="confusion-matrix"></div>

## Matriz de Confus√£o (Confusion Matrix) ‚Äì Entendendo os Acertos e Erros do Modelo

**‚úÖ O que √© a Matriz de Confus√£o (Confusion Matrix)?**  
A *matriz de confus√£o (Confusion Matrix)* √© uma tabela que mostra como o seu `modelo de classifica√ß√£o` est√° se saindo, comparando as previs√µes com os valores reais.

Ela indica onde o modelo acertou e onde errou, classe por classe.

### üß† Estrutura da Matriz

Para um problema com 3 classes (por exemplo, Iris Dataset), por exemplo:

|                    | **Classe Prevista: 0**  | **Classe Prevista: 1**  | **Classe Prevista: 2**   |
| ------------------ | ----------------------- | ----------------------- | ------------------------ |
| **Classe Real: 0** | ‚úÖ Acertos da classe 0  | ‚ùå Erros para classe 1  | ‚ùå Erros para classe 2  |
| **Classe Real: 1** | ‚ùå Erros para classe 0  | ‚úÖ Acertos da classe 1  | ‚ùå Erros para classe 2  |
| **Classe Real: 2** | ‚ùå Erros para classe 0  | ‚ùå Erros para classe 1  | ‚úÖ Acertos da classe 2  |

> **NOTE:**  
> Olhando para a tabela acima √© interessante ver quando (quantas vezes) ele errou por linha (classe real).  
> Quando ele erra n√≥s dizemos que ele **"se confundiu"** com essa classe prevista (coluna).

 - Os acertos est√£o na diagonal principal (em verde).
 - Os erros est√£o fora da diagonal (em vermelho).

### üéØ Para que serve?

 - Avaliar quais classes o modelo *confunde*.
 - Diagnosticar erros espec√≠ficos (ex: a classe 1 sempre √© confundida com a 2).
 - Melhorar o modelo, observando padr√µes de erro.

√ìtimo, agora que n√≥s j√° entendemos qual a vantagem de utilizar uma **Matriz de Confus√£o (Confusion Matrix)**, vamos criar um gr√°fico que gera essa compara√ß√£o para qualquer Rede Neural e salve em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
def plot_confusion_matrix(y_true, y_pred, class_names=None, filename="confusion_matrix.png"):
    """
    Gera e salva um gr√°fico de matriz de confus√£o.
    
    Par√¢metros:
    - y_true: r√≥tulos verdadeiros
    - y_pred: r√≥tulos previstos pelo modelo
    - class_names: nomes das classes (opcional)
    - filename: nome do arquivo de sa√≠da
    """
    # Calcula a matriz de confus√£o
    cm = confusion_matrix(y_true, y_pred)

    # Cria os r√≥tulos se n√£o forem fornecidos
    if class_names is None:
        class_names = [str(i) for i in range(cm.shape[0])]

    # Cria o gr√°fico
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names,
                yticklabels=class_names)

    plt.xlabel("Classe Predita")
    plt.ylabel("Classe Real")
    plt.title("Matriz de Confus√£o")
    plt.tight_layout()

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora n√≥s vamos:

 - Treinar o modelo;
 - Fazer previs√µes com `X_test`:
   - `y_pred = [[0.1, 0.7, 0.2], [0.8, 0.1, 0.1], [0.2, 0.3, 0.5]]` 
 - Retornar o √≠ndice de maior probabilidade em cada linha:
   - Ou seja, transforma as probabilidades em r√≥tulos previstos (classes).
   - `y_pred_classes = tf.argmax(y_pred, axis=1).numpy()`
   - Retorno: `<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 0, 2])>`

`y_pred_classes = tf.argmax(y_pred, axis=1).numpy()`  
Essa linha transforma as probabilidades de sa√≠da da rede em classes previstas (inteiros).

| Sa√≠da da rede (`y_pred`) | Classe prevista (`y_pred_classes`) |
| ------------------------ | ---------------------------------- |
| \[0.1, 0.7, 0.2]         | 1 (maior probabilidade √© 0.7)      |
| \[0.8, 0.1, 0.1]         | 0 (maior probabilidade √© 0.8)      |
| \[0.2, 0.3, 0.5]         | 2 (maior probabilidade √© 0.5)      |

O c√≥digo antes de chamar a fun√ß√£o para criar a **Matriz de Confus√£o (Confusion Matrix)** √© o seguinte:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    validation_data=(X_test, y_test_ohe),  # obrigat√≥rio para ter val_loss e val_accuracy
    epochs=50,
    batch_size=8,
    verbose=0
)

y_pred = model.predict(X_test)
y_pred_classes = tf.argmax(y_pred, axis=1).numpy()

plot_confusion_matrix(
    y_true=y_test,
    y_pred=y_pred_classes,
    class_names=["Setosa", "Versicolor", "Virginica"],
    filename="confusion-matrix-01.png"
)
```

**GR√ÅFICO:**

![img](images/confusion-matrix-01.png)  

</details>







































































































<!--- ( Projetos ) --->

---

<div id="iris-data-set"></div>

## üå∏ Iris flower data set

O **Iris Dataset** √© um dos conjuntos de dados mais famosos da hist√≥ria da ci√™ncia de dados e aprendizado de m√°quina. Ele foi publicado em 1936 pelo estat√≠stico e bi√≥logo Ronald A. Fisher.

**üìä O que ele cont√©m?**  
O conjunto de dados cont√©m 150 amostras de flores da planta Iris, divididas igualmente entre 3 esp√©cies:

 - Iris setosa;
 - Iris versicolor;
 - Iris virginica.

![img](images/iris-flowers-01.png)  

Para cada flor, foram medidas 4 caracter√≠sticas:

 - **üå∏ 1. Comprimento da s√©pala (Sepal length):**
   - **O que √© s√©pala?**
     - √â a parte verde que protege a flor quando ainda est√° em bot√£o (antes de abrir).
     - Medida: O comprimento da s√©pala em cent√≠metros (cm).
     - Exemplo: 5.1 cm.
 - **üå∏ 2. Largura da s√©pala (Sepal width):**
   - **O que mede?**
     - A largura (horizontal) da s√©pala, tamb√©m em cent√≠metros.
     - Import√¢ncia: Algumas esp√©cies t√™m s√©palas mais largas ou estreitas, o que ajuda a diferenci√°-las.
 - **üå∫ 3. Comprimento da p√©tala (Petal length):**
   - **O que √© p√©tala?**
     - √â a parte colorida da flor, que atrai polinizadores.
     - Medida: O comprimento da p√©tala em cent√≠metros.
     - Relev√¢ncia: Essa √© uma das features mais √∫teis para separar as esp√©cies.
 - **üå∫ 4. Largura da p√©tala (Petal width):**
   - Medida: A largura da p√©tala em cent√≠metros.
   - Import√¢ncia: Tamb√©m √© muito √∫til ‚Äî por exemplo, a Iris setosa tende a ter p√©talas bem estreitas.

![img](images/iris-flowers-02.png)  

**üß† Para que ele √© usado?**  
O Iris Dataset √© amplamente usado para aprender conceitos de **"classifica√ß√£o"**:

 - Redes Neurais;
 - SVM;
 - K-NN.

**‚úÖ Por que ele √© t√£o popular?**

 - Pequeno e f√°cil de entender;
 - Bem balanceado (50 amostras por classe);
 - Perfeito para iniciantes praticarem classifica√ß√£o multiclasse.

√ìtimo, agora que j√° entendemos o dataset, vamos planejar como ser√° nossa Rede Neural utilizando o mesmo. De in√≠cio vamos revisar quais s√£o as entradas:

 - **üå∏ 1. Comprimento da s√©pala (Sepal length):**
 - **üå∏ 2. Largura da s√©pala (Sepal width):**
 - **üå∫ 3. Comprimento da p√©tala (Petal length):**
 - **üå∫ 4. Largura da p√©tala (Petal width):**

> **E a sa√≠da?**  
> A sa√≠da (output) do Iris dataset √© a esp√©cie da flor, ou seja, a classe √† qual cada flor pertence.

| C√≥digo | Classe (Esp√©cie)  | Descri√ß√£o                                   |
| ------ | ----------------- | ------------------------------------------- |
| 0      | *Iris setosa*     | S√©pala larga e p√©tala curta                 |
| 1      | *Iris versicolor* | Medidas intermedi√°rias entre as outras duas |
| 2      | *Iris virginica*  | P√©talas mais longas e largas                |

Sabendo de tudo isso vamos imaginar que a nossa Rede Neural vai ter a seguinte estrutura:

![img](images/iris-ann.png)  

> **NOTE:**  
> Vejam que a sa√≠da vai ser uma das 3 classes (Iris setosa, Iris versicolor e Iris virginica).

Para come√ßar a implementa√ß√£o da nossa Rede Neural vamos importar as bibliotecas necess√°rias:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
import os
import tensorflow as tf
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
```

</details>

<br/>

Agora vamos carregar o conjunto de dados:

 - X recebe as 4 caracter√≠sticas das flores (s√©palas e p√©talas).
 - y recebe os r√≥tulos das classes (0=setosa, 1=versicolor, 2=virginica).

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Carrega o dataset Iris
iris = load_iris()
X = iris.data  # 4 features
y = iris.target  # 3 classes
```

</details>

<br/>

Agora vamos aplicar uma **normaliza√ß√£o (ou padroniza√ß√£o)** nos dados:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Pr√©-processamento
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

</details>

<br/>

Continuando, um processo comum em *Machine Learning (Deep Learning)* √© dividir os **dados em dados de treinamento** e **"dados de teste"**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Divis√£o treino/teste
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
```

 - Divide os dados em `80%` para treino e `20%` para teste.
 - `random_state=42` garante que a divis√£o seja reprodut√≠vel.

</details>

<br/>

Agora vamos aplicar **"One-hot encoding"** nas classes.

> **What? O que √© isso?**

**üü° O problema:**  
Por padr√£o, as classes do Iris dataset s√£o representadas como n√∫meros inteiros:

| Flor | Classe |
| ---- | ------ |
| A    | 0      |
| B    | 1      |
| C    | 2      |

Por exemplo, se voc√™ estiver utilizando o Python a sa√≠da deve ser algo parecido com isso:

```python
print(y)
print("Dimens√£o de y:", y.shape)
```

**OUTPUT:**
```bash
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
Dimens√£o de y: (150,)
```

> **NOTE:**  
> Vejam que n√≥s temos como sa√≠da um array 1-dimensinal com 150 elementos.

**NOTE:**  
S√≥ que esses n√∫meros **n√£o s√£o interpretados corretamente** por redes neurais quando usamos fun√ß√µes de sa√≠da como **Softmax**, que retornam **"probabilidades"** para cada classe.

**üîÅ A solu√ß√£o: One-Hot Encoding:**  
Em vez de representar a classe como um √∫nico n√∫mero inteiro, vamos representar cada classe como um vetor bin√°rio onde apenas uma posi√ß√£o √© 1 (indicando a classe), e as demais s√£o 0:

| Classe original | One-hot encoded |
| --------------- | --------------- |
| 0 (Setosa)      | \[1, 0, 0]      |
| 1 (Versicolor)  | \[0, 1, 0]      |
| 2 (Virginica)   | \[0, 0, 1]      |

**‚úÖ Aplicando Softmax:**
A fun√ß√£o *Softmax* vai transformar esses valores em algo assim:

```python
[0.65, 0.24, 0.11]
```

Ou seja:

 - **Classe 0 (Setosa):** 65% de chance;
 - **Classe 1 (Versicolor):** 24% de chance;
 - **Classe 2 (Virginica):** 11% de chance.

**üìå O que a rede est√° dizendo?**  
A *Rede Neural* acredita que:

 - A flor provavelmente √© Setosa (classe 0), com 65% de confian√ßa.
 - H√° uma chance menor de ser Versicolor (24%) ou Virginica (11%).

**üìä Por que √© √∫til?**  

 - Isso permite que o modelo **"escolha a classe"** com a **maior probabilidade** como resposta.
 - Tamb√©m permite medir o n√≠vel de incerteza nas previs√µes (ex: se todas as classes tiverem probabilidade pr√≥xima, a rede est√° em d√∫vida).

√ìtimo, agora que n√≥s j√° entendemos o que √© **"One-hot encoding"**, vamos aplicar isso na pr√°tica nos nossos `y_train` e `y_test`:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# One-hot encoding das classes
y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=3)
y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=3)
```

 - **Classe 0** ‚Üí [1, 0, 0]
 - **Classe 1** ‚Üí [0, 1, 0]
 - **Classe 2** ‚Üí [0, 0, 1]

</details>

<br/>

Continuando, agora n√≥s precisamos definir o n√∫mero de **neuronios de entrada** da nossa Rede Neural:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

Como n√≥s sabemos a nossa entrada vai ser **"X"**:

```python
print(X.shape)
```

**OUTPUT:**
```bash
(150, 4)
```

> **NOTE:**  
> Vejam que n√≥s temos 150 amostras, com 4 caracter√≠sticas (features) cada. Ou seja, nossa rede neural ter√° 4 neuronios de entrada.

[iris-v1.py](src/iris-v1.py)
```python
# Input com 4 features
n_inputs = X.shape[1]  # 4
inputs = tf.keras.Input(shape=(n_inputs,), name="input_layer")
```

</details>

<br/>

Agora n√≥s vamos definir as **Camadas Ocultas (Hidden layers)** da nossa Rede Neural:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Camadas Ocultas (Hidden Layers)
hidden1 = tf.keras.layers.Dense(5, activation="relu", name="hidden_layer_1")(inputs)
hidden2 = tf.keras.layers.Dense(3, activation="relu", name="hidden_layer_2")(hidden1)
```

</details>

<br/>

Agora n√≥s vamos conectar nossas **Camadas Ocultas (Hidden layers)** com nossa **Camada de Sa√≠da (Output layer)**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Camada de Sa√≠da (Output Layer)
output = tf.keras.layers.Dense(3, activation="softmax", name="output_layer")(hidden2)
```

</details>

<br/>

Para finalizar essa parte de camadas, vamos criar o nosso **Modelo (conectar as camadas)** e **visualizar sua estrutura**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Cria o modelo (conecta as camadas)
model = tf.keras.Model(inputs=inputs, outputs=output)
model.summary()  # Visualiza a estrutura da Rede Neural
```

**OUTPUT:**
```bash
Model: "functional"
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Layer (type)                         ‚îÉ Output Shape                ‚îÉ         Param # ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ input_layer (InputLayer)             ‚îÇ (None, 4)                   ‚îÇ               0 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ hidden_layer_1 (Dense)               ‚îÇ (None, 5)                   ‚îÇ              25 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ hidden_layer_2 (Dense)               ‚îÇ (None, 3)                   ‚îÇ              18 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ output_layer (Dense)                 ‚îÇ (None, 3)                   ‚îÇ              12 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 Total params: 55 (220.00 B)
 Trainable params: 55 (220.00 B)
 Non-trainable params: 0 (0.00 B)
```

</details>

<br/>

Agora n√≥s vamos **"preparar o modelo para treino"**. Essa parte conhecida como **"compila√ß√£o"** √© o momento em que voc√™ diz ao modelo:

> Aqui est√° **"como voc√™ vai aprender"**, **"como calcular o erro"**, e **"como vamos medir seu desempenho"**.


<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Compila√ß√£o
model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)
```

</details>

<br/>

Agora que n√≥s j√° orientamos o nosso modelo quais m√©tricas utilizar vamos treinar o nosso modelo:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Treinamento
model.fit(
    X_train,
    y_train_ohe,
    epochs=50,
    batch_size=8,
    verbose=0
)
```

 - `X_train`
   - Os dados de entrada (tamb√©m chamados de features ou atributos) usados para treinar o modelo.
   - Formato: Uma matriz com v√°rias linhas (amostras) e colunas (4 colunas no caso do Iris: s√©pala e p√©tala).
 - `y_train_ohe`
   - Os r√≥tulos das classes, ou seja, a **"resposta certa"** para cada exemplo.
   - Por que est√° em formato ohe (One-Hot Encoding)?
     - Porque a sa√≠da do modelo √© uma distribui√ß√£o de probabilidade (com Softmax), ent√£o o r√≥tulo tamb√©m precisa estar nesse formato compat√≠vel.
 - `epochs=50`
   - Define **quantas vezes o modelo ver√° todos os dados de treino**.
   - Exemplo: Se voc√™ tem 120 amostras e epochs=50, o modelo ver√° todas essas 120 amostras 50 vezes, tentando melhorar a cada repeti√ß√£o.
   - Quanto maior o n√∫mero, maior a chance do modelo aprender bem (mas cuidado com o overfitting).
 - `batch_size=8`
   - O n√∫mero de amostras que ser√£o usadas de cada vez antes de o modelo atualizar os pesos.
   - Exemplo: Se `batch_size=8`, o modelo pega 8 amostras, calcula o erro, ajusta os pesos, e s√≥ ent√£o passa para as pr√≥ximas 8.
   - Por que isso √© √∫til?
     - Treinar com batches pequenos ajuda a reduzir o uso de mem√≥ria e pode melhorar a generaliza√ß√£o.
 - `verbose=0`
   - Controla o n√≠vel de mensagens mostradas durante o treinamento.
   - Valores poss√≠veis:
     - 0: nenhum output (silencioso);
     - 1: barra de progresso por √©poca;
     - 2: uma linha por √©poca (resumo).
     - Por que usar 0?
       - √ötil para evitar polui√ß√£o visual quando n√£o queremos ver o log de treino.

</details>

<br/>

Agora n√≥s vamos **avalia (validar)** o modelo nos dados de teste:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Avalia√ß√£o
loss, accuracy = model.evaluate(X_test, y_test_ohe, verbose=0)
print(f"\nAcur√°cia do modelo: {accuracy:.2f}")
```

**OUTPUT:**
```bash
Acur√°cia do modelo: 0.70
```

**O que esses 0.70 significa?**  
Uma **acur√°cia de 0.70 (ou 70%)** significa que o nosso modelo **acertou 70% das previs√µes** no conjunto de teste.

</details>

<br/>

√ìtimo, que temos um modelo treinado (n√£o √© perfeito 70%, mas √© o que temos por agora) vamos tentar fazer algumas previs√µes:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Previs√£o com um exemplo real
sample = X_test[0].reshape(1, -1)
prediction = model(sample)
print("\nPredi√ß√£o (probabilidades):", prediction.numpy())
print("Classe prevista:", tf.argmax(prediction, axis=1).numpy())
print("Classe Real:", y_test[0])
```

**OUTPUT:**
```bash
Predi√ß√£o (probabilidades): [[0.12156641 0.6814825  0.19695109]]
Classe prevista: [1]
Classe Real: 1
```

</details>







































































































---

<div id="settings"></div>

## üöÄ Instala√ß√£o / Execu√ß√£o local

*Crie e ative o ambiente virtual (recomendado):**  

```bash
python -m venv environment
```

**LINUX:**  
```bash
source environment/bin/activate
```

**WINDOWS:**  
```bash
source environment/Scripts/activate
```

**ATUALIZE O PIP:**
```bash
python -m pip install --upgrade pip
```

**Instale as depend√™ncias:**  

```bash
pip install -U -v --require-virtualenv -r requirements.txt
```










<!--- ( REFER√äNCIAS ) --->

---

<div id="ref"></div>

## REFER√äNCIAS

 - [ChatGPT](https://chat.openai.com/)

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**

<!--->

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[](src/)
```python

```

</details>

<br/>
