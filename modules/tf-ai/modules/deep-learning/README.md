# Deep Learning

## ConteÃºdo

 - [**FunÃ§Ãµes de Perda (Loss Functions):**](#loss-functions)
   - **Problemas de RegressÃ£o:**
     - Mean Squared Error (MSE | Problemas com saÃ­da contÃ­nua, ex: preÃ§o, temperatura)
     - Mean Absolute Error (MAE | Quando vocÃª quer menos sensibilidade a outliers)
     - Huber Loss (RegressÃ£o robusta, CombinaÃ§Ã£o entre MSE e MAE, mais robusta a outliers)
   - **Problemas de ClassificaÃ§Ã£o:**
     - Binary Cross Entropy (Quando hÃ¡ apenas 2 classes, ex: spam/nÃ£o-spam)
     - Categorical Cross-Entropy (SaÃ­das em one-hot (ex: [0, 0, 1])
     - Sparse Categorical Crossentropy (SaÃ­das como rÃ³tulo inteiro, ex: 2 ao invÃ©s de [0, 0, 1])
   - **ComparaÃ§Ã£o entre distribuiÃ§Ãµes de probabilidade:**
     - Kullback-Leibler Divergence (Quando se quer medir a diferenÃ§a entre duas distribuiÃ§Ãµes)
 - [**Optimizers (ou Otimizadores):**](#optimizers)
   - Stochastic Gradient Descent (SGD)
   - SGD com Momentum
   - Learning Rate
   - Learning Rate Decay
   - Adam
   - RMSprop
   - Adagrad
   - Adadelta
   - Nadam
   - FTRL
 - **GrÃ¡ficos (Plots):**
   - **ğŸ¤– GrÃ¡ficos relacionados ao treinamento do modelo:**
     - [GrÃ¡fico de Perda (Loss) por Ã‰poca em Redes Neurais](#loss-by-epoch-plot)
     - [GrÃ¡fico de AcurÃ¡cia por Ã‰poca em Redes Neurais](#accuracy-by-epoch-plot)
     - [Interpretando os GrÃ¡ficos de Perda e AcurÃ¡cia](#loss-vs-accuracy)
   - **âœ… GrÃ¡ficos relacionados ao desempenho do modelo:**
     - [Matriz de ConfusÃ£o (Confusion Matrix) â€“ Entendendo os Acertos e Erros do Modelo](#confusion-matrix)
 - **Projetos:**
   - [ğŸŒ¸ Iris flower data set](#iris-data-set)
 - [**REFERÃŠNCIAS**](#ref)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->






































































































<!--- ( FunÃ§Ãµes de Perda (Loss Functions) ) --->

---

<div id="loss-functions"></div>

## FunÃ§Ãµes de Perda (Loss Functions)

> FunÃ§Ãµes de perda (Loss Functions) sÃ£o fÃ³rmulas matemÃ¡ticas que medem o erro entre a **"saÃ­da prevista"** pela rede neural e a **"saÃ­da real"**.

Em outras palavras, elas dizem:

 - QuÃ£o ruim estÃ¡ a previsÃ£o da rede;
 - **NOTE:** Quanto menor o valor da perda, melhor o modelo estÃ¡ aprendendo.

### ğŸ¯ Para que servem?

As *funÃ§Ãµes de perda (loss functions)** servem para orientar o processo de aprendizagem da rede neural. Durante o treinamento, a rede ajusta seus pesos internos para minimizar essa perda.

> **NOTE:**  
> â¡ï¸ Os otimizadores (como Adam, SGD, etc.) usam essa perda para saber como e quanto alterar os pesos (e bias) da rede.

### â±ï¸ Quando sÃ£o usadas?

As funÃ§Ãµes de perda **sÃ£o usadas em todo treino da rede neural**:

 - A cada batch de dados;
 - Em cada Ã©poca;
 - **NOTE:** SÃ£o indispensÃ¡veis â€” sem ela, o modelo nÃ£o aprende.

### ğŸ“¦ Tipos de FunÃ§Ãµes de Perda

> A funÃ§Ã£o de perda muda dependendo do tipo de problema (RegressÃ£o ou ClassificaÃ§Ã£o).

**ğŸ”µ 1. ClassificaÃ§Ã£o:**  
Para tarefas em que o modelo precisa prever classes (ex: detectar se uma flor Ã© iris-setosa, iris-versicolor ou iris-virginica).

| FunÃ§Ã£o de Perda                   | Quando usar                                                                |
| --------------------------------- | -------------------------------------------------------------------------- |
| `categorical_crossentropy`        | ClassificaÃ§Ã£o multiclasse com one-hot encoding                             |
| `sparse_categorical_crossentropy` | ClassificaÃ§Ã£o multiclasse com labels inteiros (sem one-hot)                |
| `binary_crossentropy`             | ClassificaÃ§Ã£o binÃ¡ria (ex: 0 ou 1)                                         |
| `Kullback-Leibler divergence`     | Quando se quer medir a diferenÃ§a entre duas distribuiÃ§Ãµes de probabilidade |

**ğŸŸ  2. RegressÃ£o:**  
Para tarefas em que o modelo precisa prever nÃºmeros contÃ­nuos (ex: preÃ§o de uma casa, temperatura, etc.)

| FunÃ§Ã£o de Perda       | Quando usar                                |
| --------------------- | ------------------------------------------ |
| `mean_squared_error`  | RegressÃ£o â€“ penaliza mais os grandes erros |
| `mean_absolute_error` | RegressÃ£o â€“ menos sensÃ­vel a outliers      |
| `huber_loss`          | RegressÃ£o robusta â€“ mistura dos dois acima |


### âš™ï¸ Como implementar?

<details>

<summary>TensorFlow (Python)</summary>

<br/>

Na prÃ¡tica, vocÃª escolhe a **funÃ§Ã£o de perda (loss function)** ao compilar o modelo com TensorFlow:

<br/>

```python
model.compile(
    loss='categorical_crossentropy',  # <- FunÃ§Ã£o de perda
)
```

</details>







































































































<!--- ( Optimizers (ou Otimizadores) ) --->

---

<div id="optimizers"></div>

## Optimizers (ou Otimizadores)

> **"Otimizadores"** sÃ£o algoritmos usados para **ajustar os pesos de uma rede neural durante o treinamento**, **minimizando a funÃ§Ã£o de perda (loss function)**.

### ğŸ¯ Para que servem?

 - Encontrar os melhores valores de pesos e bias;
 - Reduzir o erro (loss) entre as previsÃµes e os valores reais;
 - Ajudar o modelo a convergir para uma soluÃ§Ã£o precisa;
 - Tornar o treinamento mais rÃ¡pido e estÃ¡vel.

### âš™ï¸ Como implementar?

<details>

<summary>TensorFlow (Python)</summary>

<br/>

Na prÃ¡tica, vocÃª escolhe o **otimizador** ao compilar o modelo com TensorFlow:

<br/>

```python
model.compile(
    optimizer="adam",  # <- Otimizador
)
```

</details>

<br/>

| ğŸ”§ Nome                                  | ğŸ“ Breve descriÃ§Ã£o                                                                                                                                                    |
| ---------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Stochastic Gradient Descent (SGD)**    | MÃ©todo clÃ¡ssico. Atualiza pesos a cada amostra. Simples e eficiente, mas pode oscilar.                                                                                |
| **SGD com Momentum**                     | Variante do SGD. Adiciona uma "memÃ³ria" do gradiente anterior (momentum), o que ajuda a evitar oscilaÃ§Ãµes e acelera em direÃ§Ã£o Ã  soluÃ§Ã£o.                             |
| **Learning Rate**                        | HiperparÃ¢metro que controla o "tamanho do passo" que o otimizador dÃ¡ ao atualizar os pesos. Um valor muito alto pode pular o mÃ­nimo; muito baixo pode demorar demais. |
| **Learning Rate Decay**                  | EstratÃ©gia para diminuir gradualmente o learning rate conforme o treinamento avanÃ§a. Permite aprendizado rÃ¡pido no inÃ­cio e refinamento no final.                     |
| **Adam**                                 | Muito usado. Combina Momentum + RMSprop. Adapta os passos automaticamente e funciona bem em muitos problemas.                                                         |
| **RMSprop**                              | MantÃ©m histÃ³rico de gradientes com mÃ©dia exponencial. Ideal para problemas com dados nÃ£o estacionÃ¡rios (como sÃ©ries temporais).                                       |
| **Adagrad**                              | Adapta o tamanho do passo para cada parÃ¢metro com base no histÃ³rico de gradientes. Funciona bem com dados esparsos.                                                   |
| **Adadelta**                             | Variante do Adagrad que limita a acumulaÃ§Ã£o dos gradientes passados. Melhora a estabilidade.                                                                          |
| **Nadam**                                | Combina Adam com Nesterov momentum. Pode oferecer convergÃªncia mais rÃ¡pida e suave.                                                                                   |
| **FTRL (Follow The Regularized Leader)** | Otimizador especÃ­fico para problemas com muitos dados esparsos (ex: grandes sistemas de recomendaÃ§Ã£o). Muito usado no Google.                                         |







































































































<!--- ( GrÃ¡ficos ) --->

---

<div id="loss-by-epoch-plot"></div>

## GrÃ¡fico de Perda (Loss) por Ã‰poca em Redes Neurais

**ğŸ” O que Ã© "Perda (Loss) por Ã‰poca"?**  
Durante o treinamento de uma rede neural, o modelo tenta prever os resultados certos com base nos dados de entrada. A **funÃ§Ã£o de perda (loss function)** Ã© usada para medir o quanto o modelo erra em cada etapa.

 - **Perda (Loss):** Um nÃºmero que representa o erro do modelo â€” quanto menor, melhor.
 - **Ã‰poca (Epoch):** Uma passada completa por todos os dados de treino. Se vocÃª treina por 50 Ã©pocas, o modelo verÃ¡ o mesmo conjunto de dados 50 vezes.
 - **GrÃ¡fico de perda por Ã©poca:** Mostra como o *erro (loss)* muda ao longo do treinamento. Ele ajuda a entender se o modelo estÃ¡ aprendendo, estagnado ou atÃ© piorando.

> **Por que esse grÃ¡fico Ã© importante?**

 - **Ele responde perguntas como:**
   - O modelo estÃ¡ aprendendo com o tempo?
   - A perda estÃ¡ diminuindo ou aumentando?
   - O modelo estÃ¡ sofrendo overfitting?

Ã“timo, agora que nÃ³s jÃ¡ entendemos o que Ã© **"Perda (Loss) por Ã‰poca"**, vamos vamos criar uma funÃ§Ã£o que faz isso para nÃ³s para qualquer tipo de rede neural e salva o grÃ¡fico em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
import matplotlib.pyplot as plt
import os

def plot_loss(history, filename="loss_plot.png"):
    """
    Gera e salva um grÃ¡fico de perda (loss) por Ã©poca com base no objeto `history` do Keras.
    
    ParÃ¢metros:
    - history: objeto retornado por model.fit()
    - filename: nome do arquivo de imagem que serÃ¡ salvo
    """
    loss = history.history.get("loss")
    val_loss = history.history.get("val_loss")

    epochs = range(1, len(loss) + 1)

    plt.figure(figsize=(8, 5))
    plt.plot(epochs, loss, "b-", label="Loss de Treinamento")
    
    if val_loss:
        plt.plot(epochs, val_loss, "r--", label="Loss de ValidaÃ§Ã£o")

    plt.title("Perda (Loss) por Ã‰poca")
    plt.xlabel("Ã‰poca")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora Ã© sÃ³ treinar o modelo com o mÃ©todo `fit()` e pegar o objeto retornado pelo mesmo e passar para a funÃ§Ã£o `plot_loss()`:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    epochs=50,
    batch_size=8,
    verbose=0
)

plot_loss(history, filename="loss_plot-01.png")
```

**GRÃFICO:**

![img](images/loss_plot-01.png)  

</details>




















---

<div id="accuracy-by-epoch-plot"></div>

## GrÃ¡fico de AcurÃ¡cia por Ã‰poca em Redes Neurais

**ğŸ¯ O que Ã© "AcurÃ¡cia por Ã‰poca"?**  
Durante o treinamento de uma rede neural, alÃ©m de medir quanto o modelo estÃ¡ errando (com a perda), tambÃ©m queremos saber o quanto ele estÃ¡ acertando. Para isso usamos a acurÃ¡cia.

 - **AcurÃ¡cia (accuracy):** A porcentagem de acertos que o modelo teve em relaÃ§Ã£o ao total de exemplos avaliados.
 - **Ã‰poca (Epoch):** Uma rodada completa de treinamento, onde o modelo vÃª todos os dados uma vez.
 - **GrÃ¡fico de acurÃ¡cia por Ã©poca:** Mostra como a porcentagem de acertos do modelo muda a cada Ã©poca.

> **Para que serve esse grÃ¡fico?**

 - **Esse grÃ¡fico ajuda a responder:**
   - O modelo estÃ¡ melhorando ao longo do tempo?
   - O modelo estÃ¡ memorizando os dados de treino (overfitting)?
   - O desempenho estÃ¡ estagnado?

Ã‰ comum usar dois tipos de acurÃ¡cia no grÃ¡fico:

 - **AcurÃ¡cia de treino:** desempenho nos dados usados para treinar.
 - **AcurÃ¡cia de validaÃ§Ã£o:** desempenho em dados que o modelo nunca viu durante o treino.

Ã“timo, agora que nÃ³s jÃ¡ entendemos o que Ã© **"AcurÃ¡cia por Ã‰poca"**, vamos vamos criar uma funÃ§Ã£o que faz isso para nÃ³s para qualquer tipo de rede neural e salva o grÃ¡fico em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
def plot_accuracy(history, filename="accuracy_plot.png"):
    """
    Gera e salva um grÃ¡fico de acurÃ¡cia por Ã©poca com base no objeto `history` do Keras.

    ParÃ¢metros:
    - history: objeto retornado por model.fit()
    - filename: nome do arquivo de imagem que serÃ¡ salvo
    """
    acc = history.history.get("accuracy")
    val_acc = history.history.get("val_accuracy")

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(8, 5))
    plt.plot(epochs, acc, "b-", label="AcurÃ¡cia de Treinamento")
    
    if val_acc:
        plt.plot(epochs, val_acc, "g--", label="AcurÃ¡cia de ValidaÃ§Ã£o")

    plt.title("AcurÃ¡cia por Ã‰poca")
    plt.xlabel("Ã‰poca")
    plt.ylabel("AcurÃ¡cia")
    plt.legend()
    plt.grid(True)

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora Ã© sÃ³ treinar o modelo com o mÃ©todo `fit()` e pegar o objeto retornado pelo mesmo e passar para a funÃ§Ã£o `plot_accuracy()`:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    epochs=50,
    batch_size=8,
    verbose=0
)

plot_accuracy(history, filename="accuracy_plot-01.png")
```

**GRÃFICO:**

![img](images/accuracy_plot-01.png)  

</details>

<br/>




















---

<div id="loss-vs-accuracy"></div>

## Interpretando os GrÃ¡ficos de Perda e AcurÃ¡cia

Quando treinamos uma rede neural, Ã© essencial acompanhar como o modelo estÃ¡ se comportando a cada Ã©poca. Os dois principais grÃ¡ficos para isso sÃ£o:

 - GrÃ¡fico de Perda por Ã‰poca (Loss)
 - GrÃ¡fico de AcurÃ¡cia por Ã‰poca (Accuracy)

Mas, mais importante que olhar separadamente, Ã© analisar os dois juntos.

### ğŸ” Por que observar os dois grÃ¡ficos juntos?

A combinaÃ§Ã£o dos dois grÃ¡ficos ajuda vocÃª a entender se:

 - O modelo estÃ¡ aprendendo bem.
 - O modelo estÃ¡ sofrendo com overfitting (ajuste exagerado aos dados de treino).
 - O modelo estÃ¡ subajustado (underfitting), ou seja, nÃ£o aprendeu o suficiente.

### âœ… SituaÃ§Ã£o 1: O Modelo EstÃ¡ Aprendendo Bem

Comportamento esperado:

| GrÃ¡fico         | O que acontece?     |
| --------------- | ------------------- |
| ğŸ“‰ **Loss**     | Diminui com o tempo |
| ğŸ“ˆ **Accuracy** | Aumenta com o tempo |

> **â¡ï¸ InterpretaÃ§Ã£o:** 
> O modelo estÃ¡ aprendendo a generalizar e se adaptar aos dados.

### âš ï¸ SituaÃ§Ã£o 2: Overfitting (Sobreajuste)

Comportamento tÃ­pico:

| GrÃ¡fico         | Treinamento      | ValidaÃ§Ã£o         |
| --------------- | ---------------- | ----------------- |
| ğŸ“‰ **Loss**     | Diminui          | ComeÃ§a a subir ğŸ“ˆ |
| ğŸ“ˆ **Accuracy** | Sobe bastante ğŸ“ˆ | ComeÃ§a a cair ğŸ“‰  |

> **â¡ï¸ InterpretaÃ§Ã£o:**  
> O modelo estÃ¡ aprendendo bem os dados de treino, mas estÃ¡ memorizando demais e nÃ£o consegue generalizar para novos dados (test/validaÃ§Ã£o). Isso Ã© *"overfitting"*.

 - **ğŸ› ï¸ O que fazer nesse caso:**
   - Reduza o nÃºmero de Ã©pocas.
   - Use tÃ©cnicas como regularizaÃ§Ã£o (Dropout, L2, etc.).
   - Adicione mais dados ou use Data Augmentation.

### ğŸ§Š SituaÃ§Ã£o 3: Underfitting (Subajuste)

Comportamento tÃ­pico:

| GrÃ¡fico         | O que acontece?             |
| --------------- | --------------------------- |
| ğŸ“‰ **Loss**     | Fica alto ou nÃ£o muda muito |
| ğŸ“ˆ **Accuracy** | Baixa ou nÃ£o sobe muito     |

> **â¡ï¸ InterpretaÃ§Ã£o:**  
> O modelo nÃ£o estÃ¡ aprendendo o suficiente nem no treino nem na validaÃ§Ã£o.

 - **ğŸ› ï¸ O que fazer nesse caso:**
   - Aumente a complexidade do modelo (mais camadas ou neurÃ´nios).
   - Treine por mais Ã©pocas.
   - Melhore o prÃ©-processamento dos dados.

Ã“timo, agora que nÃ³s jÃ¡ entendemos qual a vantagem de comparar os grÃ¡ficos de **Perda (Loss)** e **AcurÃ¡cia (Accuracy)**, vamos criar um grÃ¡fico que gera essa comparaÃ§Ã£o para qualquer Rede Neural e salve em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
def loss_vs_accuracy_plot(history, filename="training_metrics.png"):
    """
    Gera e salva um grÃ¡fico com a perda (loss) e acurÃ¡cia (accuracy) por Ã©poca, usando o histÃ³rico de treinamento.
    
    ParÃ¢metros:
    - history: objeto retornado por model.fit()
    - filename: nome do arquivo de imagem a ser salvo
    """
    loss = history.history.get("loss")
    val_loss = history.history.get("val_loss")
    acc = history.history.get("accuracy")
    val_acc = history.history.get("val_accuracy")

    epochs = range(1, len(loss) + 1)

    plt.figure(figsize=(12, 5))

    # ğŸ”» Subplot 1: Perda
    plt.subplot(1, 2, 1)
    plt.plot(epochs, loss, "b-", label="Perda de Treinamento")
    if val_loss:
        plt.plot(epochs, val_loss, "r--", label="Perda de ValidaÃ§Ã£o")
    plt.title("Perda por Ã‰poca")
    plt.xlabel("Ã‰poca")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # ğŸ”º Subplot 2: AcurÃ¡cia
    plt.subplot(1, 2, 2)
    plt.plot(epochs, acc, "g-", label="AcurÃ¡cia de Treinamento")
    if val_acc:
        plt.plot(epochs, val_acc, "m--", label="AcurÃ¡cia de ValidaÃ§Ã£o")
    plt.title("AcurÃ¡cia por Ã‰poca")
    plt.xlabel("Ã‰poca")
    plt.ylabel("AcurÃ¡cia")
    plt.legend()
    plt.grid(True)

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora Ã© sÃ³ treinar o modelo com o mÃ©todo `fit()` e pegar o objeto retornado pelo mesmo e passar para a funÃ§Ã£o `loss_vs_accuracy_plot()`:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    validation_data=(X_test, y_test_ohe),  # obrigatÃ³rio para ter val_loss e val_accuracy
    epochs=50,
    batch_size=8,
    verbose=0
)

loss_vs_accuracy_plot(history, filename="loss-vs-accuracy-01.png")
```

**GRÃFICO:**

![img](images/loss-vs-accuracy-01.png)  

</details>




















---

<div id="confusion-matrix"></div>

## Matriz de ConfusÃ£o (Confusion Matrix) â€“ Entendendo os Acertos e Erros do Modelo

**âœ… O que Ã© a Matriz de ConfusÃ£o (Confusion Matrix)?**  
A *matriz de confusÃ£o (Confusion Matrix)* Ã© uma tabela que mostra como o seu `modelo de classificaÃ§Ã£o` estÃ¡ se saindo, comparando as previsÃµes com os valores reais.

Ela indica onde o modelo acertou e onde errou, classe por classe.

### ğŸ§  Estrutura da Matriz

Para um problema com 3 classes (por exemplo, Iris Dataset), por exemplo:

|                    | **Classe Prevista: 0**  | **Classe Prevista: 1**  | **Classe Prevista: 2**   |
| ------------------ | ----------------------- | ----------------------- | ------------------------ |
| **Classe Real: 0** | âœ… Acertos da classe 0  | âŒ Erros para classe 1  | âŒ Erros para classe 2  |
| **Classe Real: 1** | âŒ Erros para classe 0  | âœ… Acertos da classe 1  | âŒ Erros para classe 2  |
| **Classe Real: 2** | âŒ Erros para classe 0  | âŒ Erros para classe 1  | âœ… Acertos da classe 2  |

> **NOTE:**  
> Olhando para a tabela acima Ã© interessante ver quando (quantas vezes) ele errou por linha (classe real).  
> Quando ele erra nÃ³s dizemos que ele **"se confundiu"** com essa classe prevista (coluna).

 - Os acertos estÃ£o na diagonal principal (em verde).
 - Os erros estÃ£o fora da diagonal (em vermelho).

### ğŸ¯ Para que serve?

 - Avaliar quais classes o modelo *confunde*.
 - Diagnosticar erros especÃ­ficos (ex: a classe 1 sempre Ã© confundida com a 2).
 - Melhorar o modelo, observando padrÃµes de erro.

Ã“timo, agora que nÃ³s jÃ¡ entendemos qual a vantagem de utilizar uma **Matriz de ConfusÃ£o (Confusion Matrix)**, vamos criar um grÃ¡fico que gera essa comparaÃ§Ã£o para qualquer Rede Neural e salve em uma imagem:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[plots.py](src/plots.py)
```python
def plot_confusion_matrix(y_true, y_pred, class_names=None, filename="confusion_matrix.png"):
    """
    Gera e salva um grÃ¡fico de matriz de confusÃ£o.
    
    ParÃ¢metros:
    - y_true: rÃ³tulos verdadeiros
    - y_pred: rÃ³tulos previstos pelo modelo
    - class_names: nomes das classes (opcional)
    - filename: nome do arquivo de saÃ­da
    """
    # Calcula a matriz de confusÃ£o
    cm = confusion_matrix(y_true, y_pred)

    # Cria os rÃ³tulos se nÃ£o forem fornecidos
    if class_names is None:
        class_names = [str(i) for i in range(cm.shape[0])]

    # Cria o grÃ¡fico
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names,
                yticklabels=class_names)

    plt.xlabel("Classe Predita")
    plt.ylabel("Classe Real")
    plt.title("Matriz de ConfusÃ£o")
    plt.tight_layout()

    plt.savefig("../images/" + filename)
    plt.close()
```

Agora nÃ³s vamos:

 - Treinar o modelo;
 - Fazer previsÃµes com `X_test`:
   - `y_pred = [[0.1, 0.7, 0.2], [0.8, 0.1, 0.1], [0.2, 0.3, 0.5]]` 
 - Retornar o Ã­ndice de maior probabilidade em cada linha:
   - Ou seja, transforma as probabilidades em rÃ³tulos previstos (classes).
   - `y_pred_classes = tf.argmax(y_pred, axis=1).numpy()`
   - Retorno: `<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 0, 2])>`

`y_pred_classes = tf.argmax(y_pred, axis=1).numpy()`  
Essa linha transforma as probabilidades de saÃ­da da rede em classes previstas (inteiros).

| SaÃ­da da rede (`y_pred`) | Classe prevista (`y_pred_classes`) |
| ------------------------ | ---------------------------------- |
| \[0.1, 0.7, 0.2]         | 1 (maior probabilidade Ã© 0.7)      |
| \[0.8, 0.1, 0.1]         | 0 (maior probabilidade Ã© 0.8)      |
| \[0.2, 0.3, 0.5]         | 2 (maior probabilidade Ã© 0.5)      |

O cÃ³digo antes de chamar a funÃ§Ã£o para criar a **Matriz de ConfusÃ£o (Confusion Matrix)** Ã© o seguinte:

```python
# Treinamento
history = model.fit(
    X_train,
    y_train_ohe,
    validation_data=(X_test, y_test_ohe),  # obrigatÃ³rio para ter val_loss e val_accuracy
    epochs=50,
    batch_size=8,
    verbose=0
)

y_pred = model.predict(X_test)
y_pred_classes = tf.argmax(y_pred, axis=1).numpy()

plot_confusion_matrix(
    y_true=y_test,
    y_pred=y_pred_classes,
    class_names=["Setosa", "Versicolor", "Virginica"],
    filename="confusion-matrix-01.png"
)
```

**GRÃFICO:**

![img](images/confusion-matrix-01.png)  

</details>







































































































<!--- ( Projetos ) --->

---

<div id="iris-data-set"></div>

## ğŸŒ¸ Iris flower data set

O **Iris Dataset** Ã© um dos conjuntos de dados mais famosos da histÃ³ria da ciÃªncia de dados e aprendizado de mÃ¡quina. Ele foi publicado em 1936 pelo estatÃ­stico e biÃ³logo Ronald A. Fisher.

**ğŸ“Š O que ele contÃ©m?**  
O conjunto de dados contÃ©m 150 amostras de flores da planta Iris, divididas igualmente entre 3 espÃ©cies:

 - Iris setosa;
 - Iris versicolor;
 - Iris virginica.

![img](images/iris-flowers-01.png)  

Para cada flor, foram medidas 4 caracterÃ­sticas:

 - **ğŸŒ¸ 1. Comprimento da sÃ©pala (Sepal length):**
   - **O que Ã© sÃ©pala?**
     - Ã‰ a parte verde que protege a flor quando ainda estÃ¡ em botÃ£o (antes de abrir).
     - Medida: O comprimento da sÃ©pala em centÃ­metros (cm).
     - Exemplo: 5.1 cm.
 - **ğŸŒ¸ 2. Largura da sÃ©pala (Sepal width):**
   - **O que mede?**
     - A largura (horizontal) da sÃ©pala, tambÃ©m em centÃ­metros.
     - ImportÃ¢ncia: Algumas espÃ©cies tÃªm sÃ©palas mais largas ou estreitas, o que ajuda a diferenciÃ¡-las.
 - **ğŸŒº 3. Comprimento da pÃ©tala (Petal length):**
   - **O que Ã© pÃ©tala?**
     - Ã‰ a parte colorida da flor, que atrai polinizadores.
     - Medida: O comprimento da pÃ©tala em centÃ­metros.
     - RelevÃ¢ncia: Essa Ã© uma das features mais Ãºteis para separar as espÃ©cies.
 - **ğŸŒº 4. Largura da pÃ©tala (Petal width):**
   - Medida: A largura da pÃ©tala em centÃ­metros.
   - ImportÃ¢ncia: TambÃ©m Ã© muito Ãºtil â€” por exemplo, a Iris setosa tende a ter pÃ©talas bem estreitas.

![img](images/iris-flowers-02.png)  

**ğŸ§  Para que ele Ã© usado?**  
O Iris Dataset Ã© amplamente usado para aprender conceitos de **"classificaÃ§Ã£o"**:

 - Redes Neurais;
 - SVM;
 - K-NN.

**âœ… Por que ele Ã© tÃ£o popular?**

 - Pequeno e fÃ¡cil de entender;
 - Bem balanceado (50 amostras por classe);
 - Perfeito para iniciantes praticarem classificaÃ§Ã£o multiclasse.

Ã“timo, agora que jÃ¡ entendemos o dataset, vamos planejar como serÃ¡ nossa Rede Neural utilizando o mesmo. De inÃ­cio vamos revisar quais sÃ£o as entradas:

 - **ğŸŒ¸ 1. Comprimento da sÃ©pala (Sepal length):**
 - **ğŸŒ¸ 2. Largura da sÃ©pala (Sepal width):**
 - **ğŸŒº 3. Comprimento da pÃ©tala (Petal length):**
 - **ğŸŒº 4. Largura da pÃ©tala (Petal width):**

> **E a saÃ­da?**  
> A saÃ­da (output) do Iris dataset Ã© a espÃ©cie da flor, ou seja, a classe Ã  qual cada flor pertence.

| CÃ³digo | Classe (EspÃ©cie)  | DescriÃ§Ã£o                                   |
| ------ | ----------------- | ------------------------------------------- |
| 0      | *Iris setosa*     | SÃ©pala larga e pÃ©tala curta                 |
| 1      | *Iris versicolor* | Medidas intermediÃ¡rias entre as outras duas |
| 2      | *Iris virginica*  | PÃ©talas mais longas e largas                |

Sabendo de tudo isso vamos imaginar que a nossa Rede Neural vai ter a seguinte estrutura:

![img](images/iris-ann.png)  

> **NOTE:**  
> Vejam que a saÃ­da vai ser uma das 3 classes (Iris setosa, Iris versicolor e Iris virginica).

Para comeÃ§ar a implementaÃ§Ã£o da nossa Rede Neural vamos importar as bibliotecas necessÃ¡rias:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
import os
import tensorflow as tf
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
```

</details>

<br/>

Agora vamos carregar o conjunto de dados:

 - X recebe as 4 caracterÃ­sticas das flores (sÃ©palas e pÃ©talas).
 - y recebe os rÃ³tulos das classes (0=setosa, 1=versicolor, 2=virginica).

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Carrega o dataset Iris
iris = load_iris()
X = iris.data  # 4 features
y = iris.target  # 3 classes
```

</details>

<br/>

Agora vamos aplicar uma **normalizaÃ§Ã£o (ou padronizaÃ§Ã£o)** nos dados:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# PrÃ©-processamento
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

</details>

<br/>

Continuando, um processo comum em *Machine Learning (Deep Learning)* Ã© dividir os **dados em dados de treinamento** e **"dados de teste"**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# DivisÃ£o treino/teste
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
```

 - Divide os dados em `80%` para treino e `20%` para teste.
 - `random_state=42` garante que a divisÃ£o seja reprodutÃ­vel.

</details>

<br/>

Agora vamos aplicar **"One-hot encoding"** nas classes.

> **What? O que Ã© isso?**

**ğŸŸ¡ O problema:**  
Por padrÃ£o, as classes do Iris dataset sÃ£o representadas como nÃºmeros inteiros:

| Flor | Classe |
| ---- | ------ |
| A    | 0      |
| B    | 1      |
| C    | 2      |

Por exemplo, se vocÃª estiver utilizando o Python a saÃ­da deve ser algo parecido com isso:

```python
print(y)
print("DimensÃ£o de y:", y.shape)
```

**OUTPUT:**
```bash
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
DimensÃ£o de y: (150,)
```

> **NOTE:**  
> Vejam que nÃ³s temos como saÃ­da um array 1-dimensinal com 150 elementos.

**NOTE:**  
SÃ³ que esses nÃºmeros **nÃ£o sÃ£o interpretados corretamente** por redes neurais quando usamos funÃ§Ãµes de saÃ­da como **Softmax**, que retornam **"probabilidades"** para cada classe.

**ğŸ” A soluÃ§Ã£o: One-Hot Encoding:**  
Em vez de representar a classe como um Ãºnico nÃºmero inteiro, vamos representar cada classe como um vetor binÃ¡rio onde apenas uma posiÃ§Ã£o Ã© 1 (indicando a classe), e as demais sÃ£o 0:

| Classe original | One-hot encoded |
| --------------- | --------------- |
| 0 (Setosa)      | \[1, 0, 0]      |
| 1 (Versicolor)  | \[0, 1, 0]      |
| 2 (Virginica)   | \[0, 0, 1]      |

**âœ… Aplicando Softmax:**
A funÃ§Ã£o *Softmax* vai transformar esses valores em algo assim:

```python
[0.65, 0.24, 0.11]
```

Ou seja:

 - **Classe 0 (Setosa):** 65% de chance;
 - **Classe 1 (Versicolor):** 24% de chance;
 - **Classe 2 (Virginica):** 11% de chance.

**ğŸ“Œ O que a rede estÃ¡ dizendo?**  
A *Rede Neural* acredita que:

 - A flor provavelmente Ã© Setosa (classe 0), com 65% de confianÃ§a.
 - HÃ¡ uma chance menor de ser Versicolor (24%) ou Virginica (11%).

**ğŸ“Š Por que Ã© Ãºtil?**  

 - Isso permite que o modelo **"escolha a classe"** com a **maior probabilidade** como resposta.
 - TambÃ©m permite medir o nÃ­vel de incerteza nas previsÃµes (ex: se todas as classes tiverem probabilidade prÃ³xima, a rede estÃ¡ em dÃºvida).

Ã“timo, agora que nÃ³s jÃ¡ entendemos o que Ã© **"One-hot encoding"**, vamos aplicar isso na prÃ¡tica nos nossos `y_train` e `y_test`:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# One-hot encoding das classes
y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=3)
y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=3)
```

 - **Classe 0** â†’ [1, 0, 0]
 - **Classe 1** â†’ [0, 1, 0]
 - **Classe 2** â†’ [0, 0, 1]

</details>

<br/>

Continuando, agora nÃ³s precisamos definir o nÃºmero de **neuronios de entrada** da nossa Rede Neural:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

Como nÃ³s sabemos a nossa entrada vai ser **"X"**:

```python
print(X.shape)
```

**OUTPUT:**
```bash
(150, 4)
```

> **NOTE:**  
> Vejam que nÃ³s temos 150 amostras, com 4 caracterÃ­sticas (features) cada. Ou seja, nossa rede neural terÃ¡ 4 neuronios de entrada.

[iris-v1.py](src/iris-v1.py)
```python
# Input com 4 features
n_inputs = X.shape[1]  # 4
inputs = tf.keras.Input(shape=(n_inputs,), name="input_layer")
```

</details>

<br/>

Agora nÃ³s vamos definir as **Camadas Ocultas (Hidden layers)** da nossa Rede Neural:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Camadas Ocultas (Hidden Layers)
hidden1 = tf.keras.layers.Dense(5, activation="relu", name="hidden_layer_1")(inputs)
hidden2 = tf.keras.layers.Dense(3, activation="relu", name="hidden_layer_2")(hidden1)
```

</details>

<br/>

Agora nÃ³s vamos conectar nossas **Camadas Ocultas (Hidden layers)** com nossa **Camada de SaÃ­da (Output layer)**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Camada de SaÃ­da (Output Layer)
output = tf.keras.layers.Dense(3, activation="softmax", name="output_layer")(hidden2)
```

</details>

<br/>

Para finalizar essa parte de camadas, vamos criar o nosso **Modelo (conectar as camadas)** e **visualizar sua estrutura**:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Cria o modelo (conecta as camadas)
model = tf.keras.Model(inputs=inputs, outputs=output)
model.summary()  # Visualiza a estrutura da Rede Neural
```

**OUTPUT:**
```bash
Model: "functional"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                         â”ƒ Output Shape                â”ƒ         Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ input_layer (InputLayer)             â”‚ (None, 4)                   â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hidden_layer_1 (Dense)               â”‚ (None, 5)                   â”‚              25 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hidden_layer_2 (Dense)               â”‚ (None, 3)                   â”‚              18 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ output_layer (Dense)                 â”‚ (None, 3)                   â”‚              12 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 55 (220.00 B)
 Trainable params: 55 (220.00 B)
 Non-trainable params: 0 (0.00 B)
```

</details>

<br/>

Agora nÃ³s vamos **"preparar o modelo para treino"**. Essa parte conhecida como **"compilaÃ§Ã£o"** Ã© o momento em que vocÃª diz ao modelo:

> Aqui estÃ¡ **"como vocÃª vai aprender"**, **"como calcular o erro"**, e **"como vamos medir seu desempenho"**.


<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# CompilaÃ§Ã£o
model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)
```

</details>

<br/>

Agora que nÃ³s jÃ¡ orientamos o nosso modelo quais mÃ©tricas utilizar vamos treinar o nosso modelo:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# Treinamento
model.fit(
    X_train,
    y_train_ohe,
    epochs=50,
    batch_size=8,
    verbose=0
)
```

 - `X_train`
   - Os dados de entrada (tambÃ©m chamados de features ou atributos) usados para treinar o modelo.
   - Formato: Uma matriz com vÃ¡rias linhas (amostras) e colunas (4 colunas no caso do Iris: sÃ©pala e pÃ©tala).
 - `y_train_ohe`
   - Os rÃ³tulos das classes, ou seja, a **"resposta certa"** para cada exemplo.
   - Por que estÃ¡ em formato ohe (One-Hot Encoding)?
     - Porque a saÃ­da do modelo Ã© uma distribuiÃ§Ã£o de probabilidade (com Softmax), entÃ£o o rÃ³tulo tambÃ©m precisa estar nesse formato compatÃ­vel.
 - `epochs=50`
   - Define **quantas vezes o modelo verÃ¡ todos os dados de treino**.
   - Exemplo: Se vocÃª tem 120 amostras e epochs=50, o modelo verÃ¡ todas essas 120 amostras 50 vezes, tentando melhorar a cada repetiÃ§Ã£o.
   - Quanto maior o nÃºmero, maior a chance do modelo aprender bem (mas cuidado com o overfitting).
 - `batch_size=8`
   - O nÃºmero de amostras que serÃ£o usadas de cada vez antes de o modelo atualizar os pesos.
   - Exemplo: Se `batch_size=8`, o modelo pega 8 amostras, calcula o erro, ajusta os pesos, e sÃ³ entÃ£o passa para as prÃ³ximas 8.
   - Por que isso Ã© Ãºtil?
     - Treinar com batches pequenos ajuda a reduzir o uso de memÃ³ria e pode melhorar a generalizaÃ§Ã£o.
 - `verbose=0`
   - Controla o nÃ­vel de mensagens mostradas durante o treinamento.
   - Valores possÃ­veis:
     - 0: nenhum output (silencioso);
     - 1: barra de progresso por Ã©poca;
     - 2: uma linha por Ã©poca (resumo).
     - Por que usar 0?
       - Ãštil para evitar poluiÃ§Ã£o visual quando nÃ£o queremos ver o log de treino.

</details>

<br/>

Agora nÃ³s vamos **avalia (validar)** o modelo nos dados de teste:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# AvaliaÃ§Ã£o
loss, accuracy = model.evaluate(X_test, y_test_ohe, verbose=0)
print(f"\nAcurÃ¡cia do modelo: {accuracy:.2f}")
```

**OUTPUT:**
```bash
AcurÃ¡cia do modelo: 0.70
```

**O que esses 0.70 significa?**  
Uma **acurÃ¡cia de 0.70 (ou 70%)** significa que o nosso modelo **acertou 70% das previsÃµes** no conjunto de teste.

</details>

<br/>

Ã“timo, que temos um modelo treinado (nÃ£o Ã© perfeito 70%, mas Ã© o que temos por agora) vamos tentar fazer algumas previsÃµes:

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[iris-v1.py](src/iris-v1.py)
```python
# PrevisÃ£o com um exemplo real
sample = X_test[0].reshape(1, -1)
prediction = model(sample)
print("\nPrediÃ§Ã£o (probabilidades):", prediction.numpy())
print("Classe prevista:", tf.argmax(prediction, axis=1).numpy())
print("Classe Real:", y_test[0])
```

**OUTPUT:**
```bash
PrediÃ§Ã£o (probabilidades): [[0.12156641 0.6814825  0.19695109]]
Classe prevista: [1]
Classe Real: 1
```

</details>







































































































---

<div id="settings"></div>

## ğŸš€ InstalaÃ§Ã£o / ExecuÃ§Ã£o local

*Crie e ative o ambiente virtual (recomendado):**  

```bash
python -m venv environment
```

**LINUX:**  
```bash
source environment/bin/activate
```

**WINDOWS:**  
```bash
source environment/Scripts/activate
```

**ATUALIZE O PIP:**
```bash
python -m pip install --upgrade pip
```

**Instale as dependÃªncias:**  

```bash
pip install -U -v --require-virtualenv -r requirements.txt
```










<!--- ( REFERÃŠNCIAS ) --->

---

<div id="ref"></div>

## REFERÃŠNCIAS

 - [ChatGPT](https://chat.openai.com/)

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**

<!--->

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[](src/)
```python

```

</details>

<br/>
