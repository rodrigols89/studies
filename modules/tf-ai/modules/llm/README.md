# Large Language Models

## Conte√∫do

 - **Introdu√ß√£o a LLMs:**
   - [O que s√£o LLMs?](#intro-to-llm)
   - [Como o modelo "entende" linguagem?](#how-understand)
   - [Como LLMs s√£o treinados)](#how-are-trained)
   - [Diferen√ßa entre LLMs e modelos tradicionais de NLP](#llm-vs-nlp)
   - [Por que os Transformers revolucionaram o NLP?](#transformers-inovation)
   - [Como funciona o Mecanismo de Attention (Aten√ß√£o)?](#attention-mechanism)
   - [Exemplos de tarefas resolvidas por LLMs](#llm-examples)
 - [**üöÄ Instala√ß√£o / Execu√ß√£o local**](#settings)
 - [**REFER√äNCIAS**](#ref)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->






































































































<!--- ( Introdu√ß√£o a LLMs ) --->

---

<div id="intro-to-llm"></div>

## O que s√£o LLMs?

**üìò Defini√ß√£o:**  
LLMs (Large Language Models) s√£o modelos de aprendizado de m√°quina treinados para **"entender"**, **"gerar"** e **"manipular linguagem natural"**.

> **NOTE:**  
> Eles s√£o chamados de "grandes" por causa da quantidade massiva de par√¢metros (milh√µes ou bilh√µes) e por serem treinados em grandes volumes de texto da internet, livros, artigos, f√≥runs, c√≥digo-fonte etc.




















---

<div id="how-understand"></div>

### Como o modelo "entende" linguagem?

Na verdade, LLMs n√£o entendem no sentido humano. Eles aprendem probabilidades estat√≠sticas:

> Se eu vejo a frase: "O c√©u est√° ___", a palavra mais prov√°vel √© "azul".  
> Esse "palpite" √© feito com base no que ele viu durante o treinamento.




















---

<div id="how-are-trained"></div>

### Como LLMs s√£o treinados

 - **Pr√©-treinamento (Pretraining):**
   - O modelo √© exposto a grandes quantidades de texto e aprende padr√µes de linguagem por meio de tarefas como **"prever a pr√≥xima palavra"** (auto-regressivo) ou **"preencher palavras faltantes"** (m√°scara).
 - **Ajuste fino (Fine-tuning):**
   - O modelo pode ser adaptado para tarefas espec√≠ficas, como:
     - Classifica√ß√£o de texto;
     - Tradu√ß√£o;
     - Gera√ß√£o de c√≥digo;
     - Resumo de documentos.




















---

<div id="llm-vs-nlp"></div>

## Diferen√ßa entre LLMs e modelos tradicionais de NLP

 - Antes dos LLMs, os modelos de **NLP** eram **espec√≠ficos para cada tarefa**, como *an√°lise de sentimentos*, *tradu√ß√£o*, ou *resumo*.
 - Com os **LLMs**, **um √∫nico modelo pode ser usado para v√°rias tarefas** com pouco ou nenhum ajuste.

### Compara√ß√£o Direta

| Caracter√≠stica                  | Modelos Tradicionais de NLP                 | LLMs (Large Language Models)                        |
| ------------------------------- | ------------------------------------------- | --------------------------------------------------- |
| **Arquitetura**                 | Simples (SVM, Regress√£o, Naive Bayes, RNNs) | Transformer (profundo e em larga escala)            |
| **Treinamento**                 | Um modelo por tarefa                        | Um √∫nico modelo para tarefas m√∫ltiplas              |
| **Requer feature engineering?** | Sim! Manual e demorado                      | N√£o. O modelo aprende tudo automaticamente          |
| **Escalabilidade**              | Limitado                                    | Altamente escal√°vel e flex√≠vel                      |
| **Precis√£o/Desempenho**         | OK, mas limitado com grandes volumes        | Alta, especialmente com dados em larga escala       |
| **Entrada/Sa√≠da**               | Tipicamente vetores num√©ricos               | Texto puro (prompts e respostas)                    |
| **Contexto considerado**        | Curto (√†s vezes s√≥ 1 frase)                 | Longo (v√°rios par√°grafos ou at√© milhares de tokens) |
| **Exemplos**                    | TF-IDF + SVM, Word2Vec + LSTM               | GPT, BERT, T5, LLaMA, Claude, Gemini                |

### Explicando com um exemplo

 - **Modelo tradicional (pr√©-LLM):**
   - Transformar o texto em vetores (TF-IDF, Bag of Words).
   - Treinar um SVM ou uma Regress√£o Log√≠stica para prever o sentimento.
   - Modelo s√≥ serve pra essa tarefa.
 - **LLM:**
   - Voc√™ escreve:
     - `Classifique o sentimento desta frase: Estou muito feliz hoje!`
   - O modelo responde:
     - `Sentimento: Positivo`
   - **NOTE:** O mesmo modelo pode tamb√©m traduzir, resumir, gerar c√≥digo...

### Vantagens dos LLMs sobre modelos tradicionais

 - ‚úÖ **Generaliza√ß√£o:** Um modelo para muitas tarefas;
 - ‚úÖ **Zero-shot & few-shot:** Resolve tarefas com poucas instru√ß√µes;
 - ‚úÖ Menos depend√™ncia de dados rotulados;
 - ‚úÖ Contexto mais longo e melhor compreens√£o;
 - ‚úÖ Gera√ß√£o de linguagem natural mais fluida.

### Mas os modelos tradicionais morreram?

 - **‚ùå N√£o! Eles ainda s√£o √∫teis quando:**
   - Voc√™ tem poucos dados e poucos recursos computacionais;
   - A tarefa √© muito espec√≠fica e n√£o exige interpreta√ß√£o profunda;
   - Voc√™ precisa de explicabilidade clara e r√°pida.




















---

<div id="transformers-inovation"></div>

## Por que os Transformers revolucionaram o NLP?

> Transformers s√£o uma arquitetura introduzida no artigo [**"Attention is All You Need" (2017)**](https://arxiv.org/abs/1706.03762).

**NOTE:**  
Eles eliminaram a necessidade de processar palavras em sequ√™ncia como faziam **RNNs** e **LSTMs** ‚Äî e com isso, permitiram muito mais *"paralelismo"*, *"contexto global"* e *"velocidade"*.

### üö´ Problema dos modelos anteriores (RNN, LSTM)

 - Processavam tokens um por um (sequencialmente).
 - Sofriam com longas depend√™ncias ("o que foi dito 30 palavras atr√°s?").
 - Eram lentos para treinar.
 - Tinha dificuldade com frases longas e contexto amplo.

### ‚úÖ Como o Transformer resolveu tudo isso?

**A resposta:** Attention Mechanism.

> O modelo aprende a **"prestar aten√ß√£o"** nas palavras mais importantes do texto ‚Äî independentemente da posi√ß√£o!

### üß© Componentes principais do Transformer

| Componente                    | Fun√ß√£o B√°sica                                                    |
| ----------------------------- | ---------------------------------------------------------------- |
| **Embedding**                 | Converte palavras em vetores num√©ricos.                          |
| **Self-Attention**            | Calcula a import√¢ncia de cada palavra em rela√ß√£o √†s outras.      |
| **Positional Encoding**       | Adiciona informa√ß√£o da posi√ß√£o das palavras (j√° que √© paralelo). |
| **Feedforward Layers**        | Faz transforma√ß√µes profundas nos vetores.                        |
| **Normalization & Residuals** | Ajudam a estabilizar e melhorar o aprendizado.                   |




















---

<div id="attention-mechanism"></div>

## Como funciona o Mecanismo de Attention (Aten√ß√£o)?

> O mecanismo de aten√ß√£o permite que o modelo **"foque" em partes importantes da entrada** ‚Äî como humanos fazem ao ler.

Por exemplo, imagine que n√≥s temos a frase:

Imagine a frase:

> ‚ÄúA ma√ß√£ estava azeda, ent√£o ela foi jogada fora.‚Äù

 - A palavra **"ela"** poderia se referir √† **ma√ß√£** ou √† **azeda**.
 - O modelo precisa **‚Äúprestar aten√ß√£o‚Äù** nas palavras relevantes para entender corretamente.

### üßÆ F√≥rmula matem√°tica (simples)

A f√≥rmula matem√°tica (simples) √© a seguinte:

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$

 - `QK·µÄ`
   - Compara (multiplica) os queries com os keys ‚Üí gera pontua√ß√µes de aten√ß√£o.
 - `/ ‚àöd_k`
   - Normaliza para evitar explos√µes num√©ricas.
 - `softmax`
   - Transforma em probabilidades.
 - `√ó V`
   - Gera a aten√ß√£o ponderada, ou seja, a sa√≠da final.

### üî¢ Exemplo num√©rico ilustrativo

```bash
Frase: ["O", "gato", "correu"]

‚Üí Para "correu", o modelo calcula:

Q(correu) ‚Ä¢ K(O)     ‚Üí  baixa aten√ß√£o
Q(correu) ‚Ä¢ K(gato)  ‚Üí  alta aten√ß√£o
Q(correu) ‚Ä¢ K(correu)‚Üí  m√©dia aten√ß√£o
```

> **Resultado:**  
> O modelo vai ponderar mais o **"gato"**, porque **"gato correu"** tem uma rela√ß√£o forte.




















---

<div id="llm-examples"></div>

### Exemplos de tarefas resolvidas por LLMs

Vamos come√ßar com uma introdu√ß√£o de algumas tarefas que podem ser resolvidas utilizando **"LLMs"**:

| Tarefa                      | Exemplo pr√°tico                             |
| --------------------------- | ------------------------------------------- |
| **Gera√ß√£o de texto**        | Chatbots, reda√ß√£o autom√°tica                |
| **Classifica√ß√£o**           | An√°lise de sentimentos, spam vs. n√£o spam   |
| **Tradu√ß√£o**                | Ingl√™s ‚Üí Portugu√™s, etc.                    |
| **Perguntas e Respostas**   | Assistente de d√∫vidas                       |
| **Resumo autom√°tico**       | Resumir longos artigos                      |
| **Extra√ß√£o de informa√ß√µes** | Pegar nomes, datas, eventos de um texto     |
| **Gera√ß√£o de c√≥digo**       | Auto-complete em IDEs, explica√ß√£o de c√≥digo |

### üöÄ Aplica√ß√µes Reais das LLMs

| **√Årea**                   | **Aplica√ß√£o**                                                                                       |
| -------------------------- | --------------------------------------------------------------------------------------------------- |
| **Assistentes virtuais**   | Chatbots inteligentes (ex: ChatGPT, Google Bard, Alexa)                                             |
| **Educa√ß√£o**               | Tutores personalizados, corre√ß√£o autom√°tica de reda√ß√µes, explica√ß√µes sob demanda                    |
| **Sa√∫de**                  | An√°lise de prontu√°rios, resposta a perguntas m√©dicas, apoio √† decis√£o cl√≠nica                       |
| **Atendimento ao cliente** | Respostas autom√°ticas, suporte 24/7, resumo de intera√ß√µes com usu√°rios                              |
| **Pesquisa e ci√™ncia**     | S√≠ntese de artigos, gera√ß√£o de hip√≥teses, revis√£o autom√°tica de literatura                          |
| **Programa√ß√£o**            | Autocompletar c√≥digo, explicar fun√ß√µes, gerar trechos em diferentes linguagens (ex: GitHub Copilot) |
| **Tradu√ß√£o de idiomas**    | Tradu√ß√µes contextuais e multil√≠ngues, com adapta√ß√£o ao dom√≠nio espec√≠fico                           |
| **Cria√ß√£o de conte√∫do**    | Gera√ß√£o de artigos, roteiros, marketing, posts para redes sociais                                   |
| **Direito**                | An√°lise de contratos, extra√ß√£o de cl√°usulas, sumariza√ß√£o de decis√µes legais                         |
| **An√°lise de sentimentos** | Classifica√ß√£o de avalia√ß√µes e sentimentos em redes sociais e e-commerce                             |
| **Seguran√ßa cibern√©tica**  | Explica√ß√£o de exploits, an√°lise de logs e gera√ß√£o de alertas                                        |
| **Games e NPCs**           | Di√°logos gerados dinamicamente, comportamentos inteligentes, roteiros                               |

### üìà Exemplos Reais de Impacto com LLMs

| **Organiza√ß√£o / Produto** | **Uso de LLMs**                                                                          |
| --------------------------|------------------------------------------------------------------------------------------|
| `Duolingo`                | Feedback em tempo real sobre frases escritas por alunos com explica√ß√µes contextualizadas |
| `Notion AI`               | Gera√ß√£o e reformula√ß√£o de textos, resumos autom√°ticos, cria√ß√£o de tarefas                |
| `Khan Academy (GPT-4)`    | Tutor personalizado que responde d√∫vidas dos alunos com explica√ß√µes passo a passo        |
| `GitHub Copilot`          | Sugest√µes de c√≥digo em tempo real, explica√ß√µes de fun√ß√µes, gera√ß√£o de testes             |
| `GrammarlyGO`             | Reescrita e aprimoramento de textos com base no contexto do usu√°rio                      |
| `Legal Robot`             | An√°lise de contratos com explica√ß√£o em linguagem natural das cl√°usulas jur√≠dicas         |
| `You.com (YouChat)`       | Motor de busca com respostas geradas por LLM, integrando fontes e interatividade         |






































































































<!--- ( Arquitetura do Transformer: Encoder e Decode ) --->

---





   - [](#)


































































































<!--- ( üöÄ Instala√ß√£o / Execu√ß√£o local ) --->

---

<div id="settings"></div>

## üöÄ Instala√ß√£o / Execu√ß√£o local

*Crie e ative o ambiente virtual (recomendado):**  

```bash
python -m venv environment
```

**LINUX:**  
```bash
source environment/bin/activate
```

**WINDOWS:**  
```bash
source environment/Scripts/activate
```

**ATUALIZE O PIP:**
```bash
python -m pip install --upgrade pip
```

**Instale as depend√™ncias:**  

```bash
pip install -U -v --require-virtualenv -r requirements.txt
```










<!--- ( REFER√äNCIAS ) --->

---

<div id="ref"></div>

## REFER√äNCIAS

 - [ChatGPT](https://chat.openai.com/)

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**

<!--->

<details>

<summary>TensorFlow (Python)</summary>

<br/>

[](src/)
```python

```

</details>

<br/>
