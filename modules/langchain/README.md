# LangChain

> Minnhas **notas** e **c√≥digos** da ferramenta [LangChain](https://github.com/langchain-ai)

## Conte√∫do

 - **Fundamentos de LLM com LangChain:**
   - [`O que √© ChatOpenAI?`](#intro-to-chatopenai)
   - [`M√©todos de Execu√ß√£o de Runnables no LangChain`](#runnables-methods-langchain)
   - [`M√©todo .invoke()`](#invoke-method)
   - [`Modelo de Mensagens de Conversa no LangChain`](#messages-in-langchain)
   - [`"Templates de Prompt" no LangChain`](#templates-in-langchain)
 - **Indexando os dados:**
   - [`Chunks (chunk_size)`](#intro-to-chunks)
   - [`Overlap (chunk_overlap)`](#intro-to-overlap)
   - [`O que s√£o Incorpora√ß√µes de Texto? (Text Embeddings)`](#text-embeddings)
   - [`Carregadores de Documentos (Document Loaders) no LangChain`](#document-loaders)
   - [`Divis√£o de Texto (Text Splitters) no LangChain`](#text-splitters)
   - [`Gerando Texto Incorporado (Text Embeddings)`](#making-embeddings)
   - [`Indexa√ß√£o de Conhecimento (Indexing) no LangChain`](#chp02-indexing)
   - [`PGVector: Banco de Dados Vetorial com PostgreSQL no LangChain`](#chp02-pgvector)
   - [`Exemplo Completo ‚Äî Usando PGVector com LangChain`](#chp02-pgvector-exemplo)
 - **Conversando com os seus dados:**
   - [`Introdu√ß√£o ao RAG (Retrieval-Augmented Generation)`](#chp03-intro-to-rag)
   - [`Tr√™s fases de um sistema RAG`](#chp03-three-steps)
   - [`Exemplo Completo: Construindo um RAG com LangChain + PGVector`](#chp03-full-example)
   - [`Criando um "chain" de perguntas e respostas`](#chp03-chain)
   - [`Criando um RAG "runnable"`](#chp03-rag-runnable)
   - [`Criando RAG que utilizam "source", "category" nos resultados`](#chp03-source-category)
   - [`RAG conversacional com mem√≥ria`](#rag-conversational-with-memory)
 - **Pr√©-Processamento e Valida√ß√£o:**
   - [`Como um RAG pode ser avaliado`](#rag-evaluation)
 - **Dicas & Truques:**
   - [`Prompting vs. Fine Tuning vs. RAG`](#chp03-prompting-vs-fine-tuning-vs-rag)
 - **Configura√ß√µes:**
   - [`Criando o ambiente virtual`](#create-env)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->









































































































<!--- ( Fundamentos de LLM com LangChainn ) --->

---

<div id="intro-to-chatopenai"></div>

## `O que √© ChatOpenAI?`

ChatOpenAI √© a classe do LangChain respons√°vel por conversar com os chat models da OpenAI, como:

 - gpt-3.5-turbo
 - gpt-4
 - gpt-4o
 - gpt-4.1

Ela implementa a interface de Chat Models, ou seja:

 - Recebe mensagens (strings ou objetos de mensagem)
 - Envia para a API da OpenAI
 - Retorna uma resposta estruturada (n√£o apenas texto cru)

### `Que pacote √© esse "langchain_openai"?`

A partir do LangChain `0.1 ‚Üí 0.2`, o projeto foi dividido em pacotes menores.

**Antes (antigo ‚ùå):**
```python
from langchain.chat_models import ChatOpenAI
```

**Agora (correto ‚úÖ):**
```python
from langchain_openai.chat_models import ChatOpenAI
```

### `O que significa "chat_models"?`

Dentro do pacote langchain_openai, existem v√°rios m√≥dulos:

```bash
langchain_openai/
 ‚îú‚îÄ‚îÄ llms/          ‚Üí modelos antigos (completion)
 ‚îú‚îÄ‚îÄ chat_models/   ‚Üí modelos de chat (messages)
 ‚îú‚îÄ‚îÄ embeddings/    ‚Üí embeddings OpenAI
```

### `Par√¢metros da classe ChatOpenAI`

Os principais (os mais usados) par√¢metros da classe ChatOpenAI s√£o:

 - `model`
   - Define qual modelo da OpenAI ser√° usado.
   - Exemplos:
     - "gpt-3.5-turbo"
     - "gpt-4"
     - "gpt-4o"
     - "gpt-4.1"
 - `temperature`
   - Controla a criatividade da resposta.
   - 0.0 ‚ûî	Determin√≠stico
   - 0.2 ‚ûî	Mais preciso
   - 0.7 ‚ûî	Criativo
   - 1.0+ ‚ûî	Muito criativo
 - `max_tokens`
   - Limita o n√∫mero m√°ximo de tokens gerados na resposta.
 - `timeout`
   - Timeout da requisi√ß√£o (em segundos).
   - Exemplo: timeout=60
 - `verbose`
   - Mostra logs internos do LangChain.
   - Exemplo: verbose=True

Por exemplo:

[ChatOpenAI-v1.py](codes/ChatOpenAI-v1.py)
```python
from langchain_openai.chat_models import ChatOpenAI

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)
```

No exemplo acima n√≥s temos:

 - `model = ChatOpenAI()`
   - Criamos uma inst√¢ncia da classe ChatOpenAI.
 - `model="gpt-3.5-turbo"`
   - Estamos dizendo que esse modelo vai utilizar o modelo `"gpt-3.5-turbo"` da OpenAI.
 - `temperature=0`
   - Aqui estamos dizendo que queremos uma resposta determin√≠stica.
   - **NOTE:** Ou seja, a resposta vai ser sempre a mesma, mesmo se o modelo for criativo.
 - `max_tokens=100`
   - Limitamos o n√∫mero de tokens gerados na resposta.




















---

<div id="runnables-methods-langchain"></div>

## `M√©todos de Execu√ß√£o de Runnables no LangChain`

A classe **ChatOpenAI** herda de **BaseChatModel** e implementa a interface **Runnable**, o que permite executar o modelo de diferentes formas:

```bash
ChatOpenAI
   ‚îî‚îÄ‚îÄ BaseChatModel
         ‚îî‚îÄ‚îÄ Runnable
               ‚îú‚îÄ‚îÄ invoke()
               ‚îú‚îÄ‚îÄ batch()
               ‚îú‚îÄ‚îÄ stream()
               ‚îî‚îÄ‚îÄ ainvoke()
```

Ou seja, todos esses m√©todos servem para executar o modelo, mudando apenas a forma de execu√ß√£o:

 - `.invoke()` ‚Üí execu√ß√£o s√≠ncrona (mais comum)
 - `.ainvoke()` ‚Üí execu√ß√£o ass√≠ncrona (async)
 - `.stream()` ‚Üí resposta em streaming
 - `.batch()` ‚Üí m√∫ltiplas entradas de uma vez

### `1Ô∏è‚É£ O que todos eles t√™m em comum?`

Todos:

 - Executam um Runnable (ChatOpenAI, chain, prompt, RAG, etc.)
 - Usam a mesma l√≥gica interna
 - Recebem o mesmo tipo de entrada
 - Produzem o mesmo tipo de sa√≠da final

> **üìå A diferen√ßa est√° em:**  
> **como** e **quando** a resposta √© entregue.

### `2Ô∏è‚É£ .invoke() ‚Äî execu√ß√£o s√≠ncrona (padr√£o)`

**Quando usar:**

 - C√≥digo simples
 - Scripts
 - Notebooks
 - Fluxo linear

**Exemplo:**
```python
response = chat.invoke("Explain LangChain in one sentence.")
print(response.content)
```

**Caracter√≠sticas:**

 - Bloqueia at√© a resposta chegar
 - Retorna um √∫nico resultado

### `.batch() ‚Äî m√∫ltiplas entradas de uma vez`

**Quando usar:**

 - Processar muitos prompts
 - Ganhar performance
 - Reduzir overhead

**Exemplo:**
```python
inputs = [
    "Explain LangChain in one sentence.",
    "What is RAG?",
    "What is LCEL?"
]

responses = chat.batch(inputs)

for r in responses:
    print(r.content)
```

**Caracter√≠sticas:**

 - Recebe list[input]
 - Retorna list[output]
 - Pode rodar em paralelo (dependendo do backend)

### `4Ô∏è‚É£ .stream() ‚Äî resposta em streaming (token a token)`

**Quando usar:**

 - Chat em tempo real
 - Interfaces web
 - UX melhor

**Exemplo:**
```python
for chunk in chat.stream("Explain LangChain in one sentence."):
    print(chunk.content, end="", flush=True)
```

**Caracter√≠sticas:**

 - Retorna um gerador
 - Entrega partes da resposta conforme s√£o geradas
 - Ideal para interfaces interativas

### `.ainvoke() ‚Äî execu√ß√£o ass√≠ncrona (async)`

**Quando usar:**

 - FastAPI
 - Apps web
 - Alta concorr√™ncia

**Exemplo:**
```python
import asyncio

async def run():
    response = await chat.ainvoke("Explain LangChain in one sentence.")
    print(response.content)

asyncio.run(run())
```

**Caracter√≠sticas:**

 - N√£o bloqueia a thread
 - Requer async/await

### `6Ô∏è‚É£ Compara√ß√£o direta`

| M√©todo       | Execu√ß√£o   | Entrada | Sa√≠da      | Uso t√≠pico            |
| ------------ | ---------- | ------- | ---------- | --------------------- |
| `.invoke()`  | S√≠ncrona   | 1 item  | 1 resposta | Scripts / notebooks   |
| `.batch()`   | Paralela   | Lista   | Lista      | Processamento em lote |
| `.stream()`  | Streaming  | 1 item  | Gerador    | Chat em tempo real    |
| `.ainvoke()` | Ass√≠ncrona | 1 item  | 1 resposta | Web / APIs            |

### `7Ô∏è‚É£ üß† Resumo final`

 - ‚úÖ Sim, eles s√£o similares no prop√≥sito
 - ‚ùå N√£o s√£o iguais no comportamento

Todos fazem a mesma coisa:

> **‚û°Ô∏è executam um Runnable.**  
> *Mas cada um √© ideal para um cen√°rio espec√≠fico.*




















---

<div id="invoke-method"></div>

## `M√©todo .invoke()`

O m√©todo `.invoke()` √© a forma padr√£o e moderna de **executar um modelo no LangChain**.

> **üìå Em termos simples:**  
> `.invoke()` envia uma entrada para o modelo e retorna uma resposta estruturada.

### `Par√¢metros input=... do m√©todo .invoke()`

 - Esse √© o mais importante.
 - Ele pode assumir 3 formas principais.

**Forma 1 ‚Äî String simples (mais comum):** [invoke-01.py](codes/invoke-01.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

response = model.invoke("The sky is")
print(response.content)
```

**OUTPUT:**
```bash
blue and clear, with fluffy white clouds scattered across the horizon. The sun is shining brightly, casting a warm glow over everything below. It's a beautiful day to be outside and enjoy the beauty of nature.
```

> **NOTE:**  
> üìå Aqui o *LangChain* converte automaticamente a string em:  
> `HumanMessage(content="The sky is")`

**Forma 2 ‚Äî Lista de mensagens (chat expl√≠cito):** [invoke-02.py](codes/invoke-02.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="Explain LangChain in one sentence.")
]

response = model.invoke(messages)
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that aims to facilitate cross-border communication and collaboration by providing language translation services.
```

**üìå Use isso quando precisar de:**

 - system
 - assistant
 - hist√≥rico de conversa

**Forma 3 ‚Äî Dicion√°rio (com Prompt Template / LCEL):** [invoke-03.py](codes/invoke-03.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_template(
    "Explain {topic} in one sentence."
)

chain = prompt | model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

response = chain.invoke({"topic": "LangChain"})
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that aims to facilitate cross-border communication and collaboration by providing language translation services.
```

**üìå Aqui:**

 - `.invoke()` recebe um dict
 - O prompt resolve as vari√°veis antes de chamar o modelo

### `O que o m√©todo .invoke() retorna?`

Sempre retorna um objeto:

```bash
AIMessage
```

Exemplo real:

```bash
AIMessage(
    content="LangChain is a framework for building LLM-powered apps.",
    additional_kwargs={},
    response_metadata={
        "model": "gpt-3.5-turbo",
        "token_usage": {...}
    }
)
```

### `Diferen√ßa entre .invoke() e .predict() (importante)`

| M√©todo       | Status                  |
| ------------ | ----------------------- |
| `.invoke()`  | ‚úÖ Padr√£o atual        |
| `.predict()` | ‚ö†Ô∏è Legacy / deprecated |

### `Erros comuns (‚ö†Ô∏è)`

**‚ùå Erro 1 ‚Äî esperar string:**
```python
text = chat.invoke("Hello")
print(text)  # errado
```

**‚úÖ Correto:**
```python
print(text.content)
```




















---

<div id="messages-in-langchain"></div>

## `Modelo de Mensagens de Conversa no LangChain`

Para que serve `langchain_core.messages`?

```python
from langchain_core.messages import ...
```

Esse m√≥dulo define o formato padr√£o de mensagens que o LangChain usa para representar uma conversa entre:

 - sistema
 - usu√°rio
 - modelo
 - ferramentas

### `1Ô∏è‚É£ Por que isso √© importante?`

Porque modelos de chat n√£o recebem texto solto, eles recebem listas de mensagens com pap√©is:

```bash
System ‚Üí Human ‚Üí AI ‚Üí Tool ‚Üí Human ‚Üí AI
```

Se voc√™ entende isso, voc√™ entende:

 - chat
 - mem√≥ria
 - RAG
 - agentes
 - tool calling

### `2Ô∏è‚É£ Imports mais utilizados

 - `SystemMessage`
   - Define regras e comportamento do modelo.

```python
SystemMessage(content="You are a helpful assistant.")
```

 - `HumanMessage`
   - Representa a mensagem do usu√°rio.

```python
HumanMessage(content="What is LangChain?")
```

 - `AIMessage`
   - Representa respostas do modelo (√∫til para hist√≥rico).

```python
AIMessage(content="LangChain is a framework...")
```

 - `ToolMessage`
   - Representa a sa√≠da de uma ferramenta (agents / tools).

```python
ToolMessage(
    content="Result from search",
    tool_call_id="abc123"
)
```

### `3Ô∏è‚É£ Exemplo m√≠nimo completo`

[messages-01.py](codes/messages-01.py)
```python
from langchain_core.messages import (
    SystemMessage,
    HumanMessage
)
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

load_dotenv()

chat = ChatOpenAI()

messages = [
    SystemMessage(content="You are a tutor."),
    HumanMessage(content="Explain LangChain in one sentence.")
]

response = chat.invoke(messages)
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that enables developers to build decentralized language-oriented applications.
```

### `5Ô∏è‚É£ Regra de ouro üß†`

> **LangChain** √© sobre **orquestrar mensagens**, n√£o strings.

Se voc√™ domina `langchain_core.messages`, voc√™ domina:

 - conversas
 - contexto
 - hist√≥rico




















---

<div id="templates-in-langchain"></div>

## `"Templates de Prompt" no LangChain`

Para que serve `langchain_core.prompts`?

```python
from langchain_core.prompts import ...
```

> **NOTE:**  
> üìå Esse m√≥dulo **define como voc√™ constr√≥i prompts de forma estruturada**, reutiliz√°vel e segura no LangChain.

Ele existe para:

 - evitar concatena√ß√£o manual de strings
 - organizar vari√°veis de entrada
 - padronizar prompts
 - integrar prompts com modelos via LCEL

> **üìå Em resumo:**  
> Prompts deixam de ser texto solto e passam a ser componentes.

## `1Ô∏è‚É£ Por que isso √© importante?`

> **Modelos de linguagem n√£o recebem c√≥digo**, **"recebem prompts bem formados"**.

Sem templates:

 - mais erro
 - mais duplica√ß√£o
 - dif√≠cil de manter

Com templates:

 - clareza
 - reutiliza√ß√£o
 - f√°cil integra√ß√£o com *chains* e *RAG*

### 2Ô∏è‚É£ Imports mais utilizados

 - `PromptTemplate`
   - Usado para modelos que recebem texto simples (LLMs).

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    "Translate {text} to Portuguese."
)
```

 - `ChatPromptTemplate`
   - Usado para modelos de chat.

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(
    "Explain {topic} in one sentence."
)
```

 - `MessagesPlaceholder`
   - Usado para injetar hist√≥rico de conversa dinamicamente.

```python
from langchain_core.prompts import MessagesPlaceholder

ChatPromptTemplate.from_messages([
    ("system", "You are a tutor."),
    MessagesPlaceholder("history"),
    ("human", "{question}")
])
```

 - `HumanMessagePromptTemplate`
   - Define explicitamente um template de mensagem humana.
 - `SystemMessagePromptTemplate`
   - Define regras do sistema como template.

### `3Ô∏è‚É£ Exemplo m√≠nimo completo`

[prompts-01.py](codes/prompts-01.py)
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "Explain {topic} in one sentence.")
])

chain = prompt | ChatOpenAI()

response = chain.invoke({"topic": "LangChain"})
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a decentralized platform that connects businesses with language service providers for efficient and secure translation services.
```

### `5Ô∏è‚É£ Regra de ouro üß†`

 - Use **prompts** para **definir estrutura**.
 - Use **messages** para **controlar conversa**.







































































































<!--- ( RAG Parte I: Indexar os teus dados ) --->

---

<div id="intro-to-chunks"></div>

## `Chunks (chunk_size)`

> Imagina que voc√™ precisa criar um *RAG* que utiliza a **Constitui√ß√£o Federal** para auxiliar advogados.

Se, para uma pergunta sobre **direito do consumidor**, enviarmos *toda a constitui√ß√£o*, isso far√° com que o modelo de IA n√£o consiga processar todas as informa√ß√µes, j√° que, quanto maior o prompt, **menos precisa tende a ser a resposta**.

Para isso, utilizamos a t√©cnica de **"chunks"**, onde, pegamos um arquivo geral e o quebramos em v√°rios pequenos trechos:

![img](images/chunks-01.png)  

> **NOTE:**  
> üìå Podemos usar um `chunk_size` para especificar quantos caracteres teremos por **chunk**.

A *Constitui√ß√£o Federal* possui **64.488 palavras**. Se definirmos um `chunk_size` como **100**, teremos **645 mini arquivos (64.488√∑100)** da Constitui√ß√£o.

### üßæ Exemplos:

 - **Art. 1¬∫** A Rep√∫blica Federativa do Brasil, formada pela uni√£o indissol√∫vel dos Estados e Munic√≠pios e do Distrito Federal, constitui-se em Estado Democr√°tico de Direito e tem como fundamentos...
 - **Par√°grafo √∫nico.** Todo o poder emana do povo, que o exerce por meio de representantes eleitos ou diretamente, nos termos desta Constitui√ß√£o...
 - **Art. 2¬∫** S√£o Poderes da Uni√£o, independentes e harm√¥nicos entre si, o Legislativo, o Executivo e o Judici√°rio...





















---

<div id="intro-to-overlap"></div>

## `Overlap (chunk_overlap)`

**Mas agora enfrentamos outro problema:**  
Ao separar o texto por chunks, pode ser que eles **fiquem sem sentido**, j√° que partes importantes da informa√ß√£o podem ser **cortadas (separadas)**.

> **NOTE:**  
> üìå Para isso, usamos o par√¢metro `chunk_overlap`.

 - Ele define quantos caracteres de sobreposi√ß√£o haver√° entre um chunk e o pr√≥ximo.
 - üëâ Isso √© √∫til para manter o contexto entre peda√ßos consecutivos.

Por exemplo, Exemplo com `chunk_size = 500` e `chunk_overlap = 100`

```bash
[000 ... 499]
[400 ... 899]
[800 ... 1299]
```

Vejam que:

 - **Nosso primeiro chunk come√ßa em 000 e termina em 499:**
   - Ou seja, as primeiras 500 palavras da Constitui√ß√£o.
 - **Nosso segundo chunk come√ßa em 400 (por causa do "chunk_overlap = 100") e termina em 899:**
   - Ou seja, ele est√° pegando as 100 √∫ltimas palavras do chunk anterior.
   - **NOTE:** Isso √© importante para evitar perda de contexto entre os chunks.

Por exemplo, imagine que n√≥s temos o seguinte texto:

```bash
Python √© uma excelente linguagem de programa√ß√£o para web e IA.
```

Se aplicarmos:

 - `chunk_size = 7`
 - `chunk_overlap = 3`

Vamos ter:

```bash
Python √© uma excelente linguagem de programa√ß√£o para web e IA.
   |   |  |      |         |     |       |
   0   1  2      3         4     5       6
   ---------------------------------------
                chunk 1


Python √© uma excelente linguagem de programa√ß√£o para web e IA.
                           |     |       |       |    |  |  |
                           1     2       3       4    5  6  7
                           -----------------------------------
                                        chunk 2
```

> **NOTE:**  
> üìå Vejam que n√≥s pegamos as **3 √∫ltimas palavras do chunk (overlap = 3)** para n√£o perde contexto.





















---

<div id="text-embeddings"></div>

## `O que s√£o Incorpora√ß√µes de Texto? (Text Embeddings)`

> As **incorpora√ß√µes de texto** s√£o uma forma de **converter palavras ou frases do texto em dados num√©ricos que uma m√°quina pode entender**.

 - Pense nisso como se estivesse transformando o texto em uma lista de n√∫meros, em que cada n√∫mero captura uma parte do significado do texto.
 - Essa t√©cnica ajuda as m√°quinas a entender o contexto e as rela√ß√µes entre as palavras.

![img](images/embeddings-01.png)  





















---

<div id="document-loaders"></div>

## `Carregadores de Documentos (Document Loaders) no LangChain`

Para que serve `langchain_community.document_loaders`?

```python
from langchain_community.document_loaders import ...
```

Esse m√≥dulo re√∫ne **carregadores de documentos (document loaders)**, ou seja, classes respons√°veis por:

 - ler arquivos ou fontes externas (PDF, TXT, CSV, HTML, URLs, etc.)
 - extrair o conte√∫do
 - converter tudo para o formato padr√£o do LangChain: `Document`

> **üìå Em RAG, tudo come√ßa aqui.**  
> Se o dado n√£o foi carregado corretamente, o resto do pipeline falha.

### `1Ô∏è‚É£ O que √© um Document?`

Todo *loader* retorna uma lista de objetos `Document`:

```python
Document(
    page_content="texto extra√≠do",
    metadata={"source": "..."}
)
```

Esse formato √© usado depois por:

 - text splitters
 - embeddings
 - vector stores
 - retrievers

### `2Ô∏è‚É£ Imports mais utilizados`

 - `TextLoader`
   - Carrega arquivos `.txt`

```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/example.txt")
documents = loader.load()
```

> **üìå Uso comum:**  
> logs, textos simples, dumps.

 - `PyPDFLoader`
   - Extrai texto de arquivos PDF (p√°gina por p√°gina).

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("data/manual.pdf")
documents = loader.load()
```

> **NOTE:**  
> üìå Cada p√°gina vira um Document.

 - `CSVLoader`
   - L√™ arquivos .csv.

```python
from langchain_community.document_loaders import CSVLoader

loader = CSVLoader(
    file_path="data/users.csv",
    source_column="email"
)
documents = loader.load()
```

> **NOTE:**  
> üìå Cada linha vira um Document.

 - `JSONLoader`
   - Carrega dados estruturados em `JSON`.

```python
from langchain_community.document_loaders import JSONLoader

loader = JSONLoader(
    file_path="data/data.json",
    jq_schema=".items[]",
    text_content=False
)
documents = loader.load()
```

> **NOTE:**  
> üìå Muito usado com APIs e dumps de dados.

 - `UnstructuredFileLoader`
   - Loader gen√©rico para v√°rios formatos.

```python
from langchain_community.document_loaders import UnstructuredFileLoader

loader = UnstructuredFileLoader("data/report.docx")
documents = loader.load()
```

**üìå Funciona com:**

 - `.docx`
 - `.pptx`
 - `.html`
 - `.md`

 - `WebBaseLoader`
   - Carrega conte√∫do direto de URLs.

```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://docs.langchain.com/")
documents = loader.load()
```

> **NOTE:**  
> üìå Muito usado para *RAG* com *documenta√ß√£o online*.

 - `DirectoryLoader`
   - Carrega v√°rios arquivos de uma pasta.

```python
from langchain_community.document_loaders import DirectoryLoader

loader = DirectoryLoader(
    "data/",
    glob="**/*.pdf"
)
documents = loader.load()
```

> **NOTE:**  
> üìå Essencial para bases grandes de documentos.

### `Documenta√ß√£o do "Document loaders"`

> Aqui -> [Document loaders](https://docs.langchain.com/oss/python/integrations/document_loaders)





















---

<div id="text-splitters"></div>

## `Divis√£o de Texto (Text Splitters) no LangChain`

1Ô∏è‚É£ Para que serve langchain_text_splitters?

```python
from langchain_text_splitters import ...
```

Esse m√≥dulo cont√©m os **Text Splitters**, respons√°veis por **quebrar documentos grandes em peda√ßos menores (chunks)**.

üìå Isso √© essencial porque:

 - LLMs t√™m limite de tokens
 - embeddings funcionam melhor com textos menores
 - buscas vetoriais ficam mais precisas

> **NOTE:**  
> üìå Em *RAG*, o `splitter` √© o cora√ß√£o da qualidade.

### `2Ô∏è‚É£ O problema que ele resolve`

 - **Sem splitter ‚ùå:**
   - texto grande demais
   - perda de contexto
   - embeddings ruins
 - **Com splitter ‚úÖ:**
   - chunks consistentes
   - contexto preservado
   - melhor recupera√ß√£o

### `3Ô∏è‚É£ Imports mais utilizados`

 - `RecursiveCharacterTextSplitter (mais usado)`
   - Divide texto de forma inteligente, tentando manter estrutura.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)
```

Par√¢metros principais:

 - `chunk_size` ‚Üí tamanho do chunk
 - `chunk_overlap` ‚Üí sobreposi√ß√£o entre chunks
 - `separators` ‚Üí lista de separadores (opcional)

> **NOTE:**  
> üìå Padr√£o para RAG com texto natural.

 - `CharacterTextSplitter`
   - Divide texto de forma simples e direta.

```python
from langchain_text_splitters import CharacterTextSplitter

splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=300,
    chunk_overlap=30
)
```

Par√¢metros:

 - `separator` ‚Üí caractere de divis√£o (\n, .)
 - `chunk_size`
 - `chunk_overlap`

> **NOTE:**  
> üìå Bom para textos bem estruturados.

 - `TokenTextSplitter`
   - Divide baseado em tokens, n√£o caracteres.

```python
from langchain_text_splitters import TokenTextSplitter

splitter = TokenTextSplitter(
    chunk_size=256,
    chunk_overlap=25
)
```

Par√¢metros:

 - `chunk_size` ‚Üí tokens por chunk
 - `chunk_overlap`
 - `encoding_name` ‚Üí tokenizer (ex: "cl100k_base")

> **NOTE:**  
> üìå Ideal quando voc√™ precisa respeitar limite exato de tokens.

### `4Ô∏è‚É£ Regra de ouro üß†`

Escolha o `splitter` de acordo com o tipo de dado, n√£o por moda.

 - texto livre ‚Üí `RecursiveCharacterTextSplitter`
 - tokens exatos ‚Üí `TokenTextSplitter`
 - markdown ‚Üí `MarkdownTextSplitter`
 - HTML ‚Üí `HTMLTextSplitter`





















---

<div id="making-embeddings"></div>

## `Gerando Texto Incorporado (Text Embeddings)`

O LangChain tem v√°rias API para se trabalhar com Textos Incorporados (Text Embeddings) e voc√™s podem escolhar qualquer um deles de acordo com a sua necessidade.

> Aqui est√° a documenta√ß√£o oficial -> [Embedding models](https://docs.langchain.com/oss/python/integrations/text_embedding)

Por exemplo, vamos utilizar a classe `OpenAIEmbeddings` da OpenAI:

[text-embeddings-01.py](codes/text-embeddings-01.py)
```python
from langchain_openai import OpenAIEmbeddings

from dotenv import load_dotenv

load_dotenv()

model = OpenAIEmbeddings()

embeddings = model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!",
    ]
)
```

Se voc√™ desse um print() o resultado seria algo parecido com isso (n√£o vou mostrar porque √© muito grande):

**OUTPUT:**
```bash
[
  [
    -0.004845875, 0.004899438, -0.016358767, -0.024475135, -0.017341806,
      0.012571548, -0.019156644, 0.009036391, -0.010227379, -0.026945334,
      0.022861943, 0.010321903, -0.023479493, -0.0066544134, 0.007977734,
    0.0026371893, 0.025206111, -0.012048521, 0.012943339, 0.013094575,
    -0.010580265, -0.003509951, 0.004070787, 0.008639394, -0.020631202,
    ... 1511 more items
  ]
  [
      -0.009446913, -0.013253193, 0.013174579, 0.0057552797, -0.038993083,
      0.0077763423, -0.0260478, -0.0114384955, -0.0022683728, -0.016509168,
      0.041797023, 0.01787183, 0.00552271, -0.0049789557, 0.018146982,
      -0.01542166, 0.033752076, 0.006112323, 0.023872782, -0.016535373,
      -0.006623321, 0.016116094, -0.0061090477, -0.0044155475, -0.016627092,
    ... 1511 more items
  ]
  ... 3 more items
]
```





















---

<div id="chp02-indexing"></div>

## `Indexa√ß√£o de Conhecimento (Indexing) no LangChain (+Indexing vs. Retrieval)`

### `1Ô∏è‚É£ O que √© Indexing no LangChain?`

> O termo `Indexing` se refere ao processo de **transformar dados brutos (documentos)** em uma estrutura **pesquis√°vel por similaridade sem√¢ntica**.

Em termos pr√°ticos, √© quando voc√™:

 - carrega documentos
 - divide em chunks
 - gera embeddings
 - armazena tudo em um Vector Store

> **NOTE:**  
> üìå Sem `indexa√ß√£o`, n√£o existe *RAG*.

### `2Ô∏è‚É£ Por que a indexa√ß√£o √© necess√°ria?`

LLMs:

 - n√£o ‚Äúlembram‚Äù de documentos
 - n√£o pesquisam sozinhas
 - n√£o acessam arquivos diretamente

> **NOTE:**  
> üìå A `indexa√ß√£o` *cria uma mem√≥ria externa consult√°vel*.

### `3Ô∏è‚É£ Fluxo completo de Indexing`

No processo de indexa√ß√£o, o LangChain segue o seguinte fluxo:

```bash
Dados brutos
   ‚Üì
DocumentLoader
   ‚Üì
TextSplitter
   ‚Üì
Embeddings
   ‚Üì
VectorStore (Index)
```

> **NOTE:**  
> üìå O resultado final √© o √≠ndice vetorial.

### `4Ô∏è‚É£ Exemplo m√≠nimo de indexa√ß√£o`

**Passo 1 ‚Äî Carregar documentos:**
```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/manual.txt")
documents = loader.load()
```

**Passo 2 ‚Äî Dividir o texto:**
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)
```

**Passo 3 ‚Äî Gerar embeddings:**
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

**Passo 4 ‚Äî Criar o √≠ndice (Vector Store):**
```python
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    chunks,
    embeddings
)
```

> **NOTE:**  
> üìå Aqui nasce o √≠ndice.

### `5Ô∏è‚É£ Consultando o √≠ndice (busca sem√¢ntica)`

> Por√©m, ainda falta fazer a consulta sem√¢ntica nesse √≠ndice.

Para isso n√≥s vamos utilizar o conceito de **"Text Similarity"**:

```python
results = vectorstore.similarity_search(
    "How does indexing work?",
    k=3
)

for doc in results:
    print(doc.page_content)
```

### `6Ô∏è‚É£ Indexing vs Retrieval`

| Conceito    | Quando acontece | O que faz         |
| ----------- | --------------- | ----------------- |
| `Indexing`  | Antes do uso    | Cria o √≠ndice     |
| `Retrieval` | Em runtime      | Consulta o √≠ndice |

### `7Ô∏è‚É£ Persistindo e Carregando o √≠ndice`

> N√≥s tamb√©m precisamos persistir e carregar o √≠ndice de algum Banco de Dados.

**Persistindo o √≠ndice:**
```python
vectorstore.save_local("faiss_index")
```

**Carregando o √≠ndice:**
```python
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)
```





















---

<div id="chp02-pgvector"></div>

## `PGVector: Banco de Dados Vetorial com PostgreSQL no LangChain`

### `1Ô∏è‚É£ O que √© PGVector?`

PGVector √© uma extens√£o do PostgreSQL que permite armazenar e buscar embeddings (vetores) diretamente no banco de dados.

**üìå Em outras palavras:**

> PostgreSQL + vetores = Vector Store persistente e robusto

### `2Ô∏è‚É£ Por que PGVector entra nesse contexto?`

At√© agora n√≥s vimos:

```bash
DocumentLoader
‚Üí TextSplitter
‚Üí Embeddings
‚Üí VectorStore
```

O `PGVector` √© o `VectorStore` ‚Äî s√≥ que:

 - persistente
 - escal√°vel
 - SQL
 - pronto para produ√ß√£o

> **NOTE:**  
> üìå Diferente de FAISS (mem√≥ria/local), o PGVector vive em um banco real.

### `3Ô∏è‚É£ Criando um container docker com PGVector`

```bash
docker run \
    --name pgvector \
    --restart always \
    -e POSTGRES_USER=lcuser \
    -e POSTGRES_PASSWORD=lcpass \
    -e POSTGRES_DB=lcdb \
    -p 6024:5432 \
    -d pgvector/pgvector:pg16
```

 - `docker run`
   - Inicia um container Docker.
 - `--name pgvector`
   - Define o nome do container como pgvector.
   - Facilita: *"docker ps"*, *"docker stop pgvector"*
 - `--restart always`
   - Sempre inicia o container automaticamente.
 - `-e POSTGRES_USER=lcuser`
 - `-e POSTGRES_PASSWORD=lcpass`
 - `-e POSTGRES_DB=lcdb`
   - Vari√°veis de ambiente do PostgreSQL
   - Criam automaticamente:
     - `POSTGRES_USER` -> usu√°rio: langchain
     - `POSTGRES_PASSWORD` -> senha: langchain
     - `POSTGRES_DB` -> banco: langchain
 - `-p 6024:5432`
   - Mapeamento de porta
   - 5432 ‚Üí porta interna do PostgreSQL no container
   - 6024 ‚Üí porta acess√≠vel na sua m√°quina
   - Conex√£o externa:
     - *postgresql://langchain:langchain@localhost:6024/langchain*
 - `-d`
   - Roda o container em background.
 - `pgvector/pgvector:pg16`
   - Isso √©:
     - PostgreSQL 16
     - extens√£o pgvector j√° instalada
   - üìå Voc√™ n√£o precisa instalar nada manualmente. 

### `4Ô∏è‚É£ O que acontece quando esse container sobe?`

Internamente, o PostgreSQL:

 - inicia normalmente
 - ativa a extens√£o pgvector
 - fica pronto para criar colunas do tipo vector

Exemplo SQL real:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

### `5Ô∏è‚É£ Como o PGVector armazena embeddings?`

Cada embedding vira algo assim:

```sql
vector(1536)
```

Onde:

 - 1536 = dimens√£o do embedding (OpenAI small)

üìå Um registro t√≠pico:

```bash
id | content | embedding | metadata
```





















---

<div id="chp02-pgvector-exemplo"></div>

## `Exemplo Completo ‚Äî Usando PGVector com LangChain`

**üì¶ Pr√©-requisitos:**

 - Container do PGVector rodando (o Docker que voc√™ j√° subiu)
 - Vari√°vel de ambiente OPENAI_API_KEY configurada
 - Pacotes instalados:
   - `pip install langchain langchain-community langchain-openai psycopg2-binary`

Vamos come√ßar criando uma inst√¢ncia de `load_dotenv()` que vai ser respons√°vel por carregar as vari√°veis de ambiente:

[pgvector-01.py](codes/pgvector-01.py)
```python
from dotenv import load_dotenv

load_dotenv()
```

Continuando, agora vamos implementar a **String de Conex√£o com PostgreSQL (PGVector) no LangChain**:

[pgvector-01.py](codes/pgvector-01.py)
```python
CONNECTION_STRING = (
    "postgresql+psycopg2://"
    "lcuser:lcpass@localhost:6024/lcdb"
)
```

 - `postgresql`
   - Define o tipo de banco de dados.
   - O LangChain usa isso para saber:
     - que √© PostgreSQL
     - que suporta pgvector
 - `+psycopg2`
   - Define o driver Python usado para conectar ao banco.
   - Por qu√™?
     - PostgreSQL n√£o fala Python diretamente
     - o driver faz essa ponte
   - Biblioteca necess√°ria:
     - `pip install psycopg2-binary`
 - `://`
   - Separador padr√£o entre:
     - tipo de conex√£o
     - credenciais
 - `lcuser:lcpass`
   - Credenciais -> username:password
   - Isso vem diretamente do Docker:
     - `-e POSTGRES_USER=lcuser`
     - `-e POSTGRES_PASSWORD=lcpass`
 - `@localhost`
   - Define **onde o banco est√° rodando**.
   - localhost ‚Üí sua m√°quina
   - poderia ser:
     - IP
     - nome do container
     - hostname de produ√ß√£o
 -  `:6024`
   - Porta externa mapeada pelo Docker:
     - `-p 6024:5432`
     - üìå Importante:
       - 5432 ‚Üí porta interna do PostgreSQL
       - 6024 ‚Üí porta que voc√™ acessa
 - `/lcdb`
   - Nome do banco de dados.
   - Criado automaticamente pelo Docker:
     - `-e POSTGRES_DB=lcdb`

√ìtimo, agora vamos implementar um `loader de arquivos` de texto que vai ler um arquivo `.txt`:

[pgvector-01.py](codes/pgvector-01.py)
```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/example.txt")
```

> **O que tem na vari√°vel `loader`?**

```python
loader = TextLoader("data/example.txt")
print(loader)
print(type(loader))
```

**OUTPUT:**
```bash
<langchain_community.document_loaders.text.TextLoader object at 0x7fed2d540890>
<class 'langchain_community.document_loaders.text.TextLoader'>
```

Vejam que n√≥s temos apenas objetos da classe `langchain_community.document_loaders.text.TextLoader`.

> **E como eu acesso meus dados? (Meu texto em `data/example.txt`)?**

Para isso n√≥s precisamos utilizar o m√©todo `load()` da classe `TextLoader` que √© respons√°vel por ler o arquivo:

[pgvector-01.py](codes/pgvector-01.py)
```python
loader = TextLoader("data/example.txt")
text = loader.load()

print("\nType:", type(text))
print("\nContent:", text)
```

**OUTPUT:**
```bash
Type: <class 'list'>

Content: [Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.\n\nVector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.\n\nPGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nIndexing is the process of transforming raw documents into searchable vector representations.\n\nRetrieval is the process of querying those vectors to find the most relevant information for a user question.\n')]
```

√ìtimo, vejam que agora n√≥s temos:

 - **Um objeto lista:**
   - `<class 'list'>`
 - **Uma tupla `Document()`:**
   - Representando um √∫nico documento.
   - Essa tupla tem os seguintes campos:
     - `page_content`
     - `metadata`

Por exemplo, vamos ver o conte√∫do (`page_content`) e os metadados (`metadata`) do nosso documento (`Document`):

```python
for index, document in enumerate(text):
    print(f"\n------------ Document {index} ------------")
    print("\n[PAGE CONTENT]\n", document.page_content)
    print("\[nMETADATA]\n", document.metadata)
```

**OUTPUT:**
```bash
------------ Document 0 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.

PGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.

Using PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.

Indexing is the process of transforming raw documents into searchable vector representations.

Retrieval is the process of querying those vectors to find the most relevant information for a user question.


[METADATA]
 {'source': 'data/example.txt'}
```

Continuando na nossa implementa√ß√£o vamos dividir os dados do nosso arquivo `.txt` em 500 palavras (chunk_size=500) com 50 palavras de overlap (chunk_overlap=50):

[pgvector-01.py](codes/pgvector-01.py)
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter


splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
```

> **O que tem na vari√°vel `splitter`?**

```python
print(splitter)
print(type(splitter))
```

**OUTPUT:**
```bash
<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x751f76613170>
<class 'langchain_text_splitters.character.RecursiveCharacterTextSplitter'>
```

 - Novamente, temos objetos LangChain, `langchain_text_splitters.character.RecursiveCharacterTextSplitter` nesse caso.
 - Bem, se n√≥s temos objetos eles tem (ou podem ter) m√©todos e n√≥s podemos utilizar esses m√©todos.
 - A classe `RecursiveCharacterTextSplitter` tem o m√©todo `split_documents()` que divide os dados em chunks:

> **NOTE:**  
> üìå Lembrando que quando n√≥s criamos uma inst√¢ncia de `RecursiveCharacterTextSplitter()` n√≥s definimos a quantidade de peda√ßos (chunks) e a quantidade de overlap (chunk_overlap).

[pgvector-01.py](codes/pgvector-01.py)
```python
chunks = splitter.split_documents(text)
```

> **O que tem na vari√°vel `chunks`?**

```python
print("\nType:", type(chunks))
print("\nContent:", chunks)
```

**OUTPUT:**
```bash
Type: <class 'list'>

Content: [Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.'), Document(metadata={'source': 'data/example.txt'}, page_content='Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.\n\nPGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nIndexing is the process of transforming raw documents into searchable vector representations.'), Document(metadata={'source': 'data/example.txt'}, page_content='Retrieval is the process of querying those vectors to find the most relevant information for a user question.')]
```

> **U√©, √© a mesma coisa que o nosso `text = loader.load()` tem?**  
> N√£o, n√£o!

 - **Primeiro, se voc√™s prestarem aten√ß√£o ver√£o que n√≥s temos 3 objetos `Document()`:**
   - Lembram que n√≥s definimos 500 palavras por divis√£o (chunk_size)?
   - Ent√£o, cada `Document()` tem 500 palavras.

Por exemplo, vamos ver separadamente esses `Document()`:

```python
for index, document in enumerate(chunks):
    print(f"\n------------ Document {index} ------------")
    print("\n[PAGE CONTENT]\n", document.page_content)
    print("\n[METADATA]\n", document.metadata)
```

**OUTPUT:**
```bash
------------ Document 0 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 1 ------------

[PAGE CONTENT]
 Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.

PGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.

Using PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.

Indexing is the process of transforming raw documents into searchable vector representations.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 2 ------------

[PAGE CONTENT]
 Retrieval is the process of querying those vectors to find the most relevant information for a user question.

[METADATA]
 {'source': 'data/example.txt'}
```

**√ìtimo, entendendo tudo isso agora vamos transformar esses chunks em vetores:**  
Para isso, primeiro vamos importar e instanciar a classe `OpenAIEmbeddings`:

[pgvector-01.py](codes/pgvector-01.py)
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

Vejam que aqui n√≥s estamos utilizando o modelo pr√©-treinado `text-embedding-3-small` da OpenAI.

> **Agora √© s√≥ pegar nossos chunks (textos divididos) e transformar em vetores (embeddings)?**  
> N√£o nesse nosso exemplo!

**Lembram que n√≥s criamos um container com PostgreSQL que d√° suporte a pgvector?**  
Ent√£o, aqui vamos criar uma inst√¢ncia da classe `PGVector` que vai:

 - Receber os nossos chunks (textos divididos);
 - A instancia (classe respons√°vel) que vai gerar os embeddings;
 - Conex√£o com o banco de dados (string de conex√£o);
 - Um nome para a nossa cole√ß√£o:
   - Nome l√≥gico da *cole√ß√£o de vetores*.

[pgvector-01.py](codes/pgvector-01.py)
```python
from langchain_community.vectorstores import PGVector


vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)
```

**O que acontece internamente?**  
Quando esse c√≥digo roda, o *LangChain*:

```bash
1. Conecta no PostgreSQL
2. Cria tabelas se n√£o existirem
3. Para cada chunk:
      - l√™ page_content
      - gera embedding
      - salva vetor + metadata
4. Cria ou reutiliza a cole√ß√£o
```

> **O que temos na vari√°vel `vectorstore`?**

√â um objeto que sabe:

 - onde est√£o os vetores
 - como busc√°-los
 - como calcular similaridade

> **O que esse c√≥digo (at√© esse momento) N√ÉO faz?**

 - ‚ùå N√£o responde perguntas
 - ‚ùå N√£o chama LLM
 - ‚ùå N√£o faz RAG completo
 - **NOTE:** Ele apenas cria a mem√≥ria.

> **NOTE:**  
> üìå Novamente, se n√≥s temos uma inst√¢ncia de uma classe (`vectorstore`), essa inst√¢ncia tem (ou pode ter) um m√©todo.

Isso mesmo, aqui nossa inst√¢ncia `vectorstore` pode utilizar o m√©todo `similarity_search` para buscar vetores similares:

[pgvector-01.py](codes/pgvector-01.py)
```python
results = vectorstore.similarity_search(
    query="What is LangChain?",
    k=3
)
```

O c√≥digo acima faz uma **busca sem√¢ntica no √≠ndice vetorial** e **retorna os 3 textos mais relevantes**, com base no significado da pergunta.

 - üìå N√£o √© busca por palavra-chave
 - üìå N√£o √© SQL tradicional
 - üìå √â busca por similaridade de significado

> **O que temos na vari√°vel `query_results`?**

```python
query_results = vectorstore.similarity_search(
    query="What is LangChain?",
    k=3
)

print("\nType:", type(query_results))
print("\nContent:", query_results)
```

**OUTPUT:**
```bash
Type: <class 'list'>

Content: [Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.'), Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.'), Document(metadata={'source': 'data/example.txt'}, page_content='Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.\n\nPGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nIndexing is the process of transforming raw documents into searchable vector representations.')]
```

Se voc√™s prestarem bem aten√ß√£o ver√£o que n√≥s temos uma lista (`<class 'list'>`) com 3 Documents, por exemplo vamos exibir o `page_content` e `metadata` desses `Document()``, separadamente:

```python
query_results = vectorstore.similarity_search(
    query="What is LangChain?",
    k=3
)

for index, document in enumerate(query_results):
    print(f"\n------------ Document {index} ------------")
    print("\n[PAGE CONTENT]\n", document.page_content)
    print("\n[METADATA]\n", document.metadata)
```

**OUTPUT:**
```bash
------------ Document 0 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 1 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 2 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}
```










































































































<!--- ( Conversando com os seus dados ) --->

---

<div id="chp03-intro-to-rag"></div>

## `Introdu√ß√£o ao RAG (Retrieval-Augmented Generation)`

> **RAG (Retrieval-Augmented Generation)** √© uma t√©cnica utilizada para melhorar a precis√£o dos resultados gerados pelos LLMs, *"fornecendo contexto de fontes externas"*.

 - O termo foi originalmente cunhado num artigo de pesquisadores da Meta AI que descobriram que os modelos com RAG s√£o mais factuais e espec√≠ficos do que os modelos sem RAG:
   - [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
 - Sem o RAG, o LLM depende apenas dos seus dados pr√©-treinados, que podem estar desactualizados.

![img](images/intro-to-rag-01.png)  

Para entender melhor, imagine que n√≥s temos o seguinte modelo de LLM:

```python
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0
)
```

**Isso aqui √© APENAS o modelo:**

 - **‚úîÔ∏è Ele foi pr√©-treinado com:**
   - livros
   - artigos
   - c√≥digo p√∫blico
   - dados gerais (at√© uma data limite)
 - **‚úîÔ∏è Ele N√ÉO CONHECE:**
   - seu .txt
   - seu banco PostgreSQL
   - sua empresa
   - dados privados
   - dados atualizados
 - **‚úîÔ∏è Ele responde usando:**
   - probabilidade
   - padr√µes aprendidos
   - conhecimento geral

Por exemplo:

```python
llm.invoke("What is LangChain?")
```

 - ‚û°Ô∏è Resposta correta? **Provavelmente sim**
 - ‚û°Ô∏è Atualizada? **Talvez n√£o**
 - ‚û°Ô∏è Espec√≠fica do SEU projeto? **‚ùå N√ÉO**

**‚ùå Problema do modelo sozinho:**

 - Pode alucinar
 - Pode responder com informa√ß√µes desatualizadas
 - N√£o conhece dados privados
 - N√£o sabe o que voc√™ adicionou recentemente

### `Adicionando dados externos`

Como esse modelo recebe informa√ß√£o de uma fonte externa?

```python
loader = TextLoader("data/example.txt")
documents = loader.load()
```

Aqui, voc√™ est√° dizendo:

> ‚ÄúEsse conhecimento N√ÉO est√° no modelo.  
> Guarde isso fora e recupere quando eu precisar.‚Äù

### `üîë Frase para fixar (essa √© ouro)`

 - RAG n√£o faz o modelo saber mais.
 - RAG faz o modelo responder melhor com dados que ele n√£o conhecia.





















---

<div id="chp03-three-steps"></div>

## `Tr√™s fases de um sistema RAG`

Um sistema **RAG (Retrieval-Augmented Generation)** para uma aplica√ß√£o de IA segue normalmente tr√™s fases principais:

 - **Indexa√ß√£o (Indexing):**
   - Esta fase do envolve o pr√©-processamento da fonte de dados externa e o armazenamento de embeddings que representam os dados num vetor onde podem ser facilmente recuperados.
 - **Recupera√ß√£o (Retrieval):**
   - Esta fase envolve a recupera√ß√£o dos dados e dos embeddings relevantes armazenados no vetor com base na consulta de um utilizador.
 - **Gera√ß√£o (Generation):**
   - Esta fase envolve a s√≠ntese do prompt original com os documentos relevantes recuperados como um prompt final enviado ao modelo para uma previs√£o.

Vejamos a imagem abaixo para ficar mais claro:

![img](images/rag-three-steps-01.png)  





















---

<div id="chp03-full-example"></div>

## `Exemplo Completo: Construindo um RAG com LangChain + PGVector`

**üéØ Objetivo final:**

> Criar um sistema que busca informa√ß√µes em documentos e gera respostas usando um LLM, baseado apenas nesses documentos.

O fluxo que n√≥s vamos seguir vai ser o seguinte:

```bash
Documentos
 ‚Üí Loader
 ‚Üí Splitter (chunk + overlap)
 ‚Üí Embeddings
 ‚Üí Vector Store (PGVector)
 ‚Üí Retriever
 ‚Üí Prompt com contexto
 ‚Üí LLM
 ‚Üí Resposta (RAG)
```

Vamos come√ßar fazendo o nosso c√≥digo reconhecer as vari√°veis de ambiente:

**1Ô∏è‚É£ Carregando vari√°veis de ambiente:** [rag-example-01.py](codes/rag-example-01.py)
```python
from dotenv import load_dotenv
load_dotenv()
```

 - **Por qu√™?**
   - Carrega `OPENAI_API_KEY`
   - Evita hardcode de segredos
   - Essencial para produ√ß√£o

Continuando, agora vamos escrever a string de conex√£o com o Banco de Dados:

**2Ô∏è‚É£ String de conex√£o com o PostgreSQL (PGVector):** [rag-example-01.py](codes/rag-example-01.py)
```python
CONNECTION_STRING = (
    "postgresql+psycopg2://"
    "lcuser:lcpass@localhost:6024/lcdb"
)
```

O que isso define:

 - Usu√°rio: lcuser
 - Senha: lcpass
 - Host: localhost
 - Porta: 6024
 - Banco: lcdb
 - **NOTE:** üìå √â aqui que os embeddings ficar√£o persistidos.

Agora, vamos iniciar o nosso **"Loading (Ingestion)"**, ou seja, carregar *dados externos*:

**3Ô∏è‚É£ Carregando documentos (Document Loader):** [rag-example-01.py](codes/rag-example-01.py)
```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/example.txt")
documents = loader.load()
```

**O que sai daqui?**

 - `List[Document]`
   - Cada Document cont√©m:
     - *page_content* ‚Üí texto
     - *metadata* ‚Üí origem, caminho, etc.

Agora, n√≥s vamos para a etapa de **Splitting (Chunking + Overlap)** quebrar o texto em peda√ßos menores:

**4Ô∏è‚É£ Dividindo o texto (Chunking + Overlap):** [rag-example-01.py](codes/rag-example-01.py)
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)
```

**Por que isso √© obrigat√≥rio?**

 - LLMs t√™m limite de tokens
 - Embeddings funcionam melhor com texto curto
 - Overlap preserva contexto

Continuando, agora n√≥s vamos criar uma inst√¢ncia de um modelo pr√©-treinado que ser√° respons√°vel por transformar nossos documentos em *vetores (embeddings)*:

**5Ô∏è‚É£ Instanciando o modelo de embeddings:** [rag-example-01.py](codes/rag-example-01.py)
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

**O que isso far√°?**

 - Converte texto ‚Üí vetor num√©rico
 - Captura significado sem√¢ntico
 - Base da busca por similaridade
 - **NOTE:** *üìå Isso n√£o gera texto, apenas n√∫meros.*

Agora, n√≥s vamos criar o famoso **Indexing (Embedding + Vector Store)**. Essa etapa envolve:

 - Transformar cada *chunk* em *vetores (embeddings)*
 - Armazenar esses vetores em um *Vector Store*

**6Ô∏è‚É£ Criando o √≠ndice vetorial (PGVector):** [rag-example-01.py](codes/rag-example-01.py)
```python
from langchain_community.vectorstores import PGVector

vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)
```

> **NOTE:**  
> üìå ISSO √© indexa√ß√£o em RAG

**O que acontece aqui?**

 - Conecta no PostgreSQL
 - Cria tabelas
 - Gera embeddings
 - Salva vetores + metadata
 - **NOTE:** üìå Aqui nasce a mem√≥ria do seu RAG.

Agora, n√≥s vamos criar o nosso `retriever`:

**7Ô∏è‚É£ Criando o Retriever (ponte entre √≠ndice e pergunta):** [rag-example-01.py](codes/rag-example-01.py)
```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)
```

**O que √© um Retriever?**

 - Recebe uma pergunta
 - Busca os documentos mais relevantes
 - Retorna `List[Document]`
 - **NOTE:** üìå Ele n√£o gera resposta, apenas recupera contexto.

Continuando, agora n√≥s vamos criar um **modelo (template) de prompt** que vai nos auxiliar a adicionar esse contexto a pergunta recebida:

**8Ô∏è‚É£ Criando o Prompt de RAG:** [rag-example-01.py](codes/rag-example-01.py)
```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("""
Answer the question using ONLY the context below.

Context:
{context}

Question:
{question}

If the answer is not in the context, say "I don't know".
""")
```

**Por que isso √© importante?**

 - Evita alucina√ß√£o
 - For√ßa o LLM a usar o contexto
 - D√° previsibilidade √†s respostas

Agora, n√≥s vamos criar uma inst√¢ncia de algum modelo pr√©-treinado do ChatGPT que ser√° respons√°vel por nos responder as perguntas (com ajuda do contexto):

**9Ô∏è‚É£ Instanciando o LLM (Chat Model):** [rag-example-01.py](codes/rag-example-01.py)
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0
)
```

**Por que `temperature=0`?**

 - Respostas mais factuais
 - Menos criatividade
 - Ideal para RAG

Agora, n√≥s vamos criar uma pergunta simples (mas poderia ser uma pergunta de algum sistema, agente ou usu√°rio):

**üîü Criando a pergunta:** [rag-example-01.py](codes/rag-example-01.py)
```python
question = "What is LangChain and PGVector?"
```

> **Ent√£o, essa pergunta vai para o nosso modelo (inst√¢ncia llm) n√£o √©?**

N√£o, na verdade primeiro essa pergunta vai ser feita nos nossos dados externos ao modelo "llm", ou seja, no nosso contexto:

> **Em termos simples:**
> √â aqui que o RAG *‚Äúbusca conhecimento fora do modelo‚Äù*.

**1Ô∏è‚É£1Ô∏è‚É£ Recuperando o contexto:** [rag-example-01.py](codes/rag-example-01.py)
```python
docs = retriever.invoke(question)
```

**O que `docs` cont√©m?**

 - `List[Document]`
   - Ainda n√£o √© texto utiliz√°vel pelo LLM.

Agora, n√≥s vamos manipular esse `docs (List[Document])` para ser um texto que possa ser lido pelo a nossa inst√¢ncia "llm" ‚Äî **Esse trecho faz a etapa de "Augmentation (Context Construction) do RAG"**:

**1Ô∏è‚É£2Ô∏è‚É£ Transformando documentos em contexto:** [rag-example-01.py](codes/rag-example-01.py)
```python
context = "\n\n".join(
    doc.page_content for doc in docs
)
```

Continuando, agora n√≥s vamos criar um `chain`:

**1Ô∏è‚É£3Ô∏è‚É£ Criando a Chain (Prompt ‚Üí LLM):** [rag-example-01.py](codes/rag-example-01.py)
```python
chain = prompt | llm
```

Aqui, n√≥s estamos criando uma `chain` conectando:

 - um `PromptTemplate` (prompt)
 - a um `LLM` (llm)
 - **NOTE:** Usando o operador `| (pipe)`, o LangChain monta um pipeline execut√°vel.

> **Em termos simples:**  
> *‚ÄúPegue a sa√≠da do prompt e passe diretamente como entrada para o modelo.‚Äù*

Agora n√≥s vamos converter o nosso prompt (ChatPromptTemplate) em mensagens prontas para um ChatModel (nossa inst√¢ncia "llm"):

**1Ô∏è‚É£4Ô∏è‚É£ Convertendo o Prompt em uma Mensagem:** [rag-example-01.py](codes/rag-example-01.py)
```python
messages = prompt.format_messages(
    context=context,
    question=question
)
```

Vejam que n√≥s passamos:

 - Nosso contexto
 - E a pergunta

Internamente, o LangChain vai na nossa vari√°vel `prompt`, que cont√©m um modelo de prompt, e vai:

 - Substitui `{context}` pelo *texto recuperado*
 - Substitui `{question}` pela *pergunta (do usu√°rio, agent, etc.)*

Por exemplo, vamos analisar a vari√°vel `messages`:

**1Ô∏è‚É£5Ô∏è‚É£ Analisando a Mensagem:** [rag-example-01.py](codes/rag-example-01.py)
```python
messages = prompt.format_messages(
    context=context,
    question=question
)

print("\ntype:", type(messages))
print("\nmessages:", messages)
```

**OUTPUT:**
```bash
type: <class 'list'>

messages: [HumanMessage(content='\nAnswer the question using ONLY the context below.\n\nContext:\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nQuestion:\nWhat is LangChain and PGVector?\n\nIf the answer is not in the context, say "I don\'t know".\n', additional_kwargs={}, response_metadata={})]
```

Vejam que:

 - N√≥s temos uma lista: `<class 'list'>`
 - Temos uma mensagem que tem o atributo `content` com o nosso prompt otimizado

Por exemplo, vamos ver s√≥ o nosso `content`:

**1Ô∏è‚É£6Ô∏è‚É£ Analisando o ConteuÃÅdo da Mensagem:** [rag-example-01.py](codes/rag-example-01.py)
```python
messages = prompt.format_messages(
    context=context,
    question=question
)

print(messages[0].content)
```

**OUTPUT:**
```bash
Answer the question using ONLY the context below.

Context:
Using PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.

Using PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.

Using PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.

Question:
What is LangChain and PGVector?

If the answer is not in the context, say "I don't know".
```

√â esse prompt que o nosso modelo vai receber.

> **Mas, por que o `Context` est√° recebendo tr√™s respostas iguais?**

Por que n√≥s utilizamos uma pesquisa de similitudes para encontrar os documentos mais relevantes:

 - Como foi `k=3`, vamos receber 3 respostas
 - Eles, s√£o iguais por acaso, mas poderiam ser diferentes

√ìtimo, agora que n√≥s j√° temos uma mensagem (prompt) otimizada que pode ser utilizada no ChatModel, vamos chamar o m√©todo `invoke` da nossa inst√¢ncia `llm` e passar como argumento essa mensagem:

** Fazendo a pergunta ao nosso modelo:** [rag-example-01.py](codes/rag-example-01.py)
```python
answer = llm.invoke(messages)
print(answer.content)
```

**OUTPUT:**
```bash
LangChain is a tool that allows developers to build production-ready RAG systems with persistence, scalability, and SQL support. PGVector is a component used in conjunction with LangChain for this purpose.
```

> **NOTE:**  
> üéâ √ìtimo, agora n√≥s temos um RAG funcional.





















---

<div id="chp03-chain"></div>

## `Criando um "chain" de perguntas e respostas`

Lembram que n√≥s tinhamos um passo que n√≥s substituiamos as vari√°veis do nosso template pelo `contexto` e `pergunta`?

[rag-example-01.py](codes/rag-example-01.py)
```python
messages = prompt.format_messages(
    context=context,
    question=question
)
```

Ent√£o, existe uma maneira mais idiom√°tica muito utilizada para fazer isso:

[chain-01.py](codes/chain-01.py)
```python
chain = prompt | llm
```

 - **O que √© esse `|` no LangChain?**
   - No LangChain, o operador `|` n√£o √© o *‚ÄúOU‚Äù* do Python comum.
   - Ele foi sobrecargado para significar:
     - *‚ÄúPegue a sa√≠da do objeto da esquerda e passe como entrada para o objeto da direita‚Äù*.
   - **NOTE:** Tecnicamente, isso cria um RunnableSequence.

√ìtimo, como n√≥s passamos o nosso prompt para a nossa inst√¢ncia `llm` (usando o operador `|`) e salvamos na vari√°vel chain, agora n√≥s podemos utilizar o m√©todo `invoke()` a partir da vari√°vel `chain` para fazer a pergunta ao nosso modelo:

[chain-02.py](codes/chain-02.py)
```python
answer = chain.invoke({
    "context": context,
    "question": question
})
```

Vejam que n√≥s passamos como argumento:

 - **Um dicion√°rio `{}`**
 - **Que tem os campos:chaves:**
   - `context`
   - `question`

Por fim, √© s√≥ pegar essa resposta (`answer`) e imprimir:

[chain-03.py](codes/chain-03.py)
```python
print(answer.content)
```

**OUTPUT:**
```bash
LangChain is a tool that allows developers to build production-ready RAG systems with persistence, scalability, and SQL support. PGVector is a component used in conjunction with LangChain for this purpose.
```





















---

<div id="chp03-rag-runnable"></div>

## `Criando um RAG "runnable"`

At√© ent√£o, n√≥s tinhamos o seguinte c√≥digo RAG:

[chain-01.py](codes/chain-01.py)
```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv


load_dotenv()


CONNECTION_STRING = (
    "postgresql+psycopg2://"
    "lcuser:lcpass@localhost:6024/lcdb"
)

loader = TextLoader("data/example.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

prompt = ChatPromptTemplate.from_template("""
Answer the question using ONLY the context below.

Context:
{context}

Question:
{question}

If the answer is not in the context, say "I don't know".
""")

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0
)

question = "What is LangChain and PGVector?"

docs = retriever.invoke(question)

context = "\n\n".join(
    doc.page_content for doc in docs
)

chain = prompt | llm

answer = chain.invoke({
    "context": context,
    "question": question
})

print(answer.content)
```

### `üß† O que significa ‚ÄúRAG 100% com Runnable‚Äù?`

No LangChain moderno:

 - ‚ùå N√£o ficar chamando `.invoke()` manualmente em cada parte
 - ‚ùå N√£o montar contexto *‚Äúna m√£o‚Äù*
 - ‚úÖ Tudo vira `Runnable`
 - ‚úÖ O fluxo inteiro vira um *pipeline* √∫nico
 - ‚úÖ Um √∫nico `.invoke()` dispara tudo

Isso √© o que permite:

 - tracing
 - streaming
 - paralelismo
 - composi√ß√£o
 - produ√ß√£o

### `üì¶ Passo 1 ‚Äî Entender o pipeline RAG como Runnables`

Um RAG completo tem 4 est√°gios:

```bash
Pergunta
  ‚Üì
Retriever (Runnable)
  ‚Üì
Formatter de contexto (Runnable)
  ‚Üì
Prompt (Runnable)
  ‚Üì
LLM (Runnable)
```

> **NOTE:**  
> üëâ Tudo isso pode ser encadeado com `|`.

### `üìÑ Passo 2 ‚Äî Loader, Splitter e VectorStore (pr√©-processamento)`

**‚ö†Ô∏è Essas etapas N√ÉO precisam ser Runnable, porque:**

 - Elas rodam uma vez
 - S√£o prepara√ß√£o de dados
 - N√£o fazem parte da infer√™ncia

Seu c√≥digo aqui j√° est√° perfeito e permanece igual:

```python
loader = TextLoader("data/example.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)
```

### `üîç Passo 3 ‚Äî Retriever como Runnable`

Aqui come√ßa o RAG de verdade.

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)
```

Important√≠ssimo:

 - retriever j√° √© um `Runnable`
 - Ele recebe uma str (pergunta)
 - Ele retorna `List[Document]`

### `üß± Passo 4 ‚Äî Criar um Runnable para formatar o contexto`

> **O LLM n√£o entende Document.**  
> Ele entende texto!

Ent√£o precisamos de um `Runnable` intermedi√°rio:

```python
from langchain_core.runnables import RunnableLambda

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

format_context = RunnableLambda(format_docs)
```

Isso transforma:

 - `List[Document]` ‚Üí `str`

### `üìù Passo 5 ‚Äî Prompt como Runnable`

N√≥s j√° temos isso corretamente:

```python
prompt = ChatPromptTemplate.from_template("""
Answer the question using ONLY the context below.

Context:
{context}

Question:
{question}

If the answer is not in the context, say "I don't know".
""")
```

### `ü§ñ Passo 6 ‚Äî LLM como Runnable`

Aqui n√≥s tamb√©m j√° temos isso corretamente:

```python
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0
)
```

### `üîó Passo 7 ‚Äî Montar o RAG 100% Runnable`

Agora vem a parte mais importante.

**üéØ Queremos isso:**

 - Pergunta entra
 - Retriever busca contexto
 - Contexto vai para o prompt
 - Prompt vai para o LLM

```python
from langchain_core.runnables import RunnablePassthrough

rag_chain = (
    {
        "context": retriever | format_context,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
)
```

**O que est√° acontecendo aqui?**  
| Parte                   | Fun√ß√£o                   |
| ----------------------- | ------------------------ |
| `RunnablePassthrough()` | passa a pergunta adiante |
| `retriever`             | busca docs               |
| `format_context`        | transforma docs em texto |
| `{}`                    | cria inputs nomeados     |
| `prompt`                | monta mensagens          |
| `llm`                   | gera resposta            |


 - üìå Nenhuma vari√°vel solta
 - üìå Nenhuma montagem manual
 - üìå Tudo declarativo

### `‚ñ∂Ô∏è Passo 8 ‚Äî Executar o RAG (1 linha)`

Por fim, n√≥s s√≥ precisamos executar o RAG:

```python
question = "What is LangChain and PGVector?"

response = rag_chain.invoke(question)

print(response.content)
```

### `‚úÖ C√≥digo final COMPLETO (RAG 100% Runnable)`

[runnable-01.py](codes/runnable-01.py)
```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import PGVector
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from dotenv import load_dotenv

load_dotenv()

CONNECTION_STRING = (
    "postgresql+psycopg2://"
    "lcuser:lcpass@localhost:6024/lcdb"
)

# ----------------------------
# Indexa√ß√£o (offline / setup)
# ----------------------------

loader = TextLoader("../../data/history_of_computing.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)

# ----------------------------
# RAG (online / runtime)
# ----------------------------

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

format_context = RunnableLambda(format_docs)

prompt = ChatPromptTemplate.from_template("""
Answer the question using ONLY the context below.

Context:
{context}

Question:
{question}

If the answer is not in the context, say "I don't know".
""")

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0
)

rag_chain = (
    {
        "context": retriever | format_context,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
)

question = "What's computer father?"

response = rag_chain.invoke(question)

print(response.content)
```

**OUTPUT:**
```bash
LangChain is a tool that allows developers to build production-ready RAG systems with persistence, scalability, and SQL support. PGVector is a component used in conjunction with LangChain for this purpose.
```





















---

<div id="chp03-source-category"></div>

## `Criando RAG que utilizam "source", "category" nos resultados`

Aqui nesse tutorial n√≥s vamos praticar o seguinte:

 - Adicionar metadata (`source`, `category`) aos documentos
 - Salvar isso no PGVector
 - Filtrar documentos no momento da recupera√ß√£o
 - Continuar usando RAG 100% com Runnable
 - Entender conceitualmente o porqu√™ disso existir

### `üß† Por que filtros por metadata existem em RAG?`

**Sem metadata, o RAG responde assim:**

 > ‚ÄúProcure qualquer coisa parecida com a pergunta‚Äù

**Com metadata, o RAG responde assim:**

> *‚ÄúProcure apenas em documentos dessa fonte, dessa categoria, desse tipo‚Äù*

Isso √© essencial quando voc√™ tem:

 - m√∫ltiplos arquivos
 - m√∫ltiplas √°reas (docs, suporte, jur√≠dico, marketing)
 - m√∫ltiplas vers√µes
 - m√∫ltiplos clientes (multi-tenant)

### `üóÇÔ∏è Parte 1 ‚Äî Exemplos de arquivos com metadata`

Vamos criar 3 arquivos reais, cada um com um contexto diferente.

[data/langchain.txt](codes/data/langchain.txt)
```txt
LangChain is a framework for building applications with large language models.

It provides abstractions for prompts, chains, retrievers, memory, and agents.

LangChain is commonly used to build RAG systems, chatbots, and AI assistants.
```

Metadata desejada:

 - `source = "documentation"`
 - `category = "langchain"`

[data/pgvector.txt](codes/data/pgvector.txt)
```txt
PGVector is a PostgreSQL extension that enables vector similarity search.

It allows storing embeddings inside PostgreSQL and querying them efficiently.

PGVector is often used as a vector database backend for RAG systems.
```

Metadata desejada:

 - `source = "documentation"`
 - `category = "pgvector"`

[data/history.txt](codes/data/history.txt)
```txt
Ancient Greek philosophy includes figures such as Socrates, Plato, and Aristotle.

These philosophers laid the foundations of Western philosophy.
```

Metadata desejada:

 - `source = "history_book"`
 - `category = "philosophy"`

### `üß± Parte 2 ‚Äî Carregando documentos COM metadata`

O `TextLoader` aceita metadata manualmente.

**üîß Antes (nosso c√≥digo atual):** [chain-01.py](codes/chain-01.py)
```python
documents = loader.load()
```

**‚úÖ Depois (com metadata):** [rag-source-category.py](codes/rag-source-category.py)
```python
from langchain_core.documents import Document

documents = [
    Document(
        page_content=open("data/langchain.txt").read(),
        metadata={"source": "documentation", "category": "langchain"}
    ),
    Document(
        page_content=open("data/pgvector.txt").read(),
        metadata={"source": "documentation", "category": "pgvector"}
    ),
    Document(
        page_content=open("data/history.txt").read(),
        metadata={"source": "history_book", "category": "philosophy"}
    ),
]
```

> **NOTE:**  
> üìå Agora cada *chunk* herda essa metadata automaticamente.

### `‚úÇÔ∏è Parte 3 ‚Äî Splitter preserva metadata (importante!)`

Agora n√≥s vamos utilizar o `RecursiveCharacterTextSplitter` para dividir o texto em peda√ßos menores (chunks + overlap):

[rag-source-category.py](codes/rag-source-category.py)
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)
```

Aqui, cada chunk mant√©m:

 - `page_content`
 - `metadata.source`
 - `metadata.category`

### `üß† Parte 4 ‚Äî Indexa√ß√£o no PGVector (nada muda!)`

[rag-source-category.py](codes/rag-source-category.py)
```python
from langchain_postgres import PGVector
from langchain_openai import OpenAIEmbeddings


embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)
```

O PGVector agora armazena:

 - vetor
 - texto
 - metadata (JSONB)

### `üîç Parte 5 ‚Äî Recupera√ß√£o COM filtros por metadata`

Agora vem a parte que n√≥s queremos:

**üéØ Exemplo 1 ‚Äî Buscar SOMENTE LangChain:** [rag-source-category.py](codes/rag-source-category.py)
```python
langchain_retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 3,
        "filter": {
            "category": "langchain"
        }
    }
)
```

 - **üìå O RAG:**
   - ‚ùå ignora PGVector
   - ‚ùå ignora hist√≥ria
   - ‚úÖ busca s√≥ LangChain


**üéØ Exemplo 2 ‚Äî Buscar SOMENTE documenta√ß√£o t√©cnica:** [rag-source-category.py](codes/rag-source-category.py)
```python
docs_retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 3,
        "filter": {
            "source": "documentation"
        }
    }
)
```

**üéØ Exemplo 3 ‚Äî Filtro combinado (AND):** [rag-source-category.py](codes/rag-source-category.py)
```python
combined_retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 3,
        "filter": {
            "source": "documentation",
            "category": "pgvector"
        }
    }
)
```

 - Apenas documentos de PGVector
 - Apenas da documenta√ß√£o

O resto do nosso c√≥digo muda poucas coisas, ele completo ficar√° assim:

[rag-source-category.py](codes/rag-source-category.py)
```python
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_postgres import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from dotenv import load_dotenv

load_dotenv()

CONNECTION_STRING = (
    "postgresql+psycopg2://"
    "lcuser:lcpass@localhost:6024/lcdb"
)

documents = [
    Document(
        page_content=open("data/langchain.txt").read(),
        metadata={"source": "documentation", "category": "langchain"}
    ),
    Document(
        page_content=open("data/pgvector.txt").read(),
        metadata={"source": "documentation", "category": "pgvector"}
    ),
    Document(
        page_content=open("data/history.txt").read(),
        metadata={"source": "history_book", "category": "philosophy"}
    ),
]

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection=CONNECTION_STRING,  # ‚úÖ AQUI
    collection_name="ex01_documents"
)

retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 3,
        "filter": {
            "category": "pgvector"
        }
    }
)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

prompt = ChatPromptTemplate.from_template("""
Answer the question using ONLY the context below.

Context:
{context}

Question:
{question}

If the answer is not in the context, say "I don't know".
""")

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0
)

rag_chain = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
)

response = rag_chain.invoke("What is PGVector used for?")
print(response.content)
```





















---

<div id="rag-conversational-with-memory"></div>

## `RAG conversacional com mem√≥ria`

Um RAG normal tem o seguinte fluxo:

```bash
Pergunta ‚Üí Retriever ‚Üí Contexto ‚Üí LLM ‚Üí Resposta
```

**‚ö†Ô∏è Problema:**

 - Cada pergunta √© isolada
 - O modelo n√£o lembra perguntas anteriores
 - Perguntas como:
   - "E ele suporta PostgreSQL?"
   - **NOTE:** N√£o fazem sentido sem hist√≥rico.

### `‚úÖ RAG Conversacional resolve isso`

Ele adiciona mem√≥ria de conversa, ficando assim:

```bash
Hist√≥rico da conversa
        ‚Üì
Pergunta atual
        ‚Üì
Retriever (considera o hist√≥rico)
        ‚Üì
Contexto + Hist√≥rico
        ‚Üì
LLM
```

Agora o modelo entende:

 - Refer√™ncias (‚Äúele‚Äù, ‚Äúisso‚Äù, ‚Äúaquilo‚Äù)
 - Continuidade de racioc√≠nio
 - Perguntas de follow-up

## `Tipos de mem√≥ria no LangChain`

Existem v√°rios tipos de mem√≥ria, mas para RAG usamos principalmente:

 - **üß† Conversation Buffer Memory:**
   - Guarda toda a conversa
   - Simples e ideal para aprendizado
 - **üß† Conversation Summary Memory:**
   - Resume o hist√≥rico
   - Melhor para conversas longas

### `Onde a mem√≥ria entra no RAG?`

Existem dois pontos poss√≠veis:

 - **Op√ß√£o A ‚Äî Mem√≥ria s√≥ no prompt:**
   - Hist√≥rico entra junto com contexto
   - Mais simples
 - **Op√ß√£o B ‚Äî Mem√≥ria influencia o retriever:**
   - Hist√≥rico altera a busca vetorial
   - Mais avan√ßado

### `Estrutura do Prompt Conversacional`

Agora o prompt precisa de 3 coisas:

 - Hist√≥rico da conversa
 - Contexto recuperado
 - Pergunta atual

```bash
Conversation history:
{chat_history}

Context:
{context}

Question:
{question}
```










































































































<!--- ( Pr√©-Processamento e Valida√ß√£o ) --->

---

<div id="rag-evaluation"></div>

## `Como um RAG pode ser avaliado`

> Aqui, n√≥s vamos debater como um RAG pode ser avaliado.

### `1Ô∏è‚É£ O que √© ‚ÄúAvalia√ß√£o de RAG‚Äù?`

Avaliar um RAG n√£o √© perguntar:

> ‚ÄúA resposta parece boa?‚Äù

Avaliar um RAG √© responder perguntas como:

 - ‚ùì A resposta usa o contexto recuperado?
 - ‚ùì A resposta alucina?
 - ‚ùì A resposta ignora informa√ß√µes relevantes?
 - ‚ùì O retriever trouxe bons documentos?
 - ‚ùì O LLM respondeu de forma fiel ao contexto?

Em RAG, o erro pode estar em 3 lugares:

 - Recupera√ß√£o (retriever)
 - Contexto (prompt / formata√ß√£o)
 - Gera√ß√£o (LLM)

### `2Ô∏è‚É£ Dimens√µes cl√°ssicas de avalia√ß√£o de RAG`

Vamos focar nas 4 mais importantes (as que o mercado usa).

**‚úÖ 1. Relevance (Relev√¢ncia):**

> Os documentos recuperados s√£o relevantes para a pergunta?

Exemplo ruim:

 - *Pergunta:* What is LangChain?
 - *Contexto:* texto sobre filosofia grega

> **NOTE:**  
> ‚ùå Falha do retriever

**‚úÖ 2. Faithfulness (Fidelidade):**

> A resposta √© baseada apenas no contexto?

Exemplo ruim:

 - Contexto n√£o fala de vers√µes
 - Resposta inventa vers√£o do LangChain

> **NOTE:**  
> ‚ùå Alucina√ß√£o do LLM

**‚úÖ 3. Answer Correctness (Corre√ß√£o)**

> A resposta est√° correta dentro do contexto fornecido?

Importante:

 - N√£o √© ‚Äúverdade universal‚Äù
 - √â ‚Äúverdade segundo o contexto‚Äù

**‚úÖ 4. Context Coverage (Cobertura)**

> O contexto recuperado cont√©m informa√ß√£o suficiente para responder?

 - ‚ùå Se n√£o cont√©m ‚Üí problema do retriever ou chunking

### `3Ô∏è‚É£ Avalia√ß√£o manual (conceito)`

Antes de automatizar, entenda o processo humano.

**Pergunta:**
```bash
What is LangChain and PGVector?
```

**Contexto recuperado:**
```bash
LangChain is a framework...
PGVector is a PostgreSQL extension...
```

**Resposta do LLM:**
```bash
LangChain is a framework for LLM applications...
PGVector is used to store embeddings in PostgreSQL...
```

 - ‚úîÔ∏è Usa contexto
 - ‚úîÔ∏è N√£o inventa
 - ‚úîÔ∏è Responde corretamente

> **NOTE:**  
> üëâ RAG aprovado

### `4Ô∏è‚É£ Avalia√ß√£o autom√°tica: a ideia`

Vamos usar o pr√≥prio LLM como avaliador.

> **NOTE:**  
> üìå Isso √© chamado de `LLM-as-a-Judge`.

Ele recebe:

 - pergunta
 - contexto
 - resposta gerada

E avalia crit√©rios objetivos.

### `5Ô∏è‚É£ Criando um Prompt de Avalia√ß√£o`

Aqui, vamos criar um segundo prompt, s√≥ para avalia√ß√£o:

```python
evaluation_prompt = ChatPromptTemplate.from_template("""
You are evaluating a RAG system.

Question:
{question}

Context:
{context}

Answer:
{answer}

Evaluate the answer based on the following criteria:
1. Faithfulness (is the answer grounded in the context?)
2. Relevance (does it address the question?)
3. Completeness (does it fully answer the question?)

Give a score from 1 to 5 for each criterion and explain briefly.
""")
```

> **NOTE:**  
> üìå Esse prompt **n√£o responde a pergunta**, ele **julga a resposta**.

### `6Ô∏è‚É£ Fluxo completo com Avalia√ß√£o`

O fluxo para avaliar o nosso RAG ficaria assim:

```bash
Pergunta
  ‚Üì
Retriever
  ‚Üì
Contexto
  ‚Üì
LLM responde
  ‚Üì
LLM avalia a resposta
```

### `7Ô∏è‚É£ C√≥digo COMPLETO (RAG + Avalia√ß√£o)`

O c√≥digo completo com avalia√ß√£o ficaria assim:

[rag-evaluation-01.py](codes/rag-evaluation-01.py)
```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import PGVector
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from dotenv import load_dotenv

load_dotenv()

CONNECTION_STRING = (
    "postgresql+psycopg2://"
    "lcuser:lcpass@localhost:6024/lcdb"
)

# ----------------------------
# Indexa√ß√£o
# ----------------------------

loader = TextLoader("data/example.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)

# ----------------------------
# RAG
# ----------------------------

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

prompt = ChatPromptTemplate.from_template("""
Answer the question using ONLY the context below.

Context:
{context}

Question:
{question}

If the answer is not in the context, say "I don't know".
""")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

rag_chain = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
)

question = "What is LangChain and PGVector?"

docs = retriever.invoke(question)
context = format_docs(docs)

answer = rag_chain.invoke(question)

# ----------------------------
# Avalia√ß√£o do RAG
# ----------------------------

evaluation_prompt = ChatPromptTemplate.from_template("""
You are evaluating a RAG system.

Question:
{question}

Context:
{context}

Answer:
{answer}

Evaluate the answer based on:
1. Faithfulness
2. Relevance
3. Completeness

Score each from 1 to 5 and explain briefly.
""")

evaluation_chain = evaluation_prompt | llm

evaluation = evaluation_chain.invoke({
    "question": question,
    "context": context,
    "answer": answer.content
})

print("ANSWER:\n", answer.content)
print("\nEVALUATION:\n", evaluation.content)
```

**OUTPUT:**
```bash
ANSWER:
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). PGVector is used with LangChain to build production-ready RAG systems with persistence, scalability, and SQL support.

EVALUATION:
 1. Faithfulness: 5 - The answer accurately describes LangChain as a framework for applications powered by large language models and PGVector as a tool used with LangChain for building RAG systems.
2. Relevance: 5 - The answer directly addresses the question by providing information on both LangChain and PGVector and how they are used together.
3. Completeness: 5 - The answer provides a clear explanation of both LangChain and PGVector, including their purposes and how they are used in building RAG systems.

Overall, the answer is comprehensive, relevant, and faithful to the information provided in the context.
```













































































































<!--- ( Dicas & Truques ) --->

---

<div id="chp03-prompting-vs-fine-tuning-vs-rag"></div>

## `Prompting vs. Fine Tuning vs. RAG`

| T√©cnica       | Modifica o modelo?  | Usa dados externos? |
| ------------- | ------------------- | ------------------- |
| `Prompting`   | ‚ùå N√£o              | ‚ùå N√£o             |
| `Fine-tuning` | ‚úÖ Sim              | ‚ùå N√£o             |
| `RAG`         | ‚ùå N√£o              | ‚úÖ Sim             |




















<!--- ( Configura√ß√µes ) --->

---

<div id="create-env"></div>

## `Criando o ambiente virtual`

**NOTE:**  
Antes de criar o ambiente virtual, crie um arquivo chamado [.env](.env) e depois insira a sua chave de API da OpenAI:

[.env](.env)
```bash
OPENAI_API_KEY=your_api_key
```

Continuando...

**CRIA O AMBIENTE VIRTUAL:**  
```bash
python3 -m venv environment
```

**ATIVA O AMBIENTE VIRTUAL (WINDOWS):**  
```bash
source environment/Scripts/activate
```

**ATIVA O AMBIENTE VIRTUAL (LINUX):**  
```bash
source environment/bin/activate
```

**ATUALIZA O PIP:**
```bash
python -m pip install --upgrade pip --require-virtualenv
```

**INSTALA AS DEPEND√äNCIAS:**  
```bash
pip install -U -v --require-virtualenv -r requirements.txt
```

**LISTA AS DEPEND√äNCIAS:**
```bash
pip list --require-virtualenv
```

**SALVA AS DEPEND√äNCIAS:**
```bash
pip freeze > requirements.txt --require-virtualenv
```

**Now, Be Happy!!!** üò¨

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**
