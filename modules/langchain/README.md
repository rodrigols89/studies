# Aprende a usar a LangChain

> Minnhas **notas** e **c√≥digos** do livro [Aprende a usar a LangChain](https://learning.oreilly.com/library/view/aprende-a-usar/9798341637917/)

## Conte√∫do

 - **1. Fundamentos de LLM com LangChain:**
   - [`O que √© ChatOpenAI?`](#intro-to-chatopenai)
   - [`M√©todos de Execu√ß√£o de Runnables no LangChain`](#runnables-methods-langchain)
   - [`M√©todo .invoke()`](#invoke-method)
   - [`Modelo de Mensagens de Conversa no LangChain`](#messages-in-langchain)
   - [`"Templates de Prompt" no LangChain`](#templates-in-langchain)
 - **Configura√ß√µes:**
   - [`Criando o ambiente virtual`](#create-env)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->









































































































<!--- ( 1. Fundamentos de LLM com LangChainn ) --->

---

<div id="intro-to-chatopenai"></div>

## `O que √© ChatOpenAI?`

ChatOpenAI √© a classe do LangChain respons√°vel por conversar com os chat models da OpenAI, como:

 - gpt-3.5-turbo
 - gpt-4
 - gpt-4o
 - gpt-4.1

Ela implementa a interface de Chat Models, ou seja:

 - Recebe mensagens (strings ou objetos de mensagem)
 - Envia para a API da OpenAI
 - Retorna uma resposta estruturada (n√£o apenas texto cru)

### `Que pacote √© esse "langchain_openai"?`

A partir do LangChain `0.1 ‚Üí 0.2`, o projeto foi dividido em pacotes menores.

**Antes (antigo ‚ùå):**
```python
from langchain.chat_models import ChatOpenAI
```

**Agora (correto ‚úÖ):**
```python
from langchain_openai.chat_models import ChatOpenAI
```

### `O que significa "chat_models"?`

Dentro do pacote langchain_openai, existem v√°rios m√≥dulos:

```bash
langchain_openai/
 ‚îú‚îÄ‚îÄ llms/          ‚Üí modelos antigos (completion)
 ‚îú‚îÄ‚îÄ chat_models/   ‚Üí modelos de chat (messages)
 ‚îú‚îÄ‚îÄ embeddings/    ‚Üí embeddings OpenAI
```

### `Par√¢metros da classe ChatOpenAI`

Os principais (os mais usados) par√¢metros da classe ChatOpenAI s√£o:

 - `model`
   - Define qual modelo da OpenAI ser√° usado.
   - Exemplos:
     - "gpt-3.5-turbo"
     - "gpt-4"
     - "gpt-4o"
     - "gpt-4.1"
 - `temperature`
   - Controla a criatividade da resposta.
   - 0.0 ‚ûî	Determin√≠stico
   - 0.2 ‚ûî	Mais preciso
   - 0.7 ‚ûî	Criativo
   - 1.0+ ‚ûî	Muito criativo
 - `max_tokens`
   - Limita o n√∫mero m√°ximo de tokens gerados na resposta.
 - `timeout`
   - Timeout da requisi√ß√£o (em segundos).
   - Exemplo: timeout=60
 - `verbose`
   - Mostra logs internos do LangChain.
   - Exemplo: verbose=True

Por exemplo:

[chapter01/ChatOpenAI-v1.py](codes/chapter01/ChatOpenAI-v1.py)
```python
from langchain_openai.chat_models import ChatOpenAI

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)
```

No exemplo acima n√≥s temos:

 - `model = ChatOpenAI()`
   - Criamos uma inst√¢ncia da classe ChatOpenAI.
 - `model="gpt-3.5-turbo"`
   - Estamos dizendo que esse modelo vai utilizar o modelo `"gpt-3.5-turbo"` da OpenAI.
 - `temperature=0`
   - Aqui estamos dizendo que queremos uma resposta determin√≠stica.
   - **NOTE:** Ou seja, a resposta vai ser sempre a mesma, mesmo se o modelo for criativo.
 - `max_tokens=100`
   - Limitamos o n√∫mero de tokens gerados na resposta.




















---

<div id="runnables-methods-langchain"></div>

## `M√©todos de Execu√ß√£o de Runnables no LangChain`

A classe **ChatOpenAI** herda de **BaseChatModel** e implementa a interface **Runnable**, o que permite executar o modelo de diferentes formas:

```bash
ChatOpenAI
   ‚îî‚îÄ‚îÄ BaseChatModel
         ‚îî‚îÄ‚îÄ Runnable
               ‚îú‚îÄ‚îÄ invoke()
               ‚îú‚îÄ‚îÄ batch()
               ‚îú‚îÄ‚îÄ stream()
               ‚îî‚îÄ‚îÄ ainvoke()
```

Ou seja, todos esses m√©todos servem para executar o modelo, mudando apenas a forma de execu√ß√£o:

 - `.invoke()` ‚Üí execu√ß√£o s√≠ncrona (mais comum)
 - `.ainvoke()` ‚Üí execu√ß√£o ass√≠ncrona (async)
 - `.stream()` ‚Üí resposta em streaming
 - `.batch()` ‚Üí m√∫ltiplas entradas de uma vez

### `1Ô∏è‚É£ O que todos eles t√™m em comum?`

Todos:

 - Executam um Runnable (ChatOpenAI, chain, prompt, RAG, etc.)
 - Usam a mesma l√≥gica interna
 - Recebem o mesmo tipo de entrada
 - Produzem o mesmo tipo de sa√≠da final

> **üìå A diferen√ßa est√° em:**  
> **como** e **quando** a resposta √© entregue.

### `2Ô∏è‚É£ .invoke() ‚Äî execu√ß√£o s√≠ncrona (padr√£o)`

**Quando usar:**

 - C√≥digo simples
 - Scripts
 - Notebooks
 - Fluxo linear

**Exemplo:**
```python
response = chat.invoke("Explain LangChain in one sentence.")
print(response.content)
```

**Caracter√≠sticas:**

 - Bloqueia at√© a resposta chegar
 - Retorna um √∫nico resultado

### `.batch() ‚Äî m√∫ltiplas entradas de uma vez`

**Quando usar:**

 - Processar muitos prompts
 - Ganhar performance
 - Reduzir overhead

**Exemplo:**
```python
inputs = [
    "Explain LangChain in one sentence.",
    "What is RAG?",
    "What is LCEL?"
]

responses = chat.batch(inputs)

for r in responses:
    print(r.content)
```

**Caracter√≠sticas:**

 - Recebe list[input]
 - Retorna list[output]
 - Pode rodar em paralelo (dependendo do backend)

### `4Ô∏è‚É£ .stream() ‚Äî resposta em streaming (token a token)`

**Quando usar:**

 - Chat em tempo real
 - Interfaces web
 - UX melhor

**Exemplo:**
```python
for chunk in chat.stream("Explain LangChain in one sentence."):
    print(chunk.content, end="", flush=True)
```

**Caracter√≠sticas:**

 - Retorna um gerador
 - Entrega partes da resposta conforme s√£o geradas
 - Ideal para interfaces interativas

### `.ainvoke() ‚Äî execu√ß√£o ass√≠ncrona (async)`

**Quando usar:**

 - FastAPI
 - Apps web
 - Alta concorr√™ncia

**Exemplo:**
```python
import asyncio

async def run():
    response = await chat.ainvoke("Explain LangChain in one sentence.")
    print(response.content)

asyncio.run(run())
```

**Caracter√≠sticas:**

 - N√£o bloqueia a thread
 - Requer async/await

### `6Ô∏è‚É£ Compara√ß√£o direta`

| M√©todo       | Execu√ß√£o   | Entrada | Sa√≠da      | Uso t√≠pico            |
| ------------ | ---------- | ------- | ---------- | --------------------- |
| `.invoke()`  | S√≠ncrona   | 1 item  | 1 resposta | Scripts / notebooks   |
| `.batch()`   | Paralela   | Lista   | Lista      | Processamento em lote |
| `.stream()`  | Streaming  | 1 item  | Gerador    | Chat em tempo real    |
| `.ainvoke()` | Ass√≠ncrona | 1 item  | 1 resposta | Web / APIs            |

### `7Ô∏è‚É£ üß† Resumo final`

 - ‚úÖ Sim, eles s√£o similares no prop√≥sito
 - ‚ùå N√£o s√£o iguais no comportamento

Todos fazem a mesma coisa:

> **‚û°Ô∏è executam um Runnable.**  
> *Mas cada um √© ideal para um cen√°rio espec√≠fico.*




















---

<div id="invoke-method"></div>

## `M√©todo .invoke()`

O m√©todo `.invoke()` √© a forma padr√£o e moderna de **executar um modelo no LangChain**.

> **üìå Em termos simples:**  
> `.invoke()` envia uma entrada para o modelo e retorna uma resposta estruturada.

### `Par√¢metros input=... do m√©todo .invoke()`

 - Esse √© o mais importante.
 - Ele pode assumir 3 formas principais.

**Forma 1 ‚Äî String simples (mais comum):** [chapter01/invoke-01.py](codes/chapter01/invoke-01.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

response = model.invoke("The sky is")
print(response.content)
```

**OUTPUT:**
```bash
blue and clear, with fluffy white clouds scattered across the horizon. The sun is shining brightly, casting a warm glow over everything below. It's a beautiful day to be outside and enjoy the beauty of nature.
```

> **NOTE:**  
> üìå Aqui o *LangChain* converte automaticamente a string em:  
> `HumanMessage(content="The sky is")`

**Forma 2 ‚Äî Lista de mensagens (chat expl√≠cito):** [chapter01/invoke-02.py](codes/chapter01/invoke-02.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="Explain LangChain in one sentence.")
]

response = model.invoke(messages)
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that aims to facilitate cross-border communication and collaboration by providing language translation services.
```

**üìå Use isso quando precisar de:**

 - system
 - assistant
 - hist√≥rico de conversa

**Forma 3 ‚Äî Dicion√°rio (com Prompt Template / LCEL):** [chapter01/invoke-03.py](codes/chapter01/invoke-03.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_template(
    "Explain {topic} in one sentence."
)

chain = prompt | model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

response = chain.invoke({"topic": "LangChain"})
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that aims to facilitate cross-border communication and collaboration by providing language translation services.
```

**üìå Aqui:**

 - `.invoke()` recebe um dict
 - O prompt resolve as vari√°veis antes de chamar o modelo

### `O que o m√©todo .invoke() retorna?`

Sempre retorna um objeto:

```bash
AIMessage
```

Exemplo real:

```bash
AIMessage(
    content="LangChain is a framework for building LLM-powered apps.",
    additional_kwargs={},
    response_metadata={
        "model": "gpt-3.5-turbo",
        "token_usage": {...}
    }
)
```

### `Diferen√ßa entre .invoke() e .predict() (importante)`

| M√©todo       | Status                  |
| ------------ | ----------------------- |
| `.invoke()`  | ‚úÖ Padr√£o atual        |
| `.predict()` | ‚ö†Ô∏è Legacy / deprecated |

### `Erros comuns (‚ö†Ô∏è)`

**‚ùå Erro 1 ‚Äî esperar string:**
```python
text = chat.invoke("Hello")
print(text)  # errado
```

**‚úÖ Correto:**
```python
print(text.content)
```




















---

<div id="messages-in-langchain"></div>

## `Modelo de Mensagens de Conversa no LangChain`

Para que serve `langchain_core.messages`?

```python
from langchain_core.messages import ...
```

Esse m√≥dulo define o formato padr√£o de mensagens que o LangChain usa para representar uma conversa entre:

 - sistema
 - usu√°rio
 - modelo
 - ferramentas

### `1Ô∏è‚É£ Por que isso √© importante?`

Porque modelos de chat n√£o recebem texto solto, eles recebem listas de mensagens com pap√©is:

```bash
System ‚Üí Human ‚Üí AI ‚Üí Tool ‚Üí Human ‚Üí AI
```

Se voc√™ entende isso, voc√™ entende:

 - chat
 - mem√≥ria
 - RAG
 - agentes
 - tool calling

### `2Ô∏è‚É£ Imports mais utilizados

 - `SystemMessage`
   - Define regras e comportamento do modelo.

```python
SystemMessage(content="You are a helpful assistant.")
```

 - `HumanMessage`
   - Representa a mensagem do usu√°rio.

```python
HumanMessage(content="What is LangChain?")
```

 - `AIMessage`
   - Representa respostas do modelo (√∫til para hist√≥rico).

```python
AIMessage(content="LangChain is a framework...")
```

 - `ToolMessage`
   - Representa a sa√≠da de uma ferramenta (agents / tools).

```python
ToolMessage(
    content="Result from search",
    tool_call_id="abc123"
)
```

### `3Ô∏è‚É£ Exemplo m√≠nimo completo`

[chapter01/messages-01.py](codes/chapter01/messages-01.py)
```python
from langchain_core.messages import (
    SystemMessage,
    HumanMessage
)
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

load_dotenv()

chat = ChatOpenAI()

messages = [
    SystemMessage(content="You are a tutor."),
    HumanMessage(content="Explain LangChain in one sentence.")
]

response = chat.invoke(messages)
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that enables developers to build decentralized language-oriented applications.
```

### `5Ô∏è‚É£ Regra de ouro üß†`

> **LangChain** √© sobre **orquestrar mensagens**, n√£o strings.

Se voc√™ domina `langchain_core.messages`, voc√™ domina:

 - conversas
 - contexto
 - hist√≥rico




















---

<div id="templates-in-langchain"></div>

## `"Templates de Prompt" no LangChain`

Para que serve `langchain_core.prompts`?

```python
from langchain_core.prompts import ...
```

> **NOTE:**  
> Esse m√≥dulo **define como voc√™ constr√≥i prompts de forma estruturada**, reutiliz√°vel e segura no LangChain.

Ele existe para:

 - evitar concatena√ß√£o manual de strings
 - organizar vari√°veis de entrada
 - padronizar prompts
 - integrar prompts com modelos via LCEL

> **üìå Em resumo:**  
> Prompts deixam de ser texto solto e passam a ser componentes.

## `1Ô∏è‚É£ Por que isso √© importante?`

> **Modelos de linguagem n√£o recebem c√≥digo**, **"recebem prompts bem formados"**.

Sem templates:

 - mais erro
 - mais duplica√ß√£o
 - dif√≠cil de manter

Com templates:

 - clareza
 - reutiliza√ß√£o
 - f√°cil integra√ß√£o com *chains* e *RAG*

### 2Ô∏è‚É£ Imports mais utilizados

 - `PromptTemplate`
   - Usado para modelos que recebem texto simples (LLMs).

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    "Translate {text} to Portuguese."
)
```

 - `ChatPromptTemplate`
   - Usado para modelos de chat.

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(
    "Explain {topic} in one sentence."
)
```

 - `MessagesPlaceholder`
   - Usado para injetar hist√≥rico de conversa dinamicamente.

```python
from langchain_core.prompts import MessagesPlaceholder

ChatPromptTemplate.from_messages([
    ("system", "You are a tutor."),
    MessagesPlaceholder("history"),
    ("human", "{question}")
])
```

 - `HumanMessagePromptTemplate`
   - Define explicitamente um template de mensagem humana.
 - `SystemMessagePromptTemplate`
   - Define regras do sistema como template.

### `3Ô∏è‚É£ Exemplo m√≠nimo completo`

[chapter01/prompts-01.py](codes/chapter01/prompts-01.py)
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "Explain {topic} in one sentence.")
])

chain = prompt | ChatOpenAI()

response = chain.invoke({"topic": "LangChain"})
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a decentralized platform that connects businesses with language service providers for efficient and secure translation services.
```

### `5Ô∏è‚É£ Regra de ouro üß†`

 - Use **prompts** para **definir estrutura**.
 - Use **messages** para **controlar conversa**.









































































































<!--- ( Configura√ß√µes ) --->

---

<div id="create-env"></div>

## `Criando o ambiente virtual`

**NOTE:**  
Antes de criar o ambiente virtual, crie um arquivo chamado [.env](.env) e depois insira a sua chave de API da OpenAI:

[.env](.env)
```bash
OPENAI_API_KEY=your_api_key
```

Continuando...

**CRIA O AMBIENTE VIRTUAL:**  
```bash
python3 -m venv environment
```

**ATIVA O AMBIENTE VIRTUAL (WINDOWS):**  
```bash
source environment/Scripts/activate
```

**ATIVA O AMBIENTE VIRTUAL (LINUX):**  
```bash
source environment/bin/activate
```

**ATUALIZA O PIP:**
```bash
python -m pip install --upgrade pip --require-virtualenv
```

**INSTALA AS DEPEND√äNCIAS:**  
```bash
pip install -U -v --require-virtualenv -r requirements.txt
```

**LISTA AS DEPEND√äNCIAS:**
```bash
pip list --require-virtualenv
```

**SALVA AS DEPEND√äNCIAS:**
```bash
pip freeze > requirements.txt --require-virtualenv
```

**Now, Be Happy!!!** üò¨

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**
