# Aprenda a usar LangChain

> Minnhas **notas** e **c√≥digos** do livro [Aprende a usar a LangChain](https://learning.oreilly.com/library/view/aprende-a-usar/9798341637917/)

## Conte√∫do

 - **1. Fundamentos de LLM com LangChain:**
   - [`O que √© ChatOpenAI?`](#intro-to-chatopenai)
   - [`M√©todos de Execu√ß√£o de Runnables no LangChain`](#runnables-methods-langchain)
   - [`M√©todo .invoke()`](#invoke-method)
   - [`Modelo de Mensagens de Conversa no LangChain`](#messages-in-langchain)
   - [`"Templates de Prompt" no LangChain`](#templates-in-langchain)
 - **RAG Parte I: Indexar os teus dados:**
   - [`RAG (Retrieval-Augmented Generation)`](#intro-to-rag)
   - [`Chunks (chunk_size)`](#intro-to-chunks)
   - [`Overlap (chunk_overlap)`](#intro-to-overlap)
   - [`O que s√£o Incorpora√ß√µes de Texto? (Text Embeddings)`](#text-embeddings)
   - [`Carregadores de Documentos (Document Loaders) no LangChain`](#document-loaders)
   - [`Divis√£o de Texto (Text Splitters) no LangChain`](#text-splitters)
   - [`Gerando Texto Incorporado (Text Embeddings)`](#making-embeddings)
   - [`Indexa√ß√£o de Conhecimento (Indexing) no LangChain`](#chp02-indexing)
   - [`PGVector: Banco de Dados Vetorial com PostgreSQL no LangChain`](#chp02-pgvector)
   - [`Exemplo Completo ‚Äî Usando PGVector com LangChain`](#chp02-pgvector-exemplo)
 - **Configura√ß√µes:**
   - [`Criando o ambiente virtual`](#create-env)
<!---
[WHITESPACE RULES]
- Same topic = "20" Whitespace character.
- Different topic = "200" Whitespace character.
--->









































































































<!--- ( 1. Fundamentos de LLM com LangChainn ) --->

---

<div id="intro-to-chatopenai"></div>

## `O que √© ChatOpenAI?`

ChatOpenAI √© a classe do LangChain respons√°vel por conversar com os chat models da OpenAI, como:

 - gpt-3.5-turbo
 - gpt-4
 - gpt-4o
 - gpt-4.1

Ela implementa a interface de Chat Models, ou seja:

 - Recebe mensagens (strings ou objetos de mensagem)
 - Envia para a API da OpenAI
 - Retorna uma resposta estruturada (n√£o apenas texto cru)

### `Que pacote √© esse "langchain_openai"?`

A partir do LangChain `0.1 ‚Üí 0.2`, o projeto foi dividido em pacotes menores.

**Antes (antigo ‚ùå):**
```python
from langchain.chat_models import ChatOpenAI
```

**Agora (correto ‚úÖ):**
```python
from langchain_openai.chat_models import ChatOpenAI
```

### `O que significa "chat_models"?`

Dentro do pacote langchain_openai, existem v√°rios m√≥dulos:

```bash
langchain_openai/
 ‚îú‚îÄ‚îÄ llms/          ‚Üí modelos antigos (completion)
 ‚îú‚îÄ‚îÄ chat_models/   ‚Üí modelos de chat (messages)
 ‚îú‚îÄ‚îÄ embeddings/    ‚Üí embeddings OpenAI
```

### `Par√¢metros da classe ChatOpenAI`

Os principais (os mais usados) par√¢metros da classe ChatOpenAI s√£o:

 - `model`
   - Define qual modelo da OpenAI ser√° usado.
   - Exemplos:
     - "gpt-3.5-turbo"
     - "gpt-4"
     - "gpt-4o"
     - "gpt-4.1"
 - `temperature`
   - Controla a criatividade da resposta.
   - 0.0 ‚ûî	Determin√≠stico
   - 0.2 ‚ûî	Mais preciso
   - 0.7 ‚ûî	Criativo
   - 1.0+ ‚ûî	Muito criativo
 - `max_tokens`
   - Limita o n√∫mero m√°ximo de tokens gerados na resposta.
 - `timeout`
   - Timeout da requisi√ß√£o (em segundos).
   - Exemplo: timeout=60
 - `verbose`
   - Mostra logs internos do LangChain.
   - Exemplo: verbose=True

Por exemplo:

[chapter01/ChatOpenAI-v1.py](codes/chapter01/ChatOpenAI-v1.py)
```python
from langchain_openai.chat_models import ChatOpenAI

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)
```

No exemplo acima n√≥s temos:

 - `model = ChatOpenAI()`
   - Criamos uma inst√¢ncia da classe ChatOpenAI.
 - `model="gpt-3.5-turbo"`
   - Estamos dizendo que esse modelo vai utilizar o modelo `"gpt-3.5-turbo"` da OpenAI.
 - `temperature=0`
   - Aqui estamos dizendo que queremos uma resposta determin√≠stica.
   - **NOTE:** Ou seja, a resposta vai ser sempre a mesma, mesmo se o modelo for criativo.
 - `max_tokens=100`
   - Limitamos o n√∫mero de tokens gerados na resposta.




















---

<div id="runnables-methods-langchain"></div>

## `M√©todos de Execu√ß√£o de Runnables no LangChain`

A classe **ChatOpenAI** herda de **BaseChatModel** e implementa a interface **Runnable**, o que permite executar o modelo de diferentes formas:

```bash
ChatOpenAI
   ‚îî‚îÄ‚îÄ BaseChatModel
         ‚îî‚îÄ‚îÄ Runnable
               ‚îú‚îÄ‚îÄ invoke()
               ‚îú‚îÄ‚îÄ batch()
               ‚îú‚îÄ‚îÄ stream()
               ‚îî‚îÄ‚îÄ ainvoke()
```

Ou seja, todos esses m√©todos servem para executar o modelo, mudando apenas a forma de execu√ß√£o:

 - `.invoke()` ‚Üí execu√ß√£o s√≠ncrona (mais comum)
 - `.ainvoke()` ‚Üí execu√ß√£o ass√≠ncrona (async)
 - `.stream()` ‚Üí resposta em streaming
 - `.batch()` ‚Üí m√∫ltiplas entradas de uma vez

### `1Ô∏è‚É£ O que todos eles t√™m em comum?`

Todos:

 - Executam um Runnable (ChatOpenAI, chain, prompt, RAG, etc.)
 - Usam a mesma l√≥gica interna
 - Recebem o mesmo tipo de entrada
 - Produzem o mesmo tipo de sa√≠da final

> **üìå A diferen√ßa est√° em:**  
> **como** e **quando** a resposta √© entregue.

### `2Ô∏è‚É£ .invoke() ‚Äî execu√ß√£o s√≠ncrona (padr√£o)`

**Quando usar:**

 - C√≥digo simples
 - Scripts
 - Notebooks
 - Fluxo linear

**Exemplo:**
```python
response = chat.invoke("Explain LangChain in one sentence.")
print(response.content)
```

**Caracter√≠sticas:**

 - Bloqueia at√© a resposta chegar
 - Retorna um √∫nico resultado

### `.batch() ‚Äî m√∫ltiplas entradas de uma vez`

**Quando usar:**

 - Processar muitos prompts
 - Ganhar performance
 - Reduzir overhead

**Exemplo:**
```python
inputs = [
    "Explain LangChain in one sentence.",
    "What is RAG?",
    "What is LCEL?"
]

responses = chat.batch(inputs)

for r in responses:
    print(r.content)
```

**Caracter√≠sticas:**

 - Recebe list[input]
 - Retorna list[output]
 - Pode rodar em paralelo (dependendo do backend)

### `4Ô∏è‚É£ .stream() ‚Äî resposta em streaming (token a token)`

**Quando usar:**

 - Chat em tempo real
 - Interfaces web
 - UX melhor

**Exemplo:**
```python
for chunk in chat.stream("Explain LangChain in one sentence."):
    print(chunk.content, end="", flush=True)
```

**Caracter√≠sticas:**

 - Retorna um gerador
 - Entrega partes da resposta conforme s√£o geradas
 - Ideal para interfaces interativas

### `.ainvoke() ‚Äî execu√ß√£o ass√≠ncrona (async)`

**Quando usar:**

 - FastAPI
 - Apps web
 - Alta concorr√™ncia

**Exemplo:**
```python
import asyncio

async def run():
    response = await chat.ainvoke("Explain LangChain in one sentence.")
    print(response.content)

asyncio.run(run())
```

**Caracter√≠sticas:**

 - N√£o bloqueia a thread
 - Requer async/await

### `6Ô∏è‚É£ Compara√ß√£o direta`

| M√©todo       | Execu√ß√£o   | Entrada | Sa√≠da      | Uso t√≠pico            |
| ------------ | ---------- | ------- | ---------- | --------------------- |
| `.invoke()`  | S√≠ncrona   | 1 item  | 1 resposta | Scripts / notebooks   |
| `.batch()`   | Paralela   | Lista   | Lista      | Processamento em lote |
| `.stream()`  | Streaming  | 1 item  | Gerador    | Chat em tempo real    |
| `.ainvoke()` | Ass√≠ncrona | 1 item  | 1 resposta | Web / APIs            |

### `7Ô∏è‚É£ üß† Resumo final`

 - ‚úÖ Sim, eles s√£o similares no prop√≥sito
 - ‚ùå N√£o s√£o iguais no comportamento

Todos fazem a mesma coisa:

> **‚û°Ô∏è executam um Runnable.**  
> *Mas cada um √© ideal para um cen√°rio espec√≠fico.*




















---

<div id="invoke-method"></div>

## `M√©todo .invoke()`

O m√©todo `.invoke()` √© a forma padr√£o e moderna de **executar um modelo no LangChain**.

> **üìå Em termos simples:**  
> `.invoke()` envia uma entrada para o modelo e retorna uma resposta estruturada.

### `Par√¢metros input=... do m√©todo .invoke()`

 - Esse √© o mais importante.
 - Ele pode assumir 3 formas principais.

**Forma 1 ‚Äî String simples (mais comum):** [chapter01/invoke-01.py](codes/chapter01/invoke-01.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

response = model.invoke("The sky is")
print(response.content)
```

**OUTPUT:**
```bash
blue and clear, with fluffy white clouds scattered across the horizon. The sun is shining brightly, casting a warm glow over everything below. It's a beautiful day to be outside and enjoy the beauty of nature.
```

> **NOTE:**  
> üìå Aqui o *LangChain* converte automaticamente a string em:  
> `HumanMessage(content="The sky is")`

**Forma 2 ‚Äî Lista de mensagens (chat expl√≠cito):** [chapter01/invoke-02.py](codes/chapter01/invoke-02.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="Explain LangChain in one sentence.")
]

response = model.invoke(messages)
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that aims to facilitate cross-border communication and collaboration by providing language translation services.
```

**üìå Use isso quando precisar de:**

 - system
 - assistant
 - hist√≥rico de conversa

**Forma 3 ‚Äî Dicion√°rio (com Prompt Template / LCEL):** [chapter01/invoke-03.py](codes/chapter01/invoke-03.py)
```python
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_template(
    "Explain {topic} in one sentence."
)

chain = prompt | model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=100,
)

response = chain.invoke({"topic": "LangChain"})
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that aims to facilitate cross-border communication and collaboration by providing language translation services.
```

**üìå Aqui:**

 - `.invoke()` recebe um dict
 - O prompt resolve as vari√°veis antes de chamar o modelo

### `O que o m√©todo .invoke() retorna?`

Sempre retorna um objeto:

```bash
AIMessage
```

Exemplo real:

```bash
AIMessage(
    content="LangChain is a framework for building LLM-powered apps.",
    additional_kwargs={},
    response_metadata={
        "model": "gpt-3.5-turbo",
        "token_usage": {...}
    }
)
```

### `Diferen√ßa entre .invoke() e .predict() (importante)`

| M√©todo       | Status                  |
| ------------ | ----------------------- |
| `.invoke()`  | ‚úÖ Padr√£o atual        |
| `.predict()` | ‚ö†Ô∏è Legacy / deprecated |

### `Erros comuns (‚ö†Ô∏è)`

**‚ùå Erro 1 ‚Äî esperar string:**
```python
text = chat.invoke("Hello")
print(text)  # errado
```

**‚úÖ Correto:**
```python
print(text.content)
```




















---

<div id="messages-in-langchain"></div>

## `Modelo de Mensagens de Conversa no LangChain`

Para que serve `langchain_core.messages`?

```python
from langchain_core.messages import ...
```

Esse m√≥dulo define o formato padr√£o de mensagens que o LangChain usa para representar uma conversa entre:

 - sistema
 - usu√°rio
 - modelo
 - ferramentas

### `1Ô∏è‚É£ Por que isso √© importante?`

Porque modelos de chat n√£o recebem texto solto, eles recebem listas de mensagens com pap√©is:

```bash
System ‚Üí Human ‚Üí AI ‚Üí Tool ‚Üí Human ‚Üí AI
```

Se voc√™ entende isso, voc√™ entende:

 - chat
 - mem√≥ria
 - RAG
 - agentes
 - tool calling

### `2Ô∏è‚É£ Imports mais utilizados

 - `SystemMessage`
   - Define regras e comportamento do modelo.

```python
SystemMessage(content="You are a helpful assistant.")
```

 - `HumanMessage`
   - Representa a mensagem do usu√°rio.

```python
HumanMessage(content="What is LangChain?")
```

 - `AIMessage`
   - Representa respostas do modelo (√∫til para hist√≥rico).

```python
AIMessage(content="LangChain is a framework...")
```

 - `ToolMessage`
   - Representa a sa√≠da de uma ferramenta (agents / tools).

```python
ToolMessage(
    content="Result from search",
    tool_call_id="abc123"
)
```

### `3Ô∏è‚É£ Exemplo m√≠nimo completo`

[chapter01/messages-01.py](codes/chapter01/messages-01.py)
```python
from langchain_core.messages import (
    SystemMessage,
    HumanMessage
)
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

load_dotenv()

chat = ChatOpenAI()

messages = [
    SystemMessage(content="You are a tutor."),
    HumanMessage(content="Explain LangChain in one sentence.")
]

response = chat.invoke(messages)
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a blockchain platform that enables developers to build decentralized language-oriented applications.
```

### `5Ô∏è‚É£ Regra de ouro üß†`

> **LangChain** √© sobre **orquestrar mensagens**, n√£o strings.

Se voc√™ domina `langchain_core.messages`, voc√™ domina:

 - conversas
 - contexto
 - hist√≥rico




















---

<div id="templates-in-langchain"></div>

## `"Templates de Prompt" no LangChain`

Para que serve `langchain_core.prompts`?

```python
from langchain_core.prompts import ...
```

> **NOTE:**  
> üìå Esse m√≥dulo **define como voc√™ constr√≥i prompts de forma estruturada**, reutiliz√°vel e segura no LangChain.

Ele existe para:

 - evitar concatena√ß√£o manual de strings
 - organizar vari√°veis de entrada
 - padronizar prompts
 - integrar prompts com modelos via LCEL

> **üìå Em resumo:**  
> Prompts deixam de ser texto solto e passam a ser componentes.

## `1Ô∏è‚É£ Por que isso √© importante?`

> **Modelos de linguagem n√£o recebem c√≥digo**, **"recebem prompts bem formados"**.

Sem templates:

 - mais erro
 - mais duplica√ß√£o
 - dif√≠cil de manter

Com templates:

 - clareza
 - reutiliza√ß√£o
 - f√°cil integra√ß√£o com *chains* e *RAG*

### 2Ô∏è‚É£ Imports mais utilizados

 - `PromptTemplate`
   - Usado para modelos que recebem texto simples (LLMs).

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    "Translate {text} to Portuguese."
)
```

 - `ChatPromptTemplate`
   - Usado para modelos de chat.

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(
    "Explain {topic} in one sentence."
)
```

 - `MessagesPlaceholder`
   - Usado para injetar hist√≥rico de conversa dinamicamente.

```python
from langchain_core.prompts import MessagesPlaceholder

ChatPromptTemplate.from_messages([
    ("system", "You are a tutor."),
    MessagesPlaceholder("history"),
    ("human", "{question}")
])
```

 - `HumanMessagePromptTemplate`
   - Define explicitamente um template de mensagem humana.
 - `SystemMessagePromptTemplate`
   - Define regras do sistema como template.

### `3Ô∏è‚É£ Exemplo m√≠nimo completo`

[chapter01/prompts-01.py](codes/chapter01/prompts-01.py)
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "Explain {topic} in one sentence.")
])

chain = prompt | ChatOpenAI()

response = chain.invoke({"topic": "LangChain"})
print(response.content)
```

**OUTPUT:**
```bash
LangChain is a decentralized platform that connects businesses with language service providers for efficient and secure translation services.
```

### `5Ô∏è‚É£ Regra de ouro üß†`

 - Use **prompts** para **definir estrutura**.
 - Use **messages** para **controlar conversa**.







































































































<!--- ( RAG Parte I: Indexar os teus dados ) --->

---
























---

<div id="intro-to-rag"></div>

## `RAG (Retrieval-Augmented Generation)`

Retrieval‚ÄëAugmented Generation (RAG) √© uma t√©cnica que combina:

 - Recupera√ß√£o de documentos (retrieval);
 - Com modelos geradores de linguagem (generation).

Em vez de confiar apenas no conhecimento ‚Äúembutido‚Äù nos par√¢metros do modelo, o RAG permite que o sistema v√° buscar trechos de texto relevantes em uma base externa (por exemplo, Wikipedia, banco de documentos corporativos) e use essas informa√ß√µes para gerar respostas mais precisas e contextualizadas.

![img](images/rag-01.png)  

### `Quando utilizar RAG?`

 - **Base de conhecimento grande e em constante atualiza√ß√£o:**
   - Documenta√ß√µes, FAQs, bases cient√≠ficas.
 - **Dom√≠nios t√©cnicos/especializados:**
   - Jur√≠dico, m√©dico, pesquisadores que exigem precis√£o e cita√ß√µes.
 - **Sistemas de suporte ao cliente:**
   - Chatbots que precisam referenciar manuais, pol√≠ticas, termos de servi√ßo.

### `Quando n√£o utilizar RAG?`

 - **Tarefas de conversa√ß√£o livre:**
   - Bate‚Äëpapo informal, cria√ß√£o de conte√∫do criativo onde n√£o h√° necessidade de buscar fatos externos.
 - **Restri√ß√µes de lat√™ncia:**
   - Se seu sistema exige respostas em tempo real (<100‚ÄØms) e n√£o comporta o tempo extra de recupera√ß√£o.
 - **Ambientes com poucos dados:**
   - Se a base de documentos for pequena e autossuficiente, pode ser mais simples usar um LLM puro ou at√© finetuning.





















---

<div id="intro-to-chunks"></div>

## `Chunks (chunk_size)`

> Imagina que voc√™ precisa criar um *RAG* que utiliza a **Constitui√ß√£o Federal** para auxiliar advogados.

Se, para uma pergunta sobre **direito do consumidor**, enviarmos *toda a constitui√ß√£o*, isso far√° com que o modelo de IA n√£o consiga processar todas as informa√ß√µes, j√° que, quanto maior o prompt, **menos precisa tende a ser a resposta**.

Para isso, utilizamos a t√©cnica de **"chunks"**, onde, pegamos um arquivo geral e o quebramos em v√°rios pequenos trechos:

![img](images/chunks-01.png)  

> **NOTE:**  
> üìå Podemos usar um `chunk_size` para especificar quantos caracteres teremos por **chunk**.

A *Constitui√ß√£o Federal* possui **64.488 palavras**. Se definirmos um `chunk_size` como **100**, teremos **645 mini arquivos (64.488√∑100)** da Constitui√ß√£o.

### üßæ Exemplos:

 - **Art. 1¬∫** A Rep√∫blica Federativa do Brasil, formada pela uni√£o indissol√∫vel dos Estados e Munic√≠pios e do Distrito Federal, constitui-se em Estado Democr√°tico de Direito e tem como fundamentos...
 - **Par√°grafo √∫nico.** Todo o poder emana do povo, que o exerce por meio de representantes eleitos ou diretamente, nos termos desta Constitui√ß√£o...
 - **Art. 2¬∫** S√£o Poderes da Uni√£o, independentes e harm√¥nicos entre si, o Legislativo, o Executivo e o Judici√°rio...





















---

<div id="intro-to-overlap"></div>

## `Overlap (chunk_overlap)`

**Mas agora enfrentamos outro problema:**  
Ao separar o texto por chunks, pode ser que eles **fiquem sem sentido**, j√° que partes importantes da informa√ß√£o podem ser **cortadas (separadas)**.

> **NOTE:**  
> üìå Para isso, usamos o par√¢metro `chunk_overlap`.

 - Ele define quantos caracteres de sobreposi√ß√£o haver√° entre um chunk e o pr√≥ximo.
 - üëâ Isso √© √∫til para manter o contexto entre peda√ßos consecutivos.

Por exemplo, Exemplo com `chunk_size = 500` e `chunk_overlap = 100`

```bash
[000 ... 499]
[400 ... 899]
[800 ... 1299]
```

Vejam que:

 - **Nosso primeiro chunk come√ßa em 000 e termina em 499:**
   - Ou seja, as primeiras 500 palavras da Constitui√ß√£o.
 - **Nosso segundo chunk come√ßa em 400 (por causa do "chunk_overlap = 100") e termina em 899:**
   - Ou seja, ele est√° pegando as 100 √∫ltimas palavras do chunk anterior.
   - **NOTE:** Isso √© importante para evitar perda de contexto entre os chunks.

Por exemplo, imagine que n√≥s temos o seguinte texto:

```bash
Python √© uma excelente linguagem de programa√ß√£o para web e IA.
```

Se aplicarmos:

 - `chunk_size = 7`
 - `chunk_overlap = 3`

Vamos ter:

```bash
Python √© uma excelente linguagem de programa√ß√£o para web e IA.
   |   |  |      |         |     |       |
   0   1  2      3         4     5       6
   ---------------------------------------
                chunk 1


Python √© uma excelente linguagem de programa√ß√£o para web e IA.
                           |     |       |       |    |  |  |
                           1     2       3       4    5  6  7
                           -----------------------------------
                                        chunk 2
```

> **NOTE:**  
> üìå Vejam que n√≥s pegamos as **3 √∫ltimas palavras do chunk (overlap = 3)** para n√£o perde contexto.





















---

<div id="text-embeddings"></div>

## `O que s√£o Incorpora√ß√µes de Texto? (Text Embeddings)`

> As **incorpora√ß√µes de texto** s√£o uma forma de **converter palavras ou frases do texto em dados num√©ricos que uma m√°quina pode entender**.

 - Pense nisso como se estivesse transformando o texto em uma lista de n√∫meros, em que cada n√∫mero captura uma parte do significado do texto.
 - Essa t√©cnica ajuda as m√°quinas a entender o contexto e as rela√ß√µes entre as palavras.

![img](images/embeddings-01.png)  





















---

<div id="document-loaders"></div>

## `Carregadores de Documentos (Document Loaders) no LangChain`

Para que serve `langchain_community.document_loaders`?

```python
from langchain_community.document_loaders import ...
```

Esse m√≥dulo re√∫ne **carregadores de documentos (document loaders)**, ou seja, classes respons√°veis por:

 - ler arquivos ou fontes externas (PDF, TXT, CSV, HTML, URLs, etc.)
 - extrair o conte√∫do
 - converter tudo para o formato padr√£o do LangChain: `Document`

> **üìå Em RAG, tudo come√ßa aqui.**  
> Se o dado n√£o foi carregado corretamente, o resto do pipeline falha.

### `1Ô∏è‚É£ O que √© um Document?`

Todo *loader* retorna uma lista de objetos `Document`:

```python
Document(
    page_content="texto extra√≠do",
    metadata={"source": "..."}
)
```

Esse formato √© usado depois por:

 - text splitters
 - embeddings
 - vector stores
 - retrievers

### `2Ô∏è‚É£ Imports mais utilizados`

 - `TextLoader`
   - Carrega arquivos `.txt`

```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/example.txt")
documents = loader.load()
```

> **üìå Uso comum:**  
> logs, textos simples, dumps.

 - `PyPDFLoader`
   - Extrai texto de arquivos PDF (p√°gina por p√°gina).

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("data/manual.pdf")
documents = loader.load()
```

> **NOTE:**  
> üìå Cada p√°gina vira um Document.

 - `CSVLoader`
   - L√™ arquivos .csv.

```python
from langchain_community.document_loaders import CSVLoader

loader = CSVLoader(
    file_path="data/users.csv",
    source_column="email"
)
documents = loader.load()
```

> **NOTE:**  
> üìå Cada linha vira um Document.

 - `JSONLoader`
   - Carrega dados estruturados em `JSON`.

```python
from langchain_community.document_loaders import JSONLoader

loader = JSONLoader(
    file_path="data/data.json",
    jq_schema=".items[]",
    text_content=False
)
documents = loader.load()
```

> **NOTE:**  
> üìå Muito usado com APIs e dumps de dados.

 - `UnstructuredFileLoader`
   - Loader gen√©rico para v√°rios formatos.

```python
from langchain_community.document_loaders import UnstructuredFileLoader

loader = UnstructuredFileLoader("data/report.docx")
documents = loader.load()
```

**üìå Funciona com:**

 - `.docx`
 - `.pptx`
 - `.html`
 - `.md`

 - `WebBaseLoader`
   - Carrega conte√∫do direto de URLs.

```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://docs.langchain.com/")
documents = loader.load()
```

> **NOTE:**  
> üìå Muito usado para *RAG* com *documenta√ß√£o online*.

 - `DirectoryLoader`
   - Carrega v√°rios arquivos de uma pasta.

```python
from langchain_community.document_loaders import DirectoryLoader

loader = DirectoryLoader(
    "data/",
    glob="**/*.pdf"
)
documents = loader.load()
```

> **NOTE:**  
> üìå Essencial para bases grandes de documentos.

### `Documenta√ß√£o do "Document loaders"`

> Aqui -> [Document loaders](https://docs.langchain.com/oss/python/integrations/document_loaders)





















---

<div id="text-splitters"></div>

## `Divis√£o de Texto (Text Splitters) no LangChain`

1Ô∏è‚É£ Para que serve langchain_text_splitters?

```python
from langchain_text_splitters import ...
```

Esse m√≥dulo cont√©m os **Text Splitters**, respons√°veis por **quebrar documentos grandes em peda√ßos menores (chunks)**.

üìå Isso √© essencial porque:

 - LLMs t√™m limite de tokens
 - embeddings funcionam melhor com textos menores
 - buscas vetoriais ficam mais precisas

> **NOTE:**  
> üìå Em *RAG*, o `splitter` √© o cora√ß√£o da qualidade.

### `2Ô∏è‚É£ O problema que ele resolve`

 - **Sem splitter ‚ùå:**
   - texto grande demais
   - perda de contexto
   - embeddings ruins
 - **Com splitter ‚úÖ:**
   - chunks consistentes
   - contexto preservado
   - melhor recupera√ß√£o

### `3Ô∏è‚É£ Imports mais utilizados`

 - `RecursiveCharacterTextSplitter (mais usado)`
   - Divide texto de forma inteligente, tentando manter estrutura.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)
```

Par√¢metros principais:

 - `chunk_size` ‚Üí tamanho do chunk
 - `chunk_overlap` ‚Üí sobreposi√ß√£o entre chunks
 - `separators` ‚Üí lista de separadores (opcional)

> **NOTE:**  
> üìå Padr√£o para RAG com texto natural.

 - `CharacterTextSplitter`
   - Divide texto de forma simples e direta.

```python
from langchain_text_splitters import CharacterTextSplitter

splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=300,
    chunk_overlap=30
)
```

Par√¢metros:

 - `separator` ‚Üí caractere de divis√£o (\n, .)
 - `chunk_size`
 - `chunk_overlap`

> **NOTE:**  
> üìå Bom para textos bem estruturados.

 - `TokenTextSplitter`
   - Divide baseado em tokens, n√£o caracteres.

```python
from langchain_text_splitters import TokenTextSplitter

splitter = TokenTextSplitter(
    chunk_size=256,
    chunk_overlap=25
)
```

Par√¢metros:

 - `chunk_size` ‚Üí tokens por chunk
 - `chunk_overlap`
 - `encoding_name` ‚Üí tokenizer (ex: "cl100k_base")

> **NOTE:**  
> üìå Ideal quando voc√™ precisa respeitar limite exato de tokens.

### `4Ô∏è‚É£ Regra de ouro üß†`

Escolha o `splitter` de acordo com o tipo de dado, n√£o por moda.

 - texto livre ‚Üí `RecursiveCharacterTextSplitter`
 - tokens exatos ‚Üí `TokenTextSplitter`
 - markdown ‚Üí `MarkdownTextSplitter`
 - HTML ‚Üí `HTMLTextSplitter`





















---

<div id="making-embeddings"></div>

## `Gerando Texto Incorporado (Text Embeddings)`

O LangChain tem v√°rias API para se trabalhar com Textos Incorporados (Text Embeddings) e voc√™s podem escolhar qualquer um deles de acordo com a sua necessidade.

> Aqui est√° a documenta√ß√£o oficial -> [Embedding models](https://docs.langchain.com/oss/python/integrations/text_embedding)

Por exemplo, vamos utilizar a classe `OpenAIEmbeddings` da OpenAI:

[chapter02/text-embeddings-01.py](codes/chapter02/text-embeddings-01.py)
```python
from langchain_openai import OpenAIEmbeddings

from dotenv import load_dotenv

load_dotenv()

model = OpenAIEmbeddings()

embeddings = model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!",
    ]
)
```

Se voc√™ desse um print() o resultado seria algo parecido com isso (n√£o vou mostrar porque √© muito grande):

**OUTPUT:**
```bash
[
  [
    -0.004845875, 0.004899438, -0.016358767, -0.024475135, -0.017341806,
      0.012571548, -0.019156644, 0.009036391, -0.010227379, -0.026945334,
      0.022861943, 0.010321903, -0.023479493, -0.0066544134, 0.007977734,
    0.0026371893, 0.025206111, -0.012048521, 0.012943339, 0.013094575,
    -0.010580265, -0.003509951, 0.004070787, 0.008639394, -0.020631202,
    ... 1511 more items
  ]
  [
      -0.009446913, -0.013253193, 0.013174579, 0.0057552797, -0.038993083,
      0.0077763423, -0.0260478, -0.0114384955, -0.0022683728, -0.016509168,
      0.041797023, 0.01787183, 0.00552271, -0.0049789557, 0.018146982,
      -0.01542166, 0.033752076, 0.006112323, 0.023872782, -0.016535373,
      -0.006623321, 0.016116094, -0.0061090477, -0.0044155475, -0.016627092,
    ... 1511 more items
  ]
  ... 3 more items
]
```





















---

<div id="chp02-indexing"></div>

## `Indexa√ß√£o de Conhecimento (Indexing) no LangChain (+Indexing vs. Retrieval)`

### `1Ô∏è‚É£ O que √© Indexing no LangChain?`

> O termo `Indexing` se refere ao processo de **transformar dados brutos (documentos)** em uma estrutura **pesquis√°vel por similaridade sem√¢ntica**.

Em termos pr√°ticos, √© quando voc√™:

 - carrega documentos
 - divide em chunks
 - gera embeddings
 - armazena tudo em um Vector Store

> **NOTE:**  
> üìå Sem `indexa√ß√£o`, n√£o existe *RAG*.

### `2Ô∏è‚É£ Por que a indexa√ß√£o √© necess√°ria?`

LLMs:

 - n√£o ‚Äúlembram‚Äù de documentos
 - n√£o pesquisam sozinhas
 - n√£o acessam arquivos diretamente

> **NOTE:**  
> üìå A `indexa√ß√£o` *cria uma mem√≥ria externa consult√°vel*.

### `3Ô∏è‚É£ Fluxo completo de Indexing`

No processo de indexa√ß√£o, o LangChain segue o seguinte fluxo:

```bash
Dados brutos
   ‚Üì
DocumentLoader
   ‚Üì
TextSplitter
   ‚Üì
Embeddings
   ‚Üì
VectorStore (Index)
```

> **NOTE:**  
> üìå O resultado final √© o √≠ndice vetorial.

### `4Ô∏è‚É£ Exemplo m√≠nimo de indexa√ß√£o`

**Passo 1 ‚Äî Carregar documentos:**
```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/manual.txt")
documents = loader.load()
```

**Passo 2 ‚Äî Dividir o texto:**
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_documents(documents)
```

**Passo 3 ‚Äî Gerar embeddings:**
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

**Passo 4 ‚Äî Criar o √≠ndice (Vector Store):**
```python
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    chunks,
    embeddings
)
```

> **NOTE:**  
> üìå Aqui nasce o √≠ndice.

### `5Ô∏è‚É£ Consultando o √≠ndice (busca sem√¢ntica)`

> Por√©m, ainda falta fazer a consulta sem√¢ntica nesse √≠ndice.

Para isso n√≥s vamos utilizar o conceito de **"Text Similarity"**:

```python
results = vectorstore.similarity_search(
    "How does indexing work?",
    k=3
)

for doc in results:
    print(doc.page_content)
```

### `6Ô∏è‚É£ Indexing vs Retrieval`

| Conceito    | Quando acontece | O que faz         |
| ----------- | --------------- | ----------------- |
| `Indexing`  | Antes do uso    | Cria o √≠ndice     |
| `Retrieval` | Em runtime      | Consulta o √≠ndice |

### `7Ô∏è‚É£ Persistindo e Carregando o √≠ndice`

> N√≥s tamb√©m precisamos persistir e carregar o √≠ndice de algum Banco de Dados.

**Persistindo o √≠ndice:**
```python
vectorstore.save_local("faiss_index")
```

**Carregando o √≠ndice:**
```python
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)
```





















---

<div id="chp02-pgvector"></div>

## `PGVector: Banco de Dados Vetorial com PostgreSQL no LangChain`

### `1Ô∏è‚É£ O que √© PGVector?`

PGVector √© uma extens√£o do PostgreSQL que permite armazenar e buscar embeddings (vetores) diretamente no banco de dados.

**üìå Em outras palavras:**

> PostgreSQL + vetores = Vector Store persistente e robusto

### `2Ô∏è‚É£ Por que PGVector entra nesse contexto?`

At√© agora n√≥s vimos:

```bash
DocumentLoader
‚Üí TextSplitter
‚Üí Embeddings
‚Üí VectorStore
```

O `PGVector` √© o `VectorStore` ‚Äî s√≥ que:

 - persistente
 - escal√°vel
 - SQL
 - pronto para produ√ß√£o

> **NOTE:**  
> üìå Diferente de FAISS (mem√≥ria/local), o PGVector vive em um banco real.

### `3Ô∏è‚É£ Criando um container docker com PGVector`

```bash
docker run \
    --name pgvector \
    -e POSTGRES_USER=lcuser \
    -e POSTGRES_PASSWORD=lcpass \
    -e POSTGRES_DB=lcdb \
    -p 6024:5432 \
    -d pgvector/pgvector:pg16
```

 - `docker run`
   - Inicia um container Docker.
 - `--name pgvector`
   - Define o nome do container como pgvector.
   - Facilita: *"docker ps"*, *"docker stop pgvector"*
 - `-e POSTGRES_USER=lcuser`
 - `-e POSTGRES_PASSWORD=lcpass`
 - `-e POSTGRES_DB=lcdb`
   - Vari√°veis de ambiente do PostgreSQL
   - Criam automaticamente:
     - `POSTGRES_USER` -> usu√°rio: langchain
     - `POSTGRES_PASSWORD` -> senha: langchain
     - `POSTGRES_DB` -> banco: langchain
 - `-p 6024:5432`
   - Mapeamento de porta
   - 5432 ‚Üí porta interna do PostgreSQL no container
   - 6024 ‚Üí porta acess√≠vel na sua m√°quina
   - Conex√£o externa:
     - *postgresql://langchain:langchain@localhost:6024/langchain*
 - `-d`
   - Roda o container em background.
 - `pgvector/pgvector:pg16`
   - Isso √©:
     - PostgreSQL 16
     - extens√£o pgvector j√° instalada
   - üìå Voc√™ n√£o precisa instalar nada manualmente. 

### `4Ô∏è‚É£ O que acontece quando esse container sobe?`

Internamente, o PostgreSQL:

 - inicia normalmente
 - ativa a extens√£o pgvector
 - fica pronto para criar colunas do tipo vector

Exemplo SQL real:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

### `5Ô∏è‚É£ Como o PGVector armazena embeddings?`

Cada embedding vira algo assim:

```sql
vector(1536)
```

Onde:

 - 1536 = dimens√£o do embedding (OpenAI small)

üìå Um registro t√≠pico:

```bash
id | content | embedding | metadata
```





















---

<div id="chp02-pgvector-exemplo"></div>

## `Exemplo Completo ‚Äî Usando PGVector com LangChain`

**üì¶ Pr√©-requisitos:**

 - Container do PGVector rodando (o Docker que voc√™ j√° subiu)
 - Vari√°vel de ambiente OPENAI_API_KEY configurada
 - Pacotes instalados:
   - `pip install langchain langchain-community langchain-openai psycopg2-binary`

Vamos come√ßar criando uma inst√¢ncia de `load_dotenv()` que vai ser respons√°vel por carregar as vari√°veis de ambiente:

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
from dotenv import load_dotenv

load_dotenv()
```

Continuando, agora vamos implementar a **String de Conex√£o com PostgreSQL (PGVector) no LangChain**:

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
CONNECTION_STRING = (
    "postgresql+psycopg2://"
    "lcuser:lcpass@localhost:6024/lcdb"
)
```

 - `postgresql`
   - Define o tipo de banco de dados.
   - O LangChain usa isso para saber:
     - que √© PostgreSQL
     - que suporta pgvector
 - `+psycopg2`
   - Define o driver Python usado para conectar ao banco.
   - Por qu√™?
     - PostgreSQL n√£o fala Python diretamente
     - o driver faz essa ponte
   - Biblioteca necess√°ria:
     - `pip install psycopg2-binary`
 - `://`
   - Separador padr√£o entre:
     - tipo de conex√£o
     - credenciais
 - `lcuser:lcpass`
   - Credenciais -> username:password
   - Isso vem diretamente do Docker:
     - `-e POSTGRES_USER=lcuser`
     - `-e POSTGRES_PASSWORD=lcpass`
 - `@localhost`
   - Define **onde o banco est√° rodando**.
   - localhost ‚Üí sua m√°quina
   - poderia ser:
     - IP
     - nome do container
     - hostname de produ√ß√£o
 -  `:6024`
   - Porta externa mapeada pelo Docker:
     - `-p 6024:5432`
     - üìå Importante:
       - 5432 ‚Üí porta interna do PostgreSQL
       - 6024 ‚Üí porta que voc√™ acessa
 - `/lcdb`
   - Nome do banco de dados.
   - Criado automaticamente pelo Docker:
     - `-e POSTGRES_DB=lcdb`

√ìtimo, agora vamos implementar um `loader de arquivos` de texto que vai ler um arquivo `.txt`:

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/example.txt")
```

> **O que tem na vari√°vel `loader`?**

```python
loader = TextLoader("data/example.txt")
print(loader)
print(type(loader))
```

**OUTPUT:**
```bash
<langchain_community.document_loaders.text.TextLoader object at 0x7fed2d540890>
<class 'langchain_community.document_loaders.text.TextLoader'>
```

Vejam que n√≥s temos apenas objetos da classe `langchain_community.document_loaders.text.TextLoader`.

> **E como eu acesso meus dados? (Meu texto em `data/example.txt`)?**

Para isso n√≥s precisamos utilizar o m√©todo `load()` da classe `TextLoader` que √© respons√°vel por ler o arquivo:

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
loader = TextLoader("data/example.txt")
text = loader.load()

print("\nType:", type(text))
print("\nContent:", text)
```

**OUTPUT:**
```bash
Type: <class 'list'>

Content: [Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.\n\nVector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.\n\nPGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nIndexing is the process of transforming raw documents into searchable vector representations.\n\nRetrieval is the process of querying those vectors to find the most relevant information for a user question.\n')]
```

√ìtimo, vejam que agora n√≥s temos:

 - **Um objeto lista:**
   - `<class 'list'>`
 - **Uma tupla `Document()`:**
   - Representando um √∫nico documento.
   - Essa tupla tem os seguintes campos:
     - `page_content`
     - `metadata`

Por exemplo, vamos ver o conte√∫do (`page_content`) e os metadados (`metadata`) do nosso documento (`Document`):

```python
for index, document in enumerate(text):
    print(f"\n------------ Document {index} ------------")
    print("\n[PAGE CONTENT]\n", document.page_content)
    print("\[nMETADATA]\n", document.metadata)
```

**OUTPUT:**
```bash
------------ Document 0 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.

PGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.

Using PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.

Indexing is the process of transforming raw documents into searchable vector representations.

Retrieval is the process of querying those vectors to find the most relevant information for a user question.


[METADATA]
 {'source': 'data/example.txt'}
```

Continuando na nossa implementa√ß√£o vamos dividir os dados do nosso arquivo `.txt` em 500 palavras (chunk_size=500) com 50 palavras de overlap (chunk_overlap=50):

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter


splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
```

> **O que tem na vari√°vel `splitter`?**

```python
print(splitter)
print(type(splitter))
```

**OUTPUT:**
```bash
<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x751f76613170>
<class 'langchain_text_splitters.character.RecursiveCharacterTextSplitter'>
```

 - Novamente, temos objetos LangChain, `langchain_text_splitters.character.RecursiveCharacterTextSplitter` nesse caso.
 - Bem, se n√≥s temos objetos eles tem (ou podem ter) m√©todos e n√≥s podemos utilizar esses m√©todos.
 - A classe `RecursiveCharacterTextSplitter` tem o m√©todo `split_documents()` que divide os dados em chunks:

> **NOTE:**  
> üìå Lembrando que quando n√≥s criamos uma inst√¢ncia de `RecursiveCharacterTextSplitter()` n√≥s definimos a quantidade de peda√ßos (chunks) e a quantidade de overlap (chunk_overlap).

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
chunks = splitter.split_documents(text)
```

> **O que tem na vari√°vel `chunks`?**

```python
print("\nType:", type(chunks))
print("\nContent:", chunks)
```

**OUTPUT:**
```bash
Type: <class 'list'>

Content: [Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.'), Document(metadata={'source': 'data/example.txt'}, page_content='Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.\n\nPGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nIndexing is the process of transforming raw documents into searchable vector representations.'), Document(metadata={'source': 'data/example.txt'}, page_content='Retrieval is the process of querying those vectors to find the most relevant information for a user question.')]
```

> **U√©, √© a mesma coisa que o nosso `text = loader.load()` tem?**  
> N√£o, n√£o!

 - **Primeiro, se voc√™s prestarem aten√ß√£o ver√£o que n√≥s temos 3 objetos `Document()`:**
   - Lembram que n√≥s definimos 500 palavras por divis√£o (chunk_size)?
   - Ent√£o, cada `Document()` tem 500 palavras.

Por exemplo, vamos ver separadamente esses `Document()`:

```python
for index, document in enumerate(chunks):
    print(f"\n------------ Document {index} ------------")
    print("\n[PAGE CONTENT]\n", document.page_content)
    print("\n[METADATA]\n", document.metadata)
```

**OUTPUT:**
```bash
------------ Document 0 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 1 ------------

[PAGE CONTENT]
 Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.

PGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.

Using PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.

Indexing is the process of transforming raw documents into searchable vector representations.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 2 ------------

[PAGE CONTENT]
 Retrieval is the process of querying those vectors to find the most relevant information for a user question.

[METADATA]
 {'source': 'data/example.txt'}
```

**√ìtimo, entendendo tudo isso agora vamos transformar esses chunks em vetores:**  
Para isso, primeiro vamos importar e instanciar a classe `OpenAIEmbeddings`:

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

Vejam que aqui n√≥s estamos utilizando o modelo pr√©-treinado `text-embedding-3-small` da OpenAI.

> **Agora √© s√≥ pegar nossos chunks (textos divididos) e transformar em vetores (embeddings)?**  
> N√£o nesse nosso exemplo!

**Lembram que n√≥s criamos um container com PostgreSQL que d√° suporte a pgvector?**  
Ent√£o, aqui vamos criar uma inst√¢ncia da classe `PGVector` que vai:

 - Receber os nossos chunks (textos divididos);
 - A instancia (classe respons√°vel) que vai gerar os embeddings;
 - Conex√£o com o banco de dados (string de conex√£o);
 - Um nome para a nossa cole√ß√£o:
   - Nome l√≥gico da *cole√ß√£o de vetores*.

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
from langchain_community.vectorstores import PGVector


vectorstore = PGVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_string=CONNECTION_STRING,
    collection_name="ex01_documents"
)
```

**O que acontece internamente?**  
Quando esse c√≥digo roda, o *LangChain*:

```bash
1. Conecta no PostgreSQL
2. Cria tabelas se n√£o existirem
3. Para cada chunk:
      - l√™ page_content
      - gera embedding
      - salva vetor + metadata
4. Cria ou reutiliza a cole√ß√£o
```

> **O que temos na vari√°vel `vectorstore`?**

√â um objeto que sabe:

 - onde est√£o os vetores
 - como busc√°-los
 - como calcular similaridade

> **O que esse c√≥digo (at√© esse momento) N√ÉO faz?**

 - ‚ùå N√£o responde perguntas
 - ‚ùå N√£o chama LLM
 - ‚ùå N√£o faz RAG completo
 - **NOTE:** Ele apenas cria a mem√≥ria.

> **NOTE:**  
> üìå Novamente, se n√≥s temos uma inst√¢ncia de uma classe (`vectorstore`), essa inst√¢ncia tem (ou pode ter) um m√©todo.

Isso mesmo, aqui nossa inst√¢ncia `vectorstore` pode utilizar o m√©todo `similarity_search` para buscar vetores similares:

[chapter02/pgvector-01.py](codes/chapter02/pgvector-01.py)
```python
results = vectorstore.similarity_search(
    query="What is LangChain?",
    k=3
)
```

O c√≥digo acima faz uma **busca sem√¢ntica no √≠ndice vetorial** e **retorna os 3 textos mais relevantes**, com base no significado da pergunta.

 - üìå N√£o √© busca por palavra-chave
 - üìå N√£o √© SQL tradicional
 - üìå √â busca por similaridade de significado

> **O que temos na vari√°vel `query_results`?**

```python
query_results = vectorstore.similarity_search(
    query="What is LangChain?",
    k=3
)

print("\nType:", type(query_results))
print("\nContent:", query_results)
```

**OUTPUT:**
```bash
Type: <class 'list'>

Content: [Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.'), Document(metadata={'source': 'data/example.txt'}, page_content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).\n\nIt provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).\n\nOne of the core ideas of LangChain is to connect language models with external data sources.\n\nThis is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.'), Document(metadata={'source': 'data/example.txt'}, page_content='Vector databases allow semantic search, meaning that queries are matched based on meaning rather than exact keywords.\n\nPGVector is a PostgreSQL extension that enables storing and searching vector embeddings directly in a relational database.\n\nUsing PGVector with LangChain allows developers to build production-ready RAG systems with persistence, scalability, and SQL support.\n\nIndexing is the process of transforming raw documents into searchable vector representations.')]
```

Se voc√™s prestarem bem aten√ß√£o ver√£o que n√≥s temos uma lista (`<class 'list'>`) com 3 Documents, por exemplo vamos exibir o `page_content` e `metadata` desses `Document()``, separadamente:

```python
query_results = vectorstore.similarity_search(
    query="What is LangChain?",
    k=3
)

for index, document in enumerate(query_results):
    print(f"\n------------ Document {index} ------------")
    print("\n[PAGE CONTENT]\n", document.page_content)
    print("\n[METADATA]\n", document.metadata)
```

**OUTPUT:**
```bash
------------ Document 0 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 1 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}

------------ Document 2 ------------

[PAGE CONTENT]
 LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).

It provides building blocks for common use cases such as chatbots, question answering systems, and retrieval-augmented generation (RAG).

One of the core ideas of LangChain is to connect language models with external data sources.

This is achieved by loading documents, splitting them into smaller chunks, generating embeddings, and storing them in vector databases.

[METADATA]
 {'source': 'data/example.txt'}
```










































































































<!--- ( ??? ) --->










































































































<!--- ( Configura√ß√µes ) --->

---

<div id="create-env"></div>

## `Criando o ambiente virtual`

**NOTE:**  
Antes de criar o ambiente virtual, crie um arquivo chamado [.env](.env) e depois insira a sua chave de API da OpenAI:

[.env](.env)
```bash
OPENAI_API_KEY=your_api_key
```

Continuando...

**CRIA O AMBIENTE VIRTUAL:**  
```bash
python3 -m venv environment
```

**ATIVA O AMBIENTE VIRTUAL (WINDOWS):**  
```bash
source environment/Scripts/activate
```

**ATIVA O AMBIENTE VIRTUAL (LINUX):**  
```bash
source environment/bin/activate
```

**ATUALIZA O PIP:**
```bash
python -m pip install --upgrade pip --require-virtualenv
```

**INSTALA AS DEPEND√äNCIAS:**  
```bash
pip install -U -v --require-virtualenv -r requirements.txt
```

**LISTA AS DEPEND√äNCIAS:**
```bash
pip list --require-virtualenv
```

**SALVA AS DEPEND√äNCIAS:**
```bash
pip freeze > requirements.txt --require-virtualenv
```

**Now, Be Happy!!!** üò¨

---

**Rodrigo** **L**eite da **S**ilva - **rodrigols89**
