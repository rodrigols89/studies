{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe195de1",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007700a",
   "metadata": {},
   "source": [
    "# Tokenization (tokenização)\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    " - 01 - Introdução ao Tokenization (Tokenização)\n",
    " - 02 - Tokenization (tokenização) com a biblioteca NLTK (Natural Language Toolkit)\n",
    " - 03 - Tokenization (tokenização) por sentenças"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951aeb36",
   "metadata": {},
   "source": [
    "## 01 - Introdução ao Tokenization (Tokenização)\n",
    "\n",
    "> No processamento de *linguagem natural*, **Tokenization (Tokenização)** é a tarefa de pré-processamento de texto que divide o texto em componentes menores de texto *(conhecidos como tokens)*.\n",
    "\n",
    "Para muitas tarefas de processamento de linguagem natural, precisamos acessar cada palavra em uma string. Para acessar cada palavra, primeiro temos que dividir o texto em componentes menores. O método para quebrar o texto em componentes menores é chamado de **Tokenization (Tokenização)** e os componentes individuais são chamados de **tokens**.\n",
    "\n",
    "Algumas operações comuns que exigem tokenização incluem:\n",
    "\n",
    " - Descobrir quantas palavras ou frases aparecem no texto;\n",
    " - Determinar quantas vezes uma palavra ou frase específica existe;\n",
    " - Contabilização de quais termos são susceptíveis de co-ocorrer...\n",
    "\n",
    "**NOTE:**  \n",
    "Embora os **tokens** sejam geralmente palavras ou termos individuais, eles também podem ser frases ou pedaços de texto de outro tamanho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aeae13",
   "metadata": {},
   "source": [
    "## 02 - Tokenization (tokenização) com a biblioteca NLTK (Natural Language Toolkit)\n",
    "\n",
    "Para **tokenizar** palavras individuais, podemos usara função **word_tokenize()** da biblioteca **NLTK (Natural Language Toolkit)**.\n",
    "\n",
    "A função aceita uma string e retorna uma lista de palavras. Veja um exemplo simples abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e8dc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenize this text\"\n",
    "tokenized_by_word = word_tokenize(text)\n",
    "\n",
    "print(tokenized_by_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120f9aa",
   "metadata": {},
   "source": [
    "**NOTE:**  \n",
    "Veja que nós *separamos (tokenizamos)* o texto por palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91129c9e",
   "metadata": {},
   "source": [
    "## 03 - Tokenization (tokenização) por sentenças\n",
    "\n",
    "Se eu desejar aplicar uma **Tokenization (tokenização)** por sentença. Por exemplo, separando por vírgula ou pontuações?\n",
    "\n",
    "Simples, para isso basta utilizar o método **sent_tokenize()** da classe **nltk.tokenize**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162ae62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So, during test time, any word that is not present in the vocabulary will be mapped to a UNK token.', 'This is how we can tackle the problem of OOV in word tokenizers.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"So, during test time, any word that is not present in the vocabulary will be mapped to a UNK token. This is how we can tackle the problem of OOV in word tokenizers.\"\n",
    "tokenized_by_sentence = sent_tokenize(text)\n",
    "\n",
    "print(tokenized_by_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73993046",
   "metadata": {},
   "source": [
    "**NOTE:**  \n",
    "Veja que agora nós aplicamos uma *separção Tokenization (tokenização)*, porém, por sentença."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9ccbd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**REFERÊNCIAS:**  \n",
    "[CodeAcademy - Text Preprocessing](https://www.codecademy.com/learn/text-preprocessing)\n",
    "\n",
    "---\n",
    "\n",
    "**Rodrigo Leite -** *drigols*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
