{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e540210",
   "metadata": {},
   "source": [
    "# Lemmatization & Part-of-Speech Tagging\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    " - 01 - Introdução a Lemmatization (Lemmatização)\n",
    " - 02 - Lemmatization (Lemmatização) na prática\n",
    " - 03 - Introdução ao Part-of-Speech Tagging (marcação de classe gramatical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07ee33",
   "metadata": {},
   "source": [
    "## 01 - Introdução a Lemmatization (Lemmatização)\n",
    "\n",
    "> No processamento de linguagem natural, a lematização é a tarefa de normalização de pré-processamento de texto preocupada em trazer as palavras às suas formas de raiz.\n",
    "\n",
    "**NOTE:**  \n",
    "Este é um processo mais complicado do que *stemming*, porque requer que o método conheça a classe gramatical de cada palavra. Uma vez que a **Lemmatização** requer a classe gramatical, é uma abordagem menos eficiente do que a *stemming*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508447ef",
   "metadata": {},
   "source": [
    "## 02 - Lemmatization (Lemmatização) na prática\n",
    "\n",
    "Com a biblioteca NLTK é muito fácil aplicar o conceito de **Lemmatização**. Veja o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5e3210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'wa', 'founded', 'in', '1926']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # Instance.\n",
    "tokenized = [\"NBC\", \"was\", \"founded\", \"in\", \"1926\"]\n",
    "\n",
    "# Apply Lemmatization.\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d11403",
   "metadata": {},
   "source": [
    "O resultado da **Lemmatização**, salvo na variável *lemmatized* contém **'wa'**, enquanto o resto das palavras permanecem as mesmas. Não é muito útil.\n",
    "\n",
    "**NOTE:**  \n",
    "Isso aconteceu porque a instância **lemmatize** da classe **WordNetLemmatizer** trata cada palavra como um substantivo.\n",
    "\n",
    "> Para tirar proveito do poder da lematização, precisamos marcar cada palavra em nosso texto com a classe gramatical mais provável.\n",
    "\n",
    "Vamos ver outro exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9dca817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lemmatizer exists: <WordNetLemmatizer>\n",
      "Words Tokenized: ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Lemmatized Words: ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # Instance.\n",
    "\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "\n",
    "try:\n",
    "  print(f'A lemmatizer exists: {lemmatizer}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatizer`')\n",
    "try:\n",
    "  print(f'Words Tokenized: {tokenized_string}')\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_string`')\n",
    "try:\n",
    "  print(f'Lemmatized Words: {lemmatized_words}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatized_words`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff98d2",
   "metadata": {},
   "source": [
    "**NOTE:**  \n",
    "Veja que de todas essas palavras apenas **\"was\"** foi lematizado para **\"wa\"**. Como nós já sabemos isso está acontecendo porque, precisamos marcar cada palavra em nosso texto com a classe gramatical mais provável."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4645b90",
   "metadata": {},
   "source": [
    "## 03 - Introdução ao Part-of-Speech Tagging (marcação de classe gramatical)\n",
    "\n",
    "> No processamento de linguagem natural, a **marcação de classe gramatical (part-of-speech tagging)** é o processo de atribuir uma classe gramatical a cada palavra em uma string. Usar a classe gramatical pode melhorar os resultados da **Lemmatization (Lemmatização)**.\n",
    "\n",
    "Para melhorar o desempenho da lematização, precisamos encontrar a classe gramatical para cada palavra em nossa string. Essa pode ser uma tarefa um pouco que complexa (mas nem tanto).\n",
    "\n",
    "Veja os códigos abaixo que aplicam esse conceito na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584e8dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'old', 'be', 'the', 'country', 'Indonesia']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer() # Instance.\n",
    "\n",
    "  tokenized = [\"How\", \"old\", \"is\", \"the\", \"country\", \"Indonesia\"]\n",
    "  lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "  print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3ac84",
   "metadata": {},
   "source": [
    "**NOTE:**  \n",
    "Como passamos na classe gramatical, **\"is\"** foi lançado em sua raiz, **\"be\".** Isso significa que palavras como **\"was\"** e **\"were\"** serão convertidas para **be**.\n",
    "\n",
    "**AGORA VAMOS EXPLICAR ALGUMAS PARTES DO CÓDIGO ACIMA:**  \n",
    "\n",
    "**1° -** Importamos as seguintes  classes:\n",
    "\n",
    " - **wordnet:**\n",
    "   - É um banco de dados que usamos para contextualizar palavras.\n",
    " - **Counter:**\n",
    "   - É um contêiner que armazena elementos como chaves de dicionário.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "```\n",
    "\n",
    "**2° -** Dentro da nossa função, usamos a função **wordnet.synsets()** para obter um conjunto de sinônimos para a palavra:\n",
    "\n",
    "```python\n",
    "probable_part_of_speech = wordnet.synsets(word)\n",
    "```\n",
    "\n",
    "**NOTE:**  \n",
    "Os sinônimos retornados vêm com sua classe gramatical.\n",
    "\n",
    "**3° -** Use sinônimos para determinar a parte gramatical mais provável\n",
    "\n",
    "Em seguida, criamos um objeto **Counter()** e definimos cada valor para a contagem do número de sinônimos que se enquadram em cada classe de palavras:\n",
    "\n",
    "```python\n",
    "pos_counts = Counter()\n",
    "pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "```\n",
    "\n",
    "**NOTE:**  \n",
    "Esta linha conta o número de substantivos no conjunto de sinônimos: **n = nouns**.\n",
    "\n",
    "**4° -**  Retorne a classe gramatical mais comum\n",
    "\n",
    "Agora que temos uma contagem para cada classe gramatical, podemos usar o método **most_common()** do objeto **pos_counts = Counter()** para encontrar e retornar a classe gramatical mais provável:\n",
    "\n",
    "```python\n",
    "most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4930d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**REFERÊNCIAS:**  \n",
    "[CodeAcademy - Text Preprocessing](https://www.codecademy.com/learn/text-preprocessing)\n",
    "\n",
    "---\n",
    "\n",
    "**Rodrigo Leite -** *drigols*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
