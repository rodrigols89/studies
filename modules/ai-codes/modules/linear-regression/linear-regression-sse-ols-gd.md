# Regress√£o Linear (SSE, OLS, GD)

## Conte√∫do

 - [01 - Um problema envolvendo Regress√£o Linear](#01)
 - [02 - M√©todo dos M√≠nimos Quadrados (Sum of Squared Errors: SSE)](#02)
 - [03 - M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)](#03)
 - [04 - M√©todo do Gradiente Descendente](#04)
 - [05 - Fun√ß√£o de Custo](#05)
 - [06 - Tentando minimizar a Fun√ß√£o de Custo](#06)
 - [07 - A Regra da Cadeia](#07)
   - [07.1 - Aplicando a Regra da Cadeia na Fun√ß√£o de Custo](#07-1)
 - [08 - Taxa de Aprendizagem (Learning Rate)](#08)
 - [09 - Aplicando o M√©todo do Gradiente Descendente na pr√°tica](#09)

---

<div id='01'></div>

## 01 - Um problema envolvendo Regress√£o Linear

Ok, para come√ßar com nossos exemplos pr√°ticos em **Regress√µes Lineares** vamos imaginar o seguinte... Suponha que n√≥s estamos olhando a rela√ß√£o entre **notas** de alunos e seus **sal√°rios**.

O c√≥digo vai ser o seguinte:

[students.py](src/students.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

plt.figure(figsize=(10, 7))
plt.scatter(df.Grade, df.Salary, color='g')
plt.title('Grades vs Salaries')
plt.xlabel('Grade')
plt.ylabel('Salary')
plt.savefig('../images/plot-01.png', format='png')
plt.show()
```

**OUTPUT:**  

![image](images/plot-01.png)  

Essas vari√°veis s√£o conhecidas, respectivamente como:

 - **Grade (nota):** *Vari√°veis ‚Äã‚Äãindependentes*, *entradas* ou *preditoras*;
 - **Salary (sal√°rio):** *Vari√°veis ‚Äã‚Äãdependentes*, *sa√≠das* ou *respostas*.

Em uma **Regress√£o Linear** √©  comum denotar as sa√≠das com **ùë¶<sub>i</sub>** e as entradas com **ùë•<sub>i</sub>**. Se houver duas ou mais *Vari√°veis ‚ÄãIndependentes*, elas podem ser representadas como um vetor **ùê± = (ùë•‚ÇÅ,‚Ä¶, ùë•·µ£)**, onde **ùëü (ou n)** √© o n√∫mero de entradas.

Mas ent√£o, como eu consigo analisar as **notas** de alunos e seus **sal√°rios**? Bem, existem v√°rias maneiras, por√©m, vamos ver as mais comuns.

---

<div id='02'></div>

## 02 - M√©todo dos M√≠nimos Quadrados (Sum of Squared Errors: SSE)

> O m√©todo dos M√≠nimos Quadrados calcula o erro para cada ponto **(x<sub>i</sub>, y<sub>i</sub>)** em rela√ß√£o a m√©dia de todas as sa√≠das **y<sub>i</sub>**.

N√£o entendeu? Veja a f√≥rmula abaixo:

![images](images/formula-01.png)  

> Resumidamente, n√≥s estamos tirando a `variancia` de cada ponto **(x<sub>i</sub>, y<sub>i</sub>)** em rela√ß√£o a m√©dia de todos os meus **y**.

Agora vamos transformar tudo isso em Python para ficar algo mais automatizado:

[students_sse.py](src/students_sse.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['Error'] = df['Salary'] - df['Salary'].mean()
df['Squared Errors'] = df['Error']**2
print(df)
print("Sum of Squared Errors (SSE): ", round(sum(df['Squared Errors'])))
```

**OUTPUT:**  
```python
    Grade  Salary     Error  Squared Errors
0      50   50000  -22400.0    5.017600e+08
1      50   54000  -18400.0    3.385600e+08
2      46   50000  -22400.0    5.017600e+08
3      95  189000  116600.0    1.359556e+10
4      50   55000  -17400.0    3.027600e+08
5       5   40000  -32400.0    1.049760e+09
6      57   59000  -13400.0    1.795600e+08
7      42   42000  -30400.0    9.241600e+08
8      26   47000  -25400.0    6.451600e+08
9      72   78000    5600.0    3.136000e+07
10     78  119000   46600.0    2.171560e+09
11     60   95000   22600.0    5.107600e+08
12     40   49000  -23400.0    5.475600e+08
13     17   29000  -43400.0    1.883560e+09
14     85  130000   57600.0    3.317760e+09
Sum of Squared Errors (SSE):  26501600000
```

**NOTE:**  
Vale ressaltar aqui que n√≥s estamos elevando todos os erros ao quadrado<sup>2</sup> porque alguns deles v√£o ser negativos, e como n√≥s queremos **somar todos os erros** isso acabaria modificando o resultado. Ou seja, estamos deixando todos positivos.

---

<div id='03'></div>

## 03 - M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)

> At√© ent√£o n√≥s estamos utilizando uma abordagem, onde n√≥s tiravamos a **varian√ßa** de cada ponto em rela√ß√£o a m√©dia de todos os resultados **y**.

Agora vamos utilizar uma abordagem que usa uma **reta de melhor ajuste** para ver se conseguimos um efeito melhor. Ou seja, vamos criar uma reta que fique o mais pr√≥ximo poss√≠vel de todos os pontos; Tanto os pontos acima da linha quanto os abaixo.

Para isso, n√≥s podemos utilizar a **Equa√ß√£o da Reta** para criar uma linha que passe o mais pr√≥ximo poss√≠vel de todos os dados:

![image](images/0001.png)  

Tenho certeza, todos voc√™s aprenderam essa f√≥rmula na escola. Para Regress√£o Linear, usamos s√≠mbolos como estes:

**Exemplo 01:**  
![image](images/0003.png)  

**Exemplo 02:**  
![image](images/0005.png)  

**Exemplo 03:**  
![image](images/0002.png)  

**Exemplo com Nota√ß√£o em Matriz:**  
![image](images/0004.png)  

Eu sei que isso pode acabar confundindo um pouco, mas √© s√≥ se lembrar da *Equa√ß√£o da Reta*, **y = mx + b**, onde:

 - O meu **m (ou a)** representa o Coeficiente Angular:
   - Que altera a inclina√ß√£o da reta;
   - E que √© representado por um valor constante.
 - O meu **b** representa o deslocamento da Reta:
   - Intercepta√ß√£o no eixo-y;
   - Que tamb√©m √© representado por um valor constante.

**NOTE:**  
Para criar essa **reta de melhor ajuste** n√≥s precisamos dos melhores valores poss√≠veis para os termos **m** e **b** `para a amostra que estamos trabalhando`. Isso porque os coeficientes **m** e **b** variam de valores de acordo com os dados que n√≥s temos.

> √ìtimo, entendemos a ideia por tr√°s do **Algoritmo de Regress√£o Linear** que usa uma **reta de melhor ajuste** para medir o **tamanho dos nossos erros** e **explicar a rela√ß√£o entre os dados**.

Uma maneira de tentar encontrar os valores para os coeficiente **m** e **b** √© utilizando as seguintes f√≥rmulas:

![image](images/formula-04.png)  
![image](images/formula-05.png)  

Agora vamos testar essa bruxaria em Python para praticar um pouco:

[students_mb_formula.py](src/students_mb_formula.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
b = df['Salary'].mean() - (m * df['Grade'].mean())

print("Angular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))
```

**OUTPUT:**  
```python
Angular Coefficient (m): 1516
Linear Coefficient (b): -5732.0
```

Ok, agora que n√≥s j√° temos os melhores valores para os coeficientes **m** e **b** `para esse conjunto de dados`  podemos aplicar eles na **Equa√ß√£o da Reta** e criar a **reta de melhor ajuste**:

[students_bestLine_OLS.py](src/students_bestLine_OLS.py)  
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
b = df['Salary'].mean() - (m * df['Grade'].mean())

regression_line = [(m*x) + b for x in df['Grade']]

plt.figure(figsize=(10, 7))
plt.scatter(df.Grade, df.Salary, color='g')
plt.plot(df.Grade, regression_line, color='b')
plt.title('Grades vs Salaries | Ordinary Least Squares: OLS')
plt.xlabel('Grade')
plt.ylabel('Salary')
plt.grid()
plt.savefig('../images/plot-02.png', format='png')
plt.show()
```

**OUTPUT:**  

![image](images/plot-02.png)  

---

![image](images/ohmygod.gif)  

√ìtimo, agora √© s√≥ tirar a `vari√¢ncia` de cada ponto **y<sub>i</sub>** em rela√ß√£o a **reta de melhor ajuste**:

[students_error_OLS.py](src/students_error_OLS.py)
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

df['(x_i - x_mean)'] = df['Grade'] - df['Grade'].mean()
df['(y_i - y_mean)'] = df['Salary'] - df['Salary'].mean()
df['(x_i - x_mean)(y_i - y_mean)'] = df['(x_i - x_mean)'] * df['(y_i - y_mean)']
df['(x_i - x_mean)^2'] = (df['Grade'] - df['Grade'].mean())**2

m = (sum(df['(x_i - x_mean)'] * df['(y_i - y_mean)'])) / sum(df['(x_i - x_mean)^2'])
b = df['Salary'].mean() - (m * df['Grade'].mean())

df['y = mx + b'] = [(m*x) + b for x in df['Grade']]
df['y_i - y = mx + b'] = df['Salary'] - df['y = mx + b']
df['(y_i - y = mx + b)^2'] = df['y_i - y = mx + b'] ** 2

newDF = df[['Grade', 'Salary', 'y = mx + b', 'y_i - y = mx + b', '(y_i - y = mx + b)^2']]

print(newDF)
print("Sum of Squared Errors (OLS): ", round(sum(newDF['(y_i - y = mx + b)^2'])))
```

**OUTPUT**
```python
    Grade  Salary     y = mx + b  y_i - y = mx + b  (y_i - y = mx + b)^2
0      50   50000   70075.255242     -20075.255242          4.030159e+08
1      50   54000   70075.255242     -16075.255242          2.584138e+08
2      46   50000   64010.703700     -14010.703700          1.962998e+08
3      95  189000  138301.460094      50698.539906          2.570342e+09
4      50   55000   70075.255242     -15075.255242          2.272633e+08
5       5   40000    1849.050390      38150.949610          1.455495e+09
6      57   59000   80688.220441     -21688.220441          4.703789e+08
7      42   42000   57946.152157     -15946.152157          2.542798e+08
8      26   47000   33687.945987      13312.054013          1.772108e+08
9      72   78000  103430.288725     -25430.288725          6.466996e+08
10     78  119000  112527.116039       6472.883961          4.189823e+07
11     60   95000   85236.634098       9763.365902          9.532331e+07
12     40   49000   54913.876386      -5913.876386          3.497393e+07
13     17   29000   20042.705017       8957.294983          8.023313e+07
14     85  130000  123140.081238       6859.918762          4.705849e+07
Sum of Squared Errors (OLS):  6958885882
```

Agora a *Soma dos Erros* foi reduzida significativamente de **26.501.600.000** para **6.958.885.882**. Isso porque n√≥s estamos utilizando uma reta de melhor ajuste que tem os melhores valores de **m** e **b** `para esse conjunto de dados`, n√£o apenas subtraindo os pontos **(x<sub>i</sub>, y<sub>i</sub>)** pelo a m√©dia de todos os **y**.

**NOTE:**  
Mas essa solu√ß√£o n√£o √© escalon√°vel... Aplicar isso √† Regress√£o Linear foi bastante f√°cil, pois t√≠nhamos bons coeficientes e equa√ß√µes lineares, mas aplicar isso a algoritmos complexos e n√£o lineares como *Support Vector Machine* n√£o seria vi√°vel. 

> Ent√£o vamos encontrar a aproxima√ß√£o num√©rica desta solu√ß√£o por um **m√©todo iterativo**?.

---

<div id='04'></div>

## 04 - M√©todo do Gradiente Descendente

![image](images/scared.gif)  

> Calma gente, rss... Eu sei que esse nome **"Gradiente Descendente"** assusta todo mundo, mas voc√™s v√£o ver como √© simples. √â tudo quest√£o de saber quando e como ele √© aplicado.

Ent√£o, antes n√≥s estavamos utilizando o **M√©todo dos M√≠nimos Quadrados Ordin√°rios (Ordinary Least Squares: OLS)** que constru√≠a uma **reta de melhor ajuste** a partir de 2 f√≥rmulas... Agora n√≥s vamos ter que criar a mesma reta, por√©m sem aquelas f√≥rmulas. Isso porque n√≥s vamos utilizar uma **abordagem iterativa**.

Para come√ßar, vamos imaginar a seguinte hist√≥ria:

Considere que 20 pessoas (incluindo voc√™) s√£o lan√ßadas aleatoriamente no ar em uma cadeia de montanhas. Sua tarefa √© encontrar **o pico mais alto de todas as montanhas** em 30 dias.

![image](images/gd-01.jpg)  

Cada um de voc√™s tem um *walkie talkie* para se comunicar e um *alt√≠metro* para medir a altitude. Todos os dias voc√™s passam horas localizando o pico mais alto poss√≠vel e relatam sua altitude mais alta do dia a todos os outros.

![image](images/gd-02.png)  

Suponha que no dia 1 voc√™ relate 1000 p√©s; Outra pessoa informa 1230 p√©s e assim por diante. Depois, h√° uma pessoa que reporta 5000 p√©s, que √© o m√°ximo de todos.

**Lembre-se de que sua tarefa era atingir *coletivamente* o pico mais alto de todas as montanhas.**  
O que voc√™ faz a seguir no dia 2? No dia seguinte, todos se reunir√£o em dire√ß√£o √† √°rea onde a altitude m√°xima foi encontrada ontem. Eles pensar√£o que √© prov√°vel que o pico mais alto seja nesta √°rea.

> Por que algu√©m que reportou 500 p√©s ontem procuraria naquela √°rea de novo se h√° outra √°rea que j√° tem 5.000 p√©s?

**NOTE:**  
Portanto, todos os pesquisadores se movem **"rapidamente"** em dire√ß√£o ao ponto mais alto relatado. Agora, essa gan√¢ncia pode levar voc√™ ao pico mais alto de todas as montanhas, mas tamb√©m pode levar a um erro enorme... O cara que estava a 500 p√©s ontem poderia ter ido em dire√ß√£o a um pico que tinha uma altura de 10.000 p√©s! E ele o ignorou e foi em dire√ß√£o aquele de 5.000 p√©. Voc√™ realmente fica preso em um **M√°ximo Local**. E n√£o h√° como saber se voc√™ est√° preso no **M√°ximo Local**.

**Simulated Annealing:**  
Existe um Algoritmo que seria √∫til para n√≥s nesse caso, **Simulated Annealing:**, onde os pesquisadores teriam pesquisado todo o espa√ßo de pesquisa minuciosamente e sem serem tendenciosos para provavelmente encontrar os **m√°ximos globais**.

Vou deixar alguns exemplos visuais de **Simulated Annealing** para deixar mais claro:

![image](images/simulated-annealing-00.gif)  

---

![image](images/simulated-annealing-01.gif)  

---

![image](images/simulated-annealing-03.gif)  

---

![image](images/simulated-annealing-02.gif)  

Voltando para o problema das montanhas... Se tivesse como equacionar esse problema das Montanhas, ou seja, transforma isso em Matem√°tica n√≥s s√≥ precisar√≠amos tentar chegar (ou encontrar) o ponto **M√°ximo Global** da fun√ß√£o, tentando evitar ficar preso em *M√°ximos Locais*.

Agora vamos voltar para o nosso problema de **Regress√£o Linear**, onde, n√≥s quer√≠amos criar uma **reta de melhor ajuste** utilizando a *Equa√ß√£o da Reta*:

![image](images/0001.png)  

**NOTE:**  
Por√©m, vale lembrar que n√≥s n√£o vamos mais utilizar aquelas f√≥rmulas m√°gicas para tentar encontrar bons valores dos coeficientes **m** e **b** para o nosso conjunto de dados. Isso porque n√≥s vamos utilizar uma **abordagem iterativa**.

**Como assim uma Abordagem Iterativa?**  
Resumidamente, n√≥s vamos ficar tentando valores para os coeficientes **m** e **b** at√© achar a **reta de melhor ajuste** para o nosso conjunto de dados.

Um exemplo bem abstrado (e maluco) poderia ser o seguinte:

![image](images/example-01.png)  

Mas se pensarmos bem, n√≥s resolvemos um problema e agora temos outro *(mesmo que mais simples)*, que √© ficar tentando v√°rios valores para os coeficientes **m** e **b** at√© achar a **reta de melhor ajuste** para o nosso conjunto de dados.

![image](images/ELVN.gif)  

> Ent√£o, temos um problema hein? J√° pensou ter que criar 1 milh√£o de retas o tamanho do recurso computacional que seria gasto?

Ok temos um problema, como escolher os melhores valores poss√≠veis para os meus **m** e **b**? Bem, pensem comigo nas seguintes abordagens:

 - **Primeira abordagem:** N√≥s podemos ir alterando valores de pouco em pouco, por exemplo, 1 unidade por teste em **m** e **b**:
   - O problema √© que se tivermos muito longe dos melhores valores para **m** e **b** isso vai exigir muito recurso computacional.
 - **Segunda abordagem:** Essa abordagem √© muito simples, vamos aumentar valores muito grandes para testes em **m** e **b**. Por exemplo, 1000, 500, 300, 100...

**Qual a melhor abordagem? AS DUAS!**  
√â isso mesmo... Pense comigo, se eu aumentar grandes unidades (valores) para os meus **m** e **b**; E a medida que eles v√£o se aproximando da melhor reta poss√≠vel eu posso ir diminuindo esses valores at√© chegar na **reta de melhor ajuste** para o nosso conjunto de dados.

Ou seja:

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto.

![image](images/problem.gif)  

Ent√£o, o **Gradiente Descendente** √© o bixo de sete cabe√ßas que consegue aplicar esse conceito que n√≥s aprendemos agora. Mas como?

Primeiro vamos ver aqui a matem√°tica das coisas n√©:

**Equa√ß√£o da Reta - (Regress√£o Linear)**  
![image](images/linear-regression-formule-02.png)  

Agora vamos pegar o nosso gr√°fico de compara√ß√£o entre **notas** de alunos e seus **sal√°rios**:

![image](images/plot-01.png)  

Suponha que n√≥s selecionamos um desses pontos, o ponto **y<sub>i</sub>**, algo parecido com isso:

![image](images/example-02.png)  

Agora suponha que n√≥s pegamos a *equa√ß√£o da reta* e desenhamos uma reta qualquer:

![image](images/example-03.png)  

Ent√£o, agora n√≥s estamos com a seguinte situa√ß√£o:
 - Um ponto **y<sub>i</sub>** selecionado;
 - E criamos uma *equa√ß√£o reta* com valores aleat√≥rios para os coeficientes **m** e **b**.

Agora vem a pergunta chave:

> Como eu sei qu√£o distante essa reta est√° dos meus dados? - **Todos eles!**

Pode parecer muito dif√≠cil, mas √© simples:

 - **1¬™ -** Eu vou pegar o meu **x<sub>i</sub>**
 - **2¬™ -** Ver qual foi o resultado da equa√ß√£o da reta nesse ponto.
 
N√£o entendeu? Vamos ver isso visualmente:

![image](images/example-04.png)  

Veja que agora n√≥s temos outro ponto de intersec√ß√£o, por√©m esse vai representar minha **(regress√£o<sub>i</sub>)**. Mas o que isso tem a ver com a nossa pergunta chave?

> Como eu sei qu√£o distante essa reta est√° dos meus dados? - **Todos eles!**

Bem, o ideal seria que essa *equa√ß√£o da reta* passasse bem no nosso ponto **y<sub>i</sub>**,mas como podemos ver tem uma dist√¢ncia entre o meu ponto **y<sub>i</sub>** e a minha **regress√£o<sub>i</sub>**.

U√©, mas se temos uma dist√¢ncia entre esses dois pontos √© s√≥ calcular essa dist√¢ncia n√£o √©?

![image](images/genius.gif)

Ou seja, a diferen√ßa entre a minha **regress√£o<sub>i</sub>** e o meu ponto **y<sub>i</sub>**:

![image](images/example-05.png)  

---

<div id="05"></div>

## 05 - Fun√ß√£o de Custo

**Agora vamos ver alguns detalhes aqui:**

 - **1¬™ -** Essa dist√¢ncia entre o meu ponto **y<sub>i</sub>** e a minha **regress√£o<sub>i</sub>** √© o que n√≥s conhecemos como:
   - **Erro para esse ponto y<sub>i</sub>.**
 - **2¬™ -** N√≥s vamos ter que sair medindo esse erro para todos os pontos do nosso gr√°fico:
   - Ou seja, n√≥s vamos tirar o erro para todos os nossos **y<sub>i</sub>** em rela√ß√£o a essa *reta que n√≥s criamos*.
 - **3¬™ -** E por fim, fazer a soma de todos os erros para essa reta:
   - A soma de todos os erros vai n√≥s da o ***tamanho do nosso erro para essa reta***:
     - Quanto *maior* esse valor, *maior vai ser nosso erro*;
     - Quanto *menor* esse valor, *menor vai ser nosso erro*.

**Ok, mas como eu posso equacionar isso?**  
Bem isso √© o que n√≥s conhecemos como **Fun√ß√£o de Custo**, **Loss Function**, **L**, **J**... Existem v√°rias abordagens.

Alguns exemplos s√£o:

**Example-01:**  
![image](images/01.png)  

**Example-02:**  
![image](images/cf-function-01.png)

**Example-03:**  
![image](images/cf-2m.png)

Essas fun√ß√µes v√£o variar de acordo com o seu problema, Algumas utilizam:

 - **RMSE**;
 - **MSE**;
 - **MAE**
 - **ErroM√©dio**... etc.

Como n√≥s estamos focando apenas em somar todos os nossos erros e mais nada, vamos ficar com o primeiro exemplo **(Example-01)** de **Fun√ß√£o de Custo**.

---

<div id='06'></div>

## 06 - Tentando minimizar a Fun√ß√£o de Custo

Ok, n√≥s j√° temos a nossa **Fun√ß√£o de custo** que vai fazer a soma de todos os nossos erros:

![image](images/01.png)  

Recapitulando:

 - 1¬™ - N√≥s tra√ßamos uma reta;
 - 2¬™ - Calculamos o *erro* para cada ponto na reta;
 - 3¬™ - Por fim, fizemos a *soma de todos os erros* e conseguimos ***tamanho do nosso erro***.
   - N√≥s estamos elevando a subtra√ß√£o de **(reg<sub>i</sub> - y<sub>i</sub>)** PARA CADA ITERA√á√ÉO DO SOMAT√ìRIO ao quadrado<sup>2</sup> porque alguns desses valores podem ser negativos e n√≥s queremos a SOMA DE TODOS ELES.

At√© ai tudo bem, calculamos o ***tamanho do nosso erro*** para uma reta, mas lembra que n√≥s tinhamos uma abordagem?

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto - Para encontrar os melhores valores de **m** e **b**.

Ou seja, n√≥s vamos construir outra reta que n√≥s d√™ um valor menor para a nossa **Fun√ß√£o de Custo**.  
Mas como? Simples, n√≥s vamos passar novos valores para os coeficientes **m** e **b**: 

![image](images/02.png)  

Olhando para a abstra√ß√£o acima n√≥s temos que as nossas vari√°veis **m** e **b** na *equa√ß√£o da reta* v√£o receber novos valores; que v√£o ser *subtra√≠dos* de **algo (something)**. Mas que algo √© esse?  

> Ent√£o, √© ai que entra o conceito de **Derivadas**, **Derivadas Parciais** e **M√≠nimos de uma Fun√ß√£o**.

Suponha que a nossa *equa√ß√£o da reta* tem o seguinte gr√°fico (√© s√≥ um exemplo):

![image](images/example-06.png)  

Bem, no exemplo acima n√≥s temos uma par√°bola (√© s√≥ um exemplo) com v√°rias Taxas de Varia√ß√£o e n√≥s estamos interessando nos **pontos m√≠nimos da fun√ß√£o** para as nossas vari√°veis **m** e **b**.

Agora sabendo disso e da nossa abordagem n√≥s vamos:

 - Derivar a nossa **Fun√ß√£o de Custo** para **m** e para **b**:
   - Dando grandes passos quando estivermos muito longe do ponto m√≠nimo;
   - E passos curtos quando estivermos muito pr√≥ximos do ponto m√≠nimo.

Ou seja, como n√≥s temos mais de uma vari√°vel, n√≥s vamos aplicar o conceito de **Derivadas Parciais**, onde, n√≥s derivamos para uma vari√°vel e deixamos a outra como constante e depois fazemos o mesmo para a outra e vamos diminuindo at√© achar os pontos m√≠nimos poss√≠veis para os coeficientes **m** e **b**:

![image](images/der01.png)  

Ahh, entendido, mas tem 2 observa√ß√µes na abstra√ß√£o acima:

 - 1¬™ - N√≥s temos 2 constantes **Œ± (Alpha)** e **Œ≤ (Beta)** que est√£o multiplicando as nossas Derivadas Parciais;
 - 2¬™ - N√≥s estamos Derivando a nossa **Fun√ß√£o de Custo** para as vari√°veis **m** e **b** da *equa√ß√£o da reta* - **(What?)**.

<div id='07'></div>

## 07 - A Regra da Cadeia

> Por enquanto (mas s√≥ por enquanto) vamos ignorar as constantes **Œ± (Alpha)** e **Œ≤ (Beta)** e vamos relembrar um conceito matem√°tico chamado: **Regra da Cadeia**.

Em c√°lculo, a **Regra da Cadeia** √© uma f√≥rmula para a derivada da fun√ß√£o composta de duas fun√ß√µes - **What?**  
Ok, vamos para os exemplos, suponha que n√≥s temos as seguintes fun√ß√µes:

![image](images/der02.png)  

Veja que agora n√≥s temos 2 fun√ß√µes **y** e **z**, onde:

 - O meu **y** depende de **z**;
 - E o meu **z** depende de **x**.

Agora eu quero Derivar a minha fun√ß√£o **y** para **x**:

![image](images/der03.png)  

U√©, como vai ser? Se o meu **y** depende do **z**; e o meu **z** depende do **x**?

**A REGRA DA CADEIA:**  
√â ai que entra o conceito da **Regra da Cadeia**. Basta seguir a f√≥rmula abaixo:

![image](images/der04.png)  

Agora √© s√≥ tirar a Derivada de **y** para **z**; e de **z** para **x**; e depois multiplicar elas... Vai ficar assim:

![image](images/der05.png)  

**NOTE:**  
√â importante lembrar que algumas dessas vari√°veis tem rela√ß√µes com outra, por exemplo, o meu **z = 3x**, por isso n√≥s mudamos isso na hora de trabalhar a equa√ß√£o.

Mas no fim o que importa √© que a Derivada da fun√ß√£o **y** em rela√ß√£o a **x** √© **18x**.

<div id='07-1'></div>

## 07.1 - Aplicando a Regra da Cadeia na Fun√ß√£o de Custo

Ok, agora que n√≥s j√° revisamos o conceito da **Regra da Cadeia**, como eu posso aplicar ela na minha **Fun√ß√£o de Custo** para os coeficientes **m** e **b**? Bem, vamos ver as rela√ß√µes n√©?

Primeiro vamos pegar nossa **Fun√ß√£o de Custo**:

![image](images/01.png)  

Ok, dentro da *Fun√ß√£o de Custo* n√≥s temos a fun√ß√£o **reg<sub>i</sub>** que depende de **m**; E a minha *Fun√ß√£o de Custo* que depende de **reg<sub>i</sub>**. U√©, s√≥ aplicar a Regra da Cadeia ent√£o...

**NOTE:**  
Primeiro vamos apelidar na nossa fun√ß√£o **(reg<sub>i</sub> + y<sub>i</sub>)<sup>2</sup>** de ***error*** para ficar uma abstra√ß√£o mais nominal.

Agora √© s√≥ tirar a Derivada da **Fun√ß√£o de Custo** para **m** seguindo a *Regra da Cadeia*:

![image](images/der06.png)  

 - Veja que a minha **Fun√ß√£o de Custo** depende do **error**; e o meu **error** depende de **m**;
 - Agora √© s√≥ tirar a Derivada da minha **Fun√ß√£o de Custo** para o **erro**; e do meu **erro** para **m** e depois multiplicar:

![image](images/der07.png)  

Simplificando, a Derivada da minha **Fun√ß√£o de Custo** para **m** vai ser :

![image](images/der08.png)  

√ìtimo, agora √© s√≥ tirar a Derivada da **Fun√ß√£o de Custo** em rela√ß√£o a **b**:

![image](images/der09.png)  

Simplificando novamente:

![image](images/der10.png)  

**Pronto, resolvido!**  
Agora n√≥s j√° temos as Derivadas da **Fun√ß√£o de Custo** para **m** e **b**:

![image](images/der11.png)  

Como os nossos **x<sub>i</sub>** e **y<sub>i</sub>** s√£o valores que n√≥s j√° temos no gr√°fico (nossos dados) n√≥s s√≥ vamos Derivando os coeficientes **m** e **b** at√© chegar o mais pr√≥ximo poss√≠vel do m√≠nimo da **Fun√ß√£o de Custo**, seguindo a nossa abordagem:

> Caminhando r√°pido quando a gente est√° muito longe; E caminhando devagar quando a gente est√° muito perto.

![image](images/gd-graph.gif)  

---

<div id='08'></div>

## 08 - Taxa de Aprendizagem (Learning Rate)

Ok pessoas... Voltando alguns passos atr√°s, lembram que n√≥s tinhamos umas constantes que multiplicavam nossas Derivadas da **Fun√ß√£o de Custo** para **m** e **b**? Eram as constantes **Œ± (Alpha)** e **Œ≤ (Beta)**, mas o que elas significavam?

Ent√£o, isso √© o que n√≥s chamamos de **Taxa de Aprendizado (Learning Rate)**, respons√°veis por ajuste e determina o tamanho da etapa em cada itera√ß√£o enquanto se move em dire√ß√£o ao **M√≠nimo da Fun√ß√£o de Custo**.

A **Taxa de Aprendizado (Learning Rate)** √© um hiperpar√¢metro que controla o quanto alterar o modelo em resposta ao erro estimado cada vez que os pesos do modelo s√£o atualizados. Escolher a taxa de aprendizado √© desafiador, pois um valor muito pequeno pode resultar em um longo processo de treinamento que pode travar, enquanto um valor muito grande pode resultar no aprendizado de um conjunto sub-√≥timo de pesos muito r√°pido ou um processo de treinamento inst√°vel.

Seguindo a nossa abordagem de *Gradiente Descendente* a *Taxa de Veria√ß√£o* vai determina o tamanho dos nossos passos em dire√ß√£o ao **M√≠nimo** da **Fun√ß√£o de Custo**.

Veja os exemplos abaixo:

**Exemplo-01:**  
![image](images/learning-rate-01.jpg)  

**Exemplo-02:**  
![image](images/learning-rate-02.jpg)  

---

<div id='09'></div>

## 09 - Aplicando o M√©todo do Gradiente Descendente na pr√°tica

Ok, mas depois de todas essas **bruxarias te√≥ricas**, como colocar tudo isso em pr√°tica? Bem, vamos ver isso agora com Python sem precisar importar nenhuma biblioteca (mas s√≥ por agora):

[students_gd_bestLine.py](src/students_gd_bestLine.py)  
```python
from matplotlib import pyplot as plt
import pandas as pd

df = pd.DataFrame(
  {
    'Grade':[50, 50, 46, 95, 50, 5, 57, 42, 26, 72, 78, 60, 40, 17, 85],
    'Salary':[50000, 54000, 50000, 189000, 55000, 40000, 59000, 42000, 47000, 78000, 119000, 95000, 49000, 29000, 130000]
  }
)

m = 7
b = 1
learning_rate = 0.000001

for i in range(1, 1000+1, 1):
  y_pred = m*df['Grade'] + b
  m_derivative = sum(2*df['Grade']*(y_pred - df['Salary']))
  b_derivative = sum(2*(y_pred - df['Salary']))
  m = m - (learning_rate * m_derivative)
  b = b - (learning_rate * b_derivative)
  # print(m, b) # Remove comments to view step-by-step.

print("\nAngular Coefficient (m): {0}\nLinear Coefficient (b): {1}".format(round(m), round(b)))

regression_line = [(m*x) + b for x in df['Grade']]

plt.figure(figsize=(10, 7))
plt.scatter(df.Grade, df.Salary, color='g')
plt.plot(df.Grade, regression_line, color='b')
plt.title('Grades vs Salaries | Gradient descent Approach')
plt.xlabel('Grade')
plt.ylabel('Salary')
plt.grid()
plt.savefig('../images/plot-03.png', format='png')
plt.show()
```

**OUTPUT:**  
```python
Angular Coefficient (m): 1425
Linear Coefficient (b): -7
```

![image](images/plot-03.png)

**NOTE:**  
Veja que com o **M√©todo do Gradiente Descendente** n√≥s conseguimos melhores valores para os coeficientes **m** e **b** em rela√ß√£o ao **M√©todo dos M√≠nimos Quadrados Ordin√°rios (OLS)**:

 - **M√©todo dos M√≠nimos Quadrados Ordin√°rios (OLS)**:
   - Angular Coefficient (m): 1516
   - Linear Coefficient (b): -5732.0
 - **M√©todo do Gradiente Descendente**:
   - Angular Coefficient (m): 1425
   - Linear Coefficient (b): -7

---
**REFERENCES:**  
[Didatica Tech - M√ìDULO - I](https://didatica.tech/)  
[A matem√°tica do Gradiente Descendente & Regress√£o Linear (machine learning)](https://www.youtube.com/watch?v=htfh2xrnlaE)  
[Metodo da Descida do Gradiente](https://www.youtube.com/watch?v=s0VhfvCB0Vw)  
[AULA 5 - GRADIENTE DESCENDENTE EXPLICADO - CURSO DE INTELIG√äNCIA ARTIFICIAL PARA TODOS](https://www.youtube.com/watch?v=joaYDx1HTcA)  
[Optimization: Ordinary Least Squares Vs. Gradient Descent ‚Äî from scratch](https://towardsdatascience.com/https-medium-com-chayankathuria-optimization-ordinary-least-squares-gradient-descent-from-scratch-8b48151ba756)   
