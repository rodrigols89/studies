{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1691fd7",
   "metadata": {},
   "source": [
    "# Review: Overfitting,  Underfitting & O trade-off viés-variância"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6ec53",
   "metadata": {},
   "source": [
    "# 01 - O trade-off viés-variância\n",
    "\n",
    "> A medida que aumenta a *complexidade* de um modelo o **vies/bias** e **variância** vão se distanciando inversamente.\n",
    "\n",
    "Para ficar mais claro vamos analisar a imagem abaixo:\n",
    "\n",
    "![img](images/overfitting-underfitting-03.png)  \n",
    "\n",
    " - **Quanto MAIOR A COMPLEXIDADE do modelo (eixo-x), ou seja, mais parâmetros (features) são adicionados:**\n",
    "   - Maior é o Erro da Variância;\n",
    "   - Menor é o Erro do Viés/Bias.\n",
    " - **Quanto MENOR A COMPLEXIDADE do modelo (eixo-x), ou seja, menos parâmetros (features) são adicionados:**\n",
    "   - Menor é o Erro de Variância;\n",
    "   - Maior é o Erro de Viés/Bias.\n",
    " - **O erro total (Total Error) independente da Complexidade do modelo vai iniciar e terminar lá em cima, porém, ele não é constante**:\n",
    "   - Vejam que ele começa lá em cima > vai caindo > sobe novamente - Por isso, não é constante\n",
    " - **O melhor a se fazer é encontrarmos o equilíbrio entre os dois erros e o erro total:**\n",
    "   - A linha tracejada demonstra o equilíbrio (Optimum Model Complexity) entre os dois erros e o erro total e é lá onde queremos chegar.\n",
    "\n",
    "**NOTE:**  \n",
    "Olhando para a imagem e as observações acima fica claro que tem um **trade-off** entre o **viés/bias** e a **variância** e nós queremos chegar no valor mínimo possível dos dois erros simultaneamente.\n",
    "\n",
    "Agora vamos analisar outra imagem que se aproxima mais de um modelo de **Machine Learning**:\n",
    "\n",
    "![img](images/over-under-01.png)  \n",
    "\n",
    " - **Quando nós temos uma ALTA VARIÂNCIA nós temos Overfitting:**\n",
    "   - Que é quando um modelo decora os dados de treinamento, porém, não generaliza bem sobre os dados de teste (ou validaçao).\n",
    " - **Quando nós temos um ALTO VIÉS/BIAS nós temos Underfitting:**\n",
    "   - Que é quando o desempenho do modelo já é ruim no próprio treinamento.\n",
    "\n",
    "**Então, qual seria o ideal?**  \n",
    "\n",
    "![img](images/over-under-02.png)  \n",
    "\n",
    "**NOTE:**  \n",
    "Vejam que novamente nós estamos buscando o ponto de e equilíbrio entre o **viés/bias** e a **variância**. Por isso, **\"trade-off viés-variância\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc57f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbb4e6",
   "metadata": {},
   "source": [
    "# 02 - Overfitting & Underfitting\n",
    "\n",
    "Resumidamente, para entender o básico sobre **Overfitting** e **Underfitting** nós temos:\n",
    "\n",
    " - **Generalização:**\n",
    "   - Generalização refere-se a quão bem um modelo de Machine Learning aprendeu. Isto se verifica observando os resultados que o modelo produz no *set de teste (ou validação)*, em contraposição ao *set de treinamento*.\n",
    " - **Tipos de erros em Modelos de Machine Learning:**\n",
    "   - **Erro de variância (Overfitting):**\n",
    "     - A este erro dá-se o nome de *Overfitting*;\n",
    "     - O modelo com alta variância presta muita atenção aos dados de treinamento e não generaliza bem sobre os dados que não viu antes:\n",
    "       - Basicamente, é como se ele estivesse decorado os dados de treino e não sabe estimar dados que não viu, visto que ele apenas decorou os dados de treino e não aprendeu padrões.\n",
    "     - Um cenário de *Overfitting* ocorre quando, nos dados de treino, o modelo tem um desempenho excelente, porém quando utilizamos os dados de teste o resultado é ruim.\n",
    "   - **Erro de viés/bias (Underfitting):**\n",
    "     - A este erro dá-se o nome de *Underfitting*;\n",
    "     - O modelo com alto *viés/bias* presta pouca atenção aos dados de treinamento e simplifica demais o modelo:\n",
    "       - Isso sempre leva a um alto erro nos dados de treinamento e teste, visto que ele não aprendeu o suficiente nos dados de treinamento.\n",
    "     - Um cenário de *Underfitting* ocorre quando o desempenho do modelo já é ruim no próprio treinamento. Ou seja, o modelo não consegue encontrar relações entre as variáveis e o teste nem precisa acontecer.\n",
    "   - **Erro irredutível:**\n",
    "     - Também chamado de *ruído* o *Erro irredutível* não pode ser reduzido, independentemente do algoritmo usado. É o erro introduzido por exemplo, por medições incorretas na coleta de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce4599",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c70699",
   "metadata": {},
   "source": [
    "**REFERÊNCIAS:**  \n",
    "[Overfitting & Underfitting](overfitting-underfitting.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
